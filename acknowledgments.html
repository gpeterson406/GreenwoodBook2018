<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Acknowledgments | Intermediate Statistics with R</title>
  <meta name="description" content="Acknowledgments | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Acknowledgments | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Acknowledgments | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="chapter4.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a><ul>
<li class="chapter" data-level="0.1" data-path="acknowledgments.html"><a href="acknowledgments.html#section1-5"><i class="fa fa-check"></i><b>0.1</b> Summary of important R code</a></li>
<li class="chapter" data-level="0.2" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-3"><i class="fa fa-check"></i><b>0.2</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="0.3" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-4"><i class="fa fa-check"></i><b>0.3</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="0.4" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-5"><i class="fa fa-check"></i><b>0.4</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-6"><i class="fa fa-check"></i><b>0.5</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-7"><i class="fa fa-check"></i><b>0.6</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="0.7" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-8"><i class="fa fa-check"></i><b>0.7</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="0.8" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-9"><i class="fa fa-check"></i><b>0.8</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="0.9" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-10"><i class="fa fa-check"></i><b>0.9</b> Chapter summary</a></li>
<li class="chapter" data-level="0.10" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-11"><i class="fa fa-check"></i><b>0.10</b> Summary of important R code</a></li>
<li class="chapter" data-level="0.11" data-path="acknowledgments.html"><a href="acknowledgments.html#section2-12"><i class="fa fa-check"></i><b>0.11</b> Practice problems</a></li>
<li class="chapter" data-level="0.12" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-1"><i class="fa fa-check"></i><b>0.12</b> Situation</a></li>
<li class="chapter" data-level="0.13" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-2"><i class="fa fa-check"></i><b>0.13</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="0.14" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-3"><i class="fa fa-check"></i><b>0.14</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="0.15" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-4"><i class="fa fa-check"></i><b>0.15</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="0.16" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-5"><i class="fa fa-check"></i><b>0.16</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="0.17" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-6"><i class="fa fa-check"></i><b>0.17</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="0.18" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-7"><i class="fa fa-check"></i><b>0.18</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="0.19" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-8"><i class="fa fa-check"></i><b>0.19</b> Chapter summary</a></li>
<li class="chapter" data-level="0.20" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-9"><i class="fa fa-check"></i><b>0.20</b> Summary of important R code</a></li>
<li class="chapter" data-level="0.21" data-path="acknowledgments.html"><a href="acknowledgments.html#section3-10"><i class="fa fa-check"></i><b>0.21</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>1</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>1.1</b> Situation</a></li>
<li class="chapter" data-level="1.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>1.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="1.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>1.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="1.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>1.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="1.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>1.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="1.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>1.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="1.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>1.7</b> Chapter summary</a></li>
<li class="chapter" data-level="1.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>1.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>1.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>2</b> Chi-square tests</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>2.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="2.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>2.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="2.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>2.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="2.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>2.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="2.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>2.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="2.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>2.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="2.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>2.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="2.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>2.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="2.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>2.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="2.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>2.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="2.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>2.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="2.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>2.12</b> Chapter summary</a></li>
<li class="chapter" data-level="2.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>2.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="2.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>2.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>3</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>3.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="3.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>3.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="3.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>3.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="3.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>3.4</b> Inference for the correlation coefficient (Optional section)</a></li>
<li class="chapter" data-level="3.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>3.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="3.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>3.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="3.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>3.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="3.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>3.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="3.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>3.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="3.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>3.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="3.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>3.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="3.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>3.12</b> Chapter summary</a></li>
<li class="chapter" data-level="3.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>3.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>3.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>4</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>4.1</b> Model</a></li>
<li class="chapter" data-level="4.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>4.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="4.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>4.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="4.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>4.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="4.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>4.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="4.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>4.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="4.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>4.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="4.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>4.8</b> Chapter summary</a></li>
<li class="chapter" data-level="4.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>4.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>4.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>5</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>5.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="5.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>5.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="5.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>5.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="5.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>5.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="5.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>5.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="5.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>5.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="5.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>5.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="5.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>5.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="5.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>5.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="5.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>5.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="5.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>5.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="5.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>5.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="5.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>5.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="5.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>5.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="5.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>5.15</b> Chapter summary</a></li>
<li class="chapter" data-level="5.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>5.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="5.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>5.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>6</b> Case studies</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>6.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="6.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>6.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="6.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>6.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="6.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>6.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="6.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>6.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>6.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="acknowledgments" class="section level1 unnumbered">
<h1>Acknowledgments</h1>
<p>I would like to thank all the students and instructors who have provided input in the development of the current version of STAT 217 and that have impacted the choice of topics and how we try to teach them. Dr. Jim Robison-Cox initially developed this course using R and much of this work retains his initial ideas. The first three editions of the book were co-authored with Dr. Katharine Banner, who had a major impact on all aspects of the book as it exists today. Many years of teaching these topics and helping researchers use these topics has helped to refine how they are presented here. Observing students years after the course has also impacted what we try to teach in the course, trying to prepare these students for the next levels of statistics courses that they might encounter, the next class where they might need or want to use statistics, and for potentially using statistics in the rest of their lives.</p>
<p>I have intentionally taken a first person perspective at times to be able to include stories from some of those interactions to try to help you avoid some of their pitfalls in your current or future usage of statistics. When I take the perspective of “we”, I am referring to the team of instructors that help to deliver this material to the students. I would also like to thank my wife, Teresa Greenwood, for allowing me the time and providing support as I repeatedly work on this. Buster Greenwood (our dog) also has played a role in approving everything that I write. I would like to acknowledge Dr. Gordon Bril (Luther College) who introduced me to statistics while I was an undergraduate and Dr. Snehalata Huzurbazar (West Virginia University) that guided me to completing my Master’s and Ph.D. in Statistics and continues to be a valued mentor and friend to me.</p>
<p>The development of this text was initially supported with funding from Montana State University’s Instructional Innovation Grant Program with the grant <em>Towards more active learning in STAT 217</em>. This book was born with the goal of having a targeted presentation of topics that we cover (and few that we don’t) that minimizes cost to students and incorporates the statistical software R (and the interface RStudio) from day one and every day after that. The software is a free, open-source platform and so is dynamically changing over time. This has necessitated frequent revisions of the text.</p>
<p>This is Version 1.0 of the book with this title but the fifth version of most of the content. This version contains small changes to correct a few errors and R code changes from the 2017 version, adds an index of terms, and some new practice problems and case-study to the final chapter. This text has been created by Greta Linse Peterson of Great Lines Writing and Consulting Services (<a href="https://www.greatlineswriting.com/" class="uri">https://www.greatlineswriting.com/</a>) who ported the book into RStudio’s bookdown format and tried to edit and improve the writing in the text. Any remaining errors are the responsibility of Mark Greenwood. The book was developed during Fall 2013 and the text has continually evolved since its creation. In recent versions, updates of discussions of hypothesis testing have been made to reflect the newest thinking in the statistics education community. As a field, we are transitioning from fixed alpha, null hypothesis significance testing, to graded interpretations of evidence available from p-values. This update tries to further refine the presentation of interpretations of these hypothesis testing results. And continues to evolve and respond to changes in the R software that impact the methods and results that are provided here.</p>

<p>We have made every attempt to keep costs for the book as low as possible by making it possible for most pages to be printed in black and white. The printed text is available from the Montana State University Bookstore. The text (in full color and with dynamic links) is also available as a free digital download from Montana State University’s ScholarWorks repository at <a href="https://scholarworks.montana.edu/xmlui/handle/1/2999" class="uri">https://scholarworks.montana.edu/xmlui/handle/1/2999</a>.</p>
<p>You may also access the online version of the book from the couse website:
<a href="http://www.math.montana.edu/courses/s217/documents/_book/" class="uri">http://www.math.montana.edu/courses/s217/documents/_book/</a>.</p>
<p>Enjoy your journey from introductory to intermediate statistics!</p>
<p><img src="frontMatter/creative_commons_license.png" width="44" style="display: block; margin: auto;" /></p>
<p>This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" class="uri">http://creativecommons.org/licenses/by-nc-nd/4.0/</a> or send a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.</p>


<p>#Preface {#chapter1}</p>
<p>This book is designed primarily for use in a second semester
statistics course although it can also be
useful for researchers needing a quick review or ideas for using R for the
methods discussed in the text. As a text primarily designed for a second
statistics course, it presumes that you have had an introductory statistics
course. There are now many different varieties of introductory statistics from
traditional, formula-based courses (called “consensus” curriculum courses) to
more modern, computational-intensive courses that use randomization ideas to try to
enhance learning of basic statistical methods. We are not going to presume that
you have had a particular “flavor” of introductory statistics or that you had
your introductory statistics out of a particular text, just that you have had a
course that tried to introduce you to the basic terminology and ideas
underpinning statistical reasoning. We would expect that you are familiar with
the logic (or sometimes illogic) of hypothesis testing including null and
alternative hypothesis  and confidence interval construction and interpretation
and that you have seen all of this in a couple of basic situations. We start
with a review of these ideas in one and two group situations with a
quantitative response, something that you should have seen before.</p>
<p>This text covers a wide array of statistical tools that are connected through situation, methods used,
or both. As we explore various techniques, look for the identifying characteristics
of each method – what type of research questions are being addressed
(relationships or group differences, for example) and what type of variables
are being analyzed (quantitative or categorical). <strong><em>Quantitative variables</em></strong>  are made up of numerical measurements that have meaningful units attached to
them. <strong><em>Categorical variables</em></strong>  take on values that are categories or labels.
Additionally, you will need to carefully identify the <strong><em>response</em></strong>  and <strong><em>explanatory</em></strong>  variables, where
the study and variable characteristics should suggest which variables should be used
as the explanatory variables that may explain
variation in the response variable. Because this is an intermediate statistics
course, we will start to handle more complex situations (many explanatory
variables) and will provide some tools for graphical explorations to complement
the more sophisticated statistical models required to handle these situations.</p>
<p>##Overview of methods {#section1-1}</p>
<p>After you are introduced to basic statistical ideas, a wide array of statistical methods become
available. The methods explored here focus on assessing (estimating and testing
for) relationships between variables, sometimes when controlling for or
modifying relationships based on levels of another variable – which is where statistics gets interesting and really useful. Early statistical analyses (approximately 100 years ago) were
focused on describing a single variable. Your introductory statistics course
should have heavily explored methods for summarizing and doing inference in
situations with one group or where you were comparing results for two groups of
observations. Now, we get to consider more complicated situations – culminating
in a set of tools for working with multiple explanatory variables, some of
which might be categorical and related to having different groups of subjects
that are being compared. Throughout the methods we will cover, it will be
important to retain a focus on how the appropriate statistical analysis depends
on the research question and data collection process as well as the types of
variables measured.</p>
<p>Figure <a href="acknowledgments.html#fig:Figure1-1">0.1</a> frames the topics we will discuss. Taking a broad
view of the methods we will consider,
there are basically two scenarios – one when the response is quantitative and
one when the response is categorical. Examples of quantitative responses we will
see later involve <em>suggested jail sentence</em> (in years) and <em>body fat</em> (percentage).
Examples of categorical variables include <em>improvement</em> (none, some, or marked)
in a clinical trial or whether a student has turned in copied work
(never, done this on an exam or paper, or both). There are going to be some more
nuanced aspects to all these analyses as the complexity of both sides of Figure
<a href="acknowledgments.html#fig:Figure1-1">0.1</a> suggest, but note that near the bottom, each tree converges
on a single
procedure, using a <strong><em>linear model</em></strong>  for a quantitative response variable or
using a <strong><em>Chi-square test</em></strong> for a categorical response.  After selecting the
appropriate procedure and completing the necessary technical steps to get results
for a
given data set, the final step involves assessing the scope of inference
 and
types of conclusions that are appropriate based on the design of the study.</p>

<div class="figure"><span id="fig:Figure1-1"></span>
<img src="chapter1_files/image002.png" alt="Flow chart of methods." width="661" />
<p class="caption">
Figure 0.1: Flow chart of methods.
</p>
</div>
<p>We will be spending most of the semester working on methods for quantitative
response variables (the
left side of Figure <a href="acknowledgments.html#fig:Figure1-1">0.1</a> is covered in Chapters <a href="#chapter2"><strong>??</strong></a>,
<a href="#chapter3"><strong>??</strong></a>, <a href="chapter4.html#chapter4">1</a>, <a href="chapter6.html#chapter6">3</a>, <a href="chapter7.html#chapter7">4</a>, and
<a href="chapter8.html#chapter8">5</a>), stepping
over to handle the situation with a categorical response variable in Chapter <a href="chapter5.html#chapter5">2</a> (right side
of Figure <a href="acknowledgments.html#fig:Figure1-1">0.1</a>).
Chapter <a href="chapter9.html#chapter9">6</a> contains case studies
illustrating all the methods discussed previously, providing a final opportunity
to explore additional examples that illustrate how finding a
path through Figure <a href="acknowledgments.html#fig:Figure1-1">0.1</a> can lead to the appropriate analysis.</p>
<p>The first topics (Chapters <a href="#chapter1"><strong>??</strong></a>, and <a href="#chapter2"><strong>??</strong></a>) will be more
familiar as we start with single and two group situations
with a quantitative response. In your previous statistics course, you should
have seen methods for estimating and quantifying uncertainty for the mean of a
single group and for differences in the means of two groups. Once we have briefly
reviewed these methods and introduced the statistical software that we will use
throughout the course, we will consider the first new statistical material in
Chapter <a href="#chapter3"><strong>??</strong></a>. It involves the situation with a quantitative response
variable where
there are more than 2 groups to compare – this is what we call the <strong><em>One-Way
ANOVA</em></strong> situation. It generalizes the 2-independent sample hypothesis
test to handle situations where more than 2 groups are being studied. When we
learn this method, we will begin discussing model assumptions  and methods for
assessing those assumptions that will be present in every analysis involving a
quantitative response. The <strong><em>Two-Way ANOVA</em></strong> (Chapter <a href="#chapter3"><strong>??</strong></a>)
considers situations with two categorical explanatory variables and a
quantitative response. To make
this somewhat concrete, suppose we are interested in assessing differences in,
say, the <em>yield</em> of wheat from a field based on the amount of <em>fertilizer</em> applied
(none, low, or high) and <em>variety</em> of wheat (two types). Here, <em>yield</em> is a quantitative response variable that might be measured in bushels per acre and
there are two categorical explanatory variables, <em>fertilizer</em>, with 3 levels, and <em>variety</em>, with two levels. In this material, we introduce the idea of an
<strong><em>interaction</em></strong> between the two explanatory variables:  the relationship between one categorical
variable and the mean of the response changes depending on the levels of the
other categorical variable. For example, extra fertilizer might enhance the
growth of one variety and hinder the growth of another so we would say that <em>fertilizer</em> has different impacts based on the level of <em>variety</em>. Given this interaction may or may not actually be present, we will consider two versions of the model in Two-Way ANOVAs,  what are called the <strong><em>additive</em></strong>  (no interaction) and the <strong><em>interaction</em></strong>  models.</p>
<p>Following the methods for two categorical variables and a quantitative response, we explore a method for
analyzing data where the response is categorical, called the <strong><em>Chi-square test</em></strong>
in Chapter <a href="chapter5.html#chapter5">2</a>. This most closely matches the One-Way ANOVA
situation with a single categorical explanatory variable, except now the
response variable is categorical. For example, we will assess whether taking a
drug (vs taking a <strong><em>placebo</em></strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>)
has an <strong><em>effect</em></strong><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> on the type of improvement the subjects demonstrate. There
are two different scenarios
for study design that impact the analysis technique and hypotheses tested in
Chapter <a href="chapter5.html#chapter5">2</a>. If the explanatory variable reflects the group that
subjects were
obtained from, either through randomization of the treatment level to the
subjects or by taking samples from separate populations, this is called a
<strong><em>Chi-square Homogeneity Test</em></strong>.  It is also possible to obtain a single sample
from a population and then obtain information on the levels of the explanatory
variable for each
subject. We will analyze these results using what is called a <strong><em>Chi-square Independence Test</em></strong>.
 They both use the same test statistic but we use slightly different graphics and are testing different hypotheses in these two related
situations. Figure <a href="acknowledgments.html#fig:Figure1-1">0.1</a> also shows that if we had a quantitative explanatory
variable and a categorical response that we would need to “bin” or create
categories of responses from the quantitative variable to use the Chi-square
testing methods.</p>
<p>If the predictor and response variables are both quantitative, we start with
scatterplots, correlation,
and <strong><em>simple linear regression</em></strong> models (Chapters <a href="chapter6.html#chapter6">3</a> and
<a href="chapter7.html#chapter7">4</a>) – things you should have seen, at least to some degree,
previously. The biggest differences here will be
the depth of exploration of diagnostics and inferences for this model and
discussions of transformations of variables.  If there is more than one
explanatory variable, then we say that we are doing <strong><em>multiple linear regression</em></strong>
(Chapter <a href="chapter8.html#chapter8">5</a>) – the “multiple” part of the name reflects that there will
be more
than one explanatory variable. We use the same name if we have a mix of
categorical and quantitative predictor variables but there are some new issues
in setting up the models and interpreting the coefficients that we need to
consider. In the situation with one categorical predictor and one quantitative
predictor, we revisit the idea of an interaction.

It allows us to consider situations
where the estimated relationship between a quantitative predictor and the
mean response
varies among different levels of the categorical variable.</p>
<p>By the end of Chapter <a href="chapter9.html#chapter9">6</a> you should be able to identify, perform
using the statistical software R <span class="citation">(R Core Team <a href="#ref-R-base" role="doc-biblioref">2019</a>)</span>, and interpret the results from each of these methods. There
is a lot to learn, but many of the tools for using R and interpreting results
of the analyses accumulate and repeat during the semester. If you work hard to
understand the initial methods, it will help you when the methods get more
complicated. You will likely feel like you are just starting to learn how to
use R at the end of the semester and for learning a new language that is
actually an accomplishment. We will just be taking you on the first steps of a
potentially long journey and it is up to you to decide how much further you
want to go with learning the software.</p>
<p>All the methods you will learn require you to carefully consider how the data were collected, how that
pertains to the population of interest, and how that impacts the inferences
that can be made. The <strong><em>scope of inference</em></strong> from the bottom of Figure
<a href="acknowledgments.html#fig:Figure1-1">0.1</a> is our shorthand term for remembering to think about two aspects
of the study – <strong><em>random assignment</em></strong> and <strong><em>random sampling</em></strong>.
  In a given
situation, you need to use the description of the study to decide if the
explanatory variable was randomly assigned to study units (this allows for <strong><em>causal inferences</em></strong>  if differences are detected) or not (so no causal statements
are possible). As an example, think about two studies, one where students are
randomly assigned to either get tutoring with their statistics course or not
and another where the students are asked at the end of the semester whether
they sought out tutoring or not. Suppose we compare the final grades in the
course for the two groups (tutoring/not) and find a big difference. In the
first study with random assignment,  we can say the tutoring caused the
differences we observed. In the second, we could only say that the tutoring was
associated with differences but because students self-selected the group they
ended up in, we can’t say that the tutoring caused the differences. The other
aspect of scope of inference concerns random sampling: If the data were obtained
using a random sampling mechanism, then our inferences can be safely extended
to the population that the sample was taken from. However, if we have a non-random
sample, our inference can only apply to the sample collected. In the previous
example, the difference would be studying a random sample of students from the
population of, say, Introductory Statistics students at a university vs
studying a sample of students that volunteered for the research project, maybe
for extra credit in the class. We could still randomly assign them to
tutoring/not but the non-random sample would only lead to conclusions about
those students that volunteered. The most powerful scope of inference is when there
are randomly assigned levels of explanatory variables with a random sample from
a population – conclusions would be about causal impacts that would happen in the
population.</p>
<p>By the end of this material, you should have some basic R skills and abilities to create basic ANOVA and
Regression models, as well as to handle Chi-square testing situations.
Together, this should prepare you for future statistics courses or for other
situations where you are expected to be able to identify an appropriate
analysis, do the calculations for a given data set, and then effectively
communicate interpretations for the methods discussed here.</p>
<p>##Getting started in R {#section1-2}</p>
<p>You will need to download the statistical software package called R and an enhanced interface to R called
RStudio <span class="citation">(RStudio Team <a href="#ref-RStudio" role="doc-biblioref">2018</a>)</span>. They are open source and free to download and use
(and will always be that way). This means that the skills you learn now can
follow you the rest of your life. R is becoming the primary language of
statistics and is being adopted across academia, government, and businesses to
help manage and learn from the growing volume of data being obtained. Hopefully
you will get a sense of some of the power of R in this book.</p>
<p>The next pages will walk you through the process of getting the software downloaded and provide you with
an initial experience using RStudio to do things that should look familiar
even though the interface will be a new experience. Do not expect to master R
quickly – it takes years (sorry!) even if you know the statistical methods
being used. We will try to keep all your interactions with R code in a similar
code format and that should help you in learning how to use R as we move
through various methods. We will also usually provide you with example code. Everyone
that learns R starts with copying other people’s code and then making changes
for specific applications – so expect to go back to examples from the text and focus
on learning how to modify that code to work for your particular data set. Only
really experienced R users “know” functions without having to check other
resources. After we complete this basic introduction, Chapter <a href="#chapter2"><strong>??</strong></a> begins doing
more sophisticated things with R, allowing us to compare quantitative responses
from two groups, make some graphical displays, do hypothesis testing  and create
confidence intervals in a couple of different ways.</p>
<p>You will have two downloading activities to complete before you can do anything
more than read this book<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. First, you need to download R. It is the engine that will do all the computing
for us, but you will only interact with it once. Go to <a href="http://cran.rstudio.com" class="uri">http://cran.rstudio.com</a>
and click on the “<strong>Download R for…</strong>” button that
corresponds to your operating system. On the next page, click on “<strong>base</strong>” and then it will take you
to a screen to download the most current version of R that is compiled for your
operating system, something like “<strong>Download R 3.5.1 for Windows</strong>”. Click on that link and then open
the file you downloaded. You will need to select your preferred language (choose English so your instructor can help you), then hit “<strong>Next</strong>”
until it starts to unpack and install the program (all the base settings will be fine). After you hit “<strong>Finish</strong>” you will not do anything further with R directly.</p>
<p>Second, you need to download RStudio. It is an enhanced interface that will make interacting with
R less frustrating. To download RStudio, go near the bottom of <a href="https://www.rstudio.com/products/rstudio/download/" class="uri">https://www.rstudio.com/products/rstudio/download/</a> and select the correct version under
“Installers for Supported Platforms” for your operating system. Download and
then install RStudio using the installer. From this point forward, you should only
open RStudio; it provides your interface with R. Note that both R and RStudio
are updated frequently (up to four times a year) and if you downloaded either
more than a few months previously, you should download the up-to-date versions,
especially if something you are trying to do is not working. Sometimes code
will not work in older versions of R and sometimes old code won’t work in new
versions of R.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>To get started, we can complete some basic tasks in R using the RStudio
interface. When you open RStudio, you will see a screen like Figure
<a href="acknowledgments.html#fig:Figure1-2">0.2</a>. The
added annotation in this and the following screen-grabs is there to help you
get initially oriented to the software interface. R is command-line software –
meaning that most of the time you have to create code and then enter and execute
it at a command prompt to get any results. RStudio makes the management and
execution of that code more efficient than the basic version of R. In RStudio,
the lower left panel is called the “console” window and is where you can type R
code directly into R or where you will see the code you run and (most
importantly!) where the results of your executed commands will show up. The
most basic interaction with R is available once you get the cursor active at
the command prompt “&gt;” by clicking in that panel (look for a blinking
vertical line). The upper left panel is for writing, saving, and running your R
code. Once you have code available in this window, the “Run” button will
execute the code for the line that your cursor is on or for any text that you
have highlighted with your mouse. The “data management” or environment panel is
in the upper right, providing information on what data sets have been loaded.
It also contains the “Import Dataset” button that provides the easiest way for
you to read a data set into R so you can analyze it. The lower right panel
contains information on the “Packages” (additional code we will download and
install to add functionality to R) that are available and is where you will see
plots that you make and requests for “Help” on specific functions.</p>

<div class="figure"><span id="fig:Figure1-2"></span>
<img src="chapter1_files/fig1.2.png" alt="Initial RStudio layout." width="492" />
<p class="caption">
Figure 0.2: Initial RStudio layout.
</p>
</div>
<p>As a first interaction with R we can use it as a calculator. To do this, click near the command prompt
(<code>&gt;</code>) in the lower left “console” panel, type 3+4, and then hit enter. It
should look like this:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span><span class="op">+</span><span class="dv">4</span>
[<span class="dv">1</span>] <span class="dv">7</span></code></pre>
<p>You can do more interesting calculations, like finding the mean of the
numbers -3, 5, 7, and 8 by adding them up and dividing by 4:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span>(<span class="op">-</span><span class="dv">3</span><span class="op">+</span><span class="dv">5</span><span class="op">+</span><span class="dv">7</span><span class="op">+</span><span class="dv">8</span>)<span class="op">/</span><span class="dv">4</span>
[<span class="dv">1</span>] <span class="fl">4.25</span></code></pre>
<p>Note that the parentheses help R to figure out your desired order of operations. If you drop that grouping, you get
a very different (and wrong!) result:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="dv">-3</span><span class="op">+</span><span class="dv">5</span><span class="op">+</span><span class="dv">7</span><span class="op">+</span><span class="dv">8</span><span class="op">/</span><span class="dv">4</span>
[<span class="dv">1</span>] <span class="dv">11</span></code></pre>
<p>We could estimate the standard deviation similarly using the formula you might remember from introductory
statistics, but that will only work in very limited situations. To use the real
power of R this semester, we need to work with data sets that store the
observations for our subjects in <em>variables</em>.
Basically, we need to store observations in named vectors (one dimensional
arrays) that contain a list of the observations. To create a vector containing
the four numbers and assign it to a variable named <em>variable1</em>, we need to
create a vector using the function
<code>c</code> which means “combine the items” that follow, if they are inside
parentheses and have commas separating the values,
as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">8</span>)
[<span class="dv">1</span>] <span class="dv">-3</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">8</span></code></pre>
<p>To get this vector stored in a variable called <em>variable1</em> we need to
use the assignment operator, <code>&lt;-</code> (read as “is defined to contain”) that assigns
the information on the right into the variable that you are creating on
the left.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span>variable1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">8</span>)</code></pre>
<p>In R, the assignment operator, <code>&lt;-</code>, is created by typing a
“less than” symbol <code>&lt;</code> followed by a “minus” sign (<code>-</code>)
<strong>without a space between them</strong>. If you
ever want to see what numbers are residing in an object in R, just type
its name and hit <em>enter</em>. You can see how that variable contains the same
information that was initially generated by
<code>c(-3, 5, 7, 8)</code> but is easier to access since we just need the text
for the variable name representing that vector.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span>variable1
[<span class="dv">1</span>] <span class="dv">-3</span> <span class="dv">5</span> <span class="dv">7</span> <span class="dv">8</span></code></pre>
<p>With the data stored in a variable, we can use functions such as
<code>mean</code> and
<code>sd</code> to find the mean and standard deviation of the observations contained in
<code>variable1</code>:

</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(variable1)
[<span class="dv">1</span>] <span class="fl">4.25</span>
<span class="op">&gt;</span><span class="st"> </span><span class="kw">sd</span>(variable1)
[<span class="dv">1</span>] <span class="fl">4.99166</span></code></pre>
<p>When dealing with real data, we will often have information about more than one
variable. We could enter all observations by hand for each variable but this is
prone to error and onerous for all but the smallest data sets. If you are to
ever utilize the power of statistics in the evolving data-centered world, data
management has to be accomplished in a more sophisticated way. While you can
manage data sets quite effectively in R, it is often easiest to start with your
data set in something like Microsoft Excel or OpenOffice’s Calc. You want to
make sure that observations are in the rows and the names of variables are in
the columns and that there is no “extra stuff” in the spreadsheet. If you have
missing observations, they should be represented with blank cells. The file should
be saved as a “.csv” file (stands for comma-separated values although Excel
calls it “CSV (Comma Delimited)”), which basically strips off some of the junk
that Excel adds to the necessary information in the file. Excel will tell you that
this is a bad idea, but it actually creates a more stable archival format and
one that R can use directly.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p>The following code to read in the data set relies on an R package called
<code>readr</code> <span class="citation">(Wickham, Hester, and Francois <a href="#ref-R-readr" role="doc-biblioref">2018</a>)</span>. Packages in R provide additional functions and data sets that
are not available in the initial download of R or RStudio. To get access to the packages,
first “install” (basically
download) and then “load” the package. To install an R package, go to the <strong>Packages</strong>
tab in the lower right panel of
RStudio. Click on the <strong>Install</strong> button and then type in the name of the package in
the box (here type in <code>readr</code>).

RStudio will try to auto-complete the package name
you are typing which should help you make sure you got it typed correctly. This will
be the first of <em>many</em> times that we will mention that R is case sensitive – in
other words, <code>Readr</code> is different from <code>readr</code> in R syntax and this sort of
thing applies to everything you do in R. You should only need to install each R
package once on a given computer. If you ever see a message that R can’t find a
package, make sure it appears in the list in the <strong>Packages</strong> tab. If it
doesn’t, repeat the previous steps to install it.</p>
<table>
<tbody>
<tr class="odd">
<td><strong>Important</strong>: R is case sensitive! <code>Readr</code> is not the same as <code>readr</code>!</td>
</tr>
</tbody>
</table>
<p>After installing the package, we need to load it to make it active in a given work
session. Go to the command prompt and type (or copy and paste) <code>require(readr)</code> or <code>library(readr)</code>:
</p>
<pre><code>&gt; require(readr)</code></pre>
<p>With a data set converted to a CSV file and <code>readr</code> installed and loaded, we need to read the data set into the active workspace.

There are two ways to do this, either using the point-and-click GUI in RStudio (click
the “Import Dataset” button in the upper right “Environment” panel as
indicated in Figure <a href="acknowledgments.html#fig:Figure1-2">0.2</a>) or modifying the <code>read_csv</code>
function to find the file of interest. To practice this, you can
download an Excel (.xls) file from
<a href="http://www.math.montana.edu/courses/s217/documents/treadmill.xls" class="uri">http://www.math.montana.edu/courses/s217/documents/treadmill.xls</a>
that contains observations on 31 males that volunteered for a study on methods
for measuring fitness <span class="citation">(Westfall and Young <a href="#ref-Westfall1993" role="doc-biblioref">1993</a>)</span>.
In the spreadsheet, you will find a data set that
starts and ends with the following information (only results for Subjects 1, 2,
30, and 31 shown here):</p>
<table>
<colgroup>
<col width="9%" />
<col width="13%" />
<col width="17%" />
<col width="11%" />
<col width="13%" />
<col width="10%" />
<col width="18%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Sub-
ject</th>
<th align="left">Tread-
MillOx</th>
<th align="left">TreadMill-
MaxPulse</th>
<th align="right">RunTime</th>
<th align="right">RunPulse</th>
<th align="left">Rest
Pulse</th>
<th align="right">BodyWeight</th>
<th align="right">Age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">60.05</td>
<td align="left">186</td>
<td align="right">8.63</td>
<td align="right">170</td>
<td align="left">48</td>
<td align="right">81.87</td>
<td align="right">38</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">59.57</td>
<td align="left">172</td>
<td align="right">8.17</td>
<td align="right">166</td>
<td align="left">40</td>
<td align="right">68.15</td>
<td align="right">42</td>
</tr>
<tr class="odd">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="right">…</td>
<td align="right">…</td>
<td align="left">…</td>
<td align="right">…</td>
<td align="right">…</td>
</tr>
<tr class="even">
<td align="left">30</td>
<td align="left">39.2</td>
<td align="left">172</td>
<td align="right">12.88</td>
<td align="right">168</td>
<td align="left">44</td>
<td align="right">91.63</td>
<td align="right">54</td>
</tr>
<tr class="odd">
<td align="left">31</td>
<td align="left">37.39</td>
<td align="left">192</td>
<td align="right">14.03</td>
<td align="right">186</td>
<td align="left">56</td>
<td align="right">87.66</td>
<td align="right">45</td>
</tr>
</tbody>
</table>
<p>The variables contain information on the subject number (<em>Subject</em>), subjects’
maximum treadmill oxygen consumption (<em>TreadMillOx</em>, in ml per kg per minute, also called maximum VO2) and
maximum pulse rate (<em>TreadMillMaxPulse</em>, in beats per minute), time to run 1.5
miles (<em>Run Time</em>, in minutes), maximum pulse
during 1.5 mile run (<em>RunPulse</em>, in beats per minute), resting pulse rate
(<em>RestPulse</em>, beats per minute), Body Weight (<em>BodyWeight</em>, in kg), and <em>Age</em>
(in years). Open the file in Excel or equivalent software and then save it as
a .csv file in a location you can find on your computer. Then go to RStudio
and click on <strong>File</strong>, then <strong>Import Dataset</strong>, then <strong>From Text (readr)…</strong><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>

Find your file and click “<strong>Import</strong>”. R will store the data set as an object with the same name
as the .csv file. You could use another name as well, but it is
often easiest just to keep the data
set name in R related to the original file name. You should see some text appear
in the console (lower left panel) like in Figure <a href="acknowledgments.html#fig:Figure1-3">0.3</a>. The text
that is created
will look something like the following – if you had stored the file in a drive
labeled D:, it would be:</p>
<pre class="sourceCode r"><code class="sourceCode r">treadmill &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;D:/treadmill.csv&quot;</span>)</code></pre>
<p>What is put inside the
<code>" "</code> will depend on the location and name of your saved .csv file. A
version of the data set in what looks like a
spreadsheet will appear in the upper left window due to the second line of
code (<code>View(treadmill</code>)).</p>

<div class="figure"><span id="fig:Figure1-3"></span>
<img src="chapter1_files/fig1.3.png" alt="RStudio with initial data set loaded." width="484" />
<p class="caption">
Figure 0.3: RStudio with initial data set loaded.
</p>
</div>
<p>Just directly typing (or using) a line of code like this is actually the
other way that we can read in
files. If you choose to use the text-only interface, then you need to tell R
where to look in your computer to find the data file. <code>read_csv</code> is a
function that takes a path as an argument. To use it, specify the path to
your data file, put quotes around it, and put it as the input to
<code>read_csv(...)</code>. For some examples later in the book, you will be able to
copy a command like this from the text and read data sets and other
code directly from the course folder, assuming you are connected to the
internet.</p>
<p>To verify that you read the data set in correctly, it is always good to check
its contents. We can view the first and last rows in the data set using the
<code>head</code> and <code>tail</code> functions on the data set, which show the following
results for the
<code>treadmill</code> data. Note that you will sometimes need to resize the console
window in RStudio to get all the columns to display
in a single row which can be performed by dragging the gray bars that separate
the panels.

</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">head</span>(treadmill)
<span class="co"># A tibble: 6 x 8</span>
  Subject TreadMillOx TreadMillMaxPulse RunTime RunPulse RestPulse BodyWeight   Age
    <span class="op">&lt;</span>int<span class="op">&gt;</span><span class="st">       </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st">             </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">   </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st">    </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">     </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">      </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st"> </span><span class="er">&lt;</span>int<span class="op">&gt;</span>
<span class="dv">1</span>       <span class="dv">1</span>       <span class="fl">60.05</span>               <span class="dv">186</span>    <span class="fl">8.63</span>      <span class="dv">170</span>        <span class="dv">48</span>      <span class="fl">81.87</span>    <span class="dv">38</span>
<span class="dv">2</span>       <span class="dv">2</span>       <span class="fl">59.57</span>               <span class="dv">172</span>    <span class="fl">8.17</span>      <span class="dv">166</span>        <span class="dv">40</span>      <span class="fl">68.15</span>    <span class="dv">42</span>
<span class="dv">3</span>       <span class="dv">3</span>       <span class="fl">54.62</span>               <span class="dv">155</span>    <span class="fl">8.92</span>      <span class="dv">146</span>        <span class="dv">48</span>      <span class="fl">70.87</span>    <span class="dv">50</span>
<span class="dv">4</span>       <span class="dv">4</span>       <span class="fl">54.30</span>               <span class="dv">168</span>    <span class="fl">8.65</span>      <span class="dv">156</span>        <span class="dv">45</span>      <span class="fl">85.84</span>    <span class="dv">44</span>
<span class="dv">5</span>       <span class="dv">5</span>       <span class="fl">51.85</span>               <span class="dv">170</span>   <span class="fl">10.33</span>      <span class="dv">166</span>        <span class="dv">50</span>      <span class="fl">83.12</span>    <span class="dv">54</span>
<span class="dv">6</span>       <span class="dv">6</span>       <span class="fl">50.55</span>               <span class="dv">155</span>    <span class="fl">9.93</span>      <span class="dv">148</span>        <span class="dv">49</span>      <span class="fl">59.08</span>    <span class="dv">57</span>

<span class="op">&gt;</span><span class="st"> </span><span class="kw">tail</span>(treadmill)
<span class="co"># A tibble: 6 x 8</span>
  Subject TreadMillOx TreadMillMaxPulse RunTime RunPulse RestPulse BodyWeight   Age
    <span class="op">&lt;</span>int<span class="op">&gt;</span><span class="st">       </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st">             </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">   </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st">    </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">     </span><span class="er">&lt;</span>int<span class="op">&gt;</span><span class="st">      </span><span class="er">&lt;</span>dbl<span class="op">&gt;</span><span class="st"> </span><span class="er">&lt;</span>int<span class="op">&gt;</span>
<span class="dv">1</span>      <span class="dv">26</span>       <span class="fl">44.61</span>               <span class="dv">182</span>   <span class="fl">11.37</span>      <span class="dv">178</span>        <span class="dv">62</span>      <span class="fl">89.47</span>    <span class="dv">44</span>
<span class="dv">2</span>      <span class="dv">27</span>       <span class="fl">40.84</span>               <span class="dv">172</span>   <span class="fl">10.95</span>      <span class="dv">168</span>        <span class="dv">57</span>      <span class="fl">69.63</span>    <span class="dv">51</span>
<span class="dv">3</span>      <span class="dv">28</span>       <span class="fl">39.44</span>               <span class="dv">176</span>   <span class="fl">13.08</span>      <span class="dv">174</span>        <span class="dv">63</span>      <span class="fl">81.42</span>    <span class="dv">44</span>
<span class="dv">4</span>      <span class="dv">29</span>       <span class="fl">39.41</span>               <span class="dv">176</span>   <span class="fl">12.63</span>      <span class="dv">174</span>        <span class="dv">58</span>      <span class="fl">73.37</span>    <span class="dv">57</span>
<span class="dv">5</span>      <span class="dv">30</span>       <span class="fl">39.20</span>               <span class="dv">172</span>   <span class="fl">12.88</span>      <span class="dv">168</span>        <span class="dv">44</span>      <span class="fl">91.63</span>    <span class="dv">54</span>
<span class="dv">6</span>      <span class="dv">31</span>       <span class="fl">37.39</span>               <span class="dv">192</span>   <span class="fl">14.03</span>      <span class="dv">186</span>        <span class="dv">56</span>      <span class="fl">87.66</span>    <span class="dv">45</span></code></pre>

<p>When you require a package, you may see a warning message about versions of the package and versions of
R – this is <em>usually</em> something you can ignore. Other warning messages could
be more ominous for proceeding but before getting too concerned, there are
couple of basic things to check.

First, double check that the package is
installed (see
previous steps). Second, check for typographical errors in your code –
especially for mis-spellings or unintended capitalization. If you are still
having issues, try repeating the installation process. If that fails, find
someone more used to using R to help you (for example in the Math Learning
Center or by emailing your instructor).<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>To help you go from basic to intermediate R usage and especially to help with more
complicated problems, you will want to learn how to manage and save your R code.
The best way to do this is using the upper left panel in RStudio using what
are called R Scripts, which are files that have a file extension of “.R”. To
start a new “.R” file to store your code, click on <strong>File</strong>, then
<strong>New File</strong>, then <strong>R Script</strong>. This will create a blank page to enter and
edit code – then save the file as something like “MyFileName.R” in your preferred location.
Saving your code will mean that you can return to where you
were working last by simply re-running the saved script file. With code in the
script window, you can place the cursor on a line of code or highlight a chunk
of code and hit the “Run” button<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>
on the upper part of the panel. It will appear
in the console with results just like what you would obtain if you typed it
after the command prompt and hit enter for each line. Figure <a href="acknowledgments.html#fig:Figure1-4">0.4</a>
shows the screen with the code used in this
section in the upper left panel, saved in
a file called “CH0.R”, with the results of highlighting and executing the first
section of code using the “Run” button.</p>

<div class="figure"><span id="fig:Figure1-4"></span>
<img src="chapter1_files/fig1.4.png" alt="RStudio with highlighted code run." width="1368" />
<p class="caption">
Figure 0.4: RStudio with highlighted code run.
</p>
</div>
<p>##Basic summary statistics, histograms, and boxplots using R {#section1-3}</p>
<p>For the following material, you will need to install and load the <code>mosaic</code> package <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaic" role="doc-biblioref">2019</a>)</span>.</p>
<p></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">require</span>(mosaic)</code></pre>
<p>It provides a suite of enhanced functions to aid our initial explorations. With RStudio running, the <code>mosaic</code> package loaded, a place to write and
save code, and the <code>treadmill</code> data set loaded, we can (finally!) start to
summarize the results of the study. The <code>treadmill</code> object is what R calls a
<strong><em>tibble</em></strong><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> and contains columns corresponding to each variable in
the spreadsheet. Every
function in R will involve specifying the variable(s) of interest and how you
want to use them. To access a particular variable (column) in a tibble, you
can use a $ between the name of the tibble and the name of the variable of
interest, generically as <code>tibblename$variablename</code>. You can think of this as <em>tibblename’s variablename</em> where the <em>’s</em> is replaced by the dollar sign. To identify the
<code>RunTime</code> variable here it would be <code>treadmill$RunTime</code>. In the command line it would look like:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span>treadmill<span class="op">$</span>RunTime
[<span class="dv">1</span>]  <span class="fl">8.63</span>  <span class="fl">8.17</span>  <span class="fl">8.92</span>  <span class="fl">8.65</span> <span class="fl">10.33</span>  <span class="fl">9.93</span> <span class="fl">10.13</span> <span class="fl">10.08</span>  <span class="fl">9.22</span>  <span class="fl">8.95</span> <span class="fl">10.85</span>  <span class="fl">9.40</span> <span class="fl">11.50</span> <span class="fl">10.50</span>
[<span class="dv">15</span>] <span class="fl">10.60</span> <span class="fl">10.25</span> <span class="fl">10.00</span> <span class="fl">11.17</span> <span class="fl">10.47</span> <span class="fl">11.95</span>  <span class="fl">9.63</span> <span class="fl">10.07</span> <span class="fl">11.08</span> <span class="fl">11.63</span> <span class="fl">11.12</span> <span class="fl">11.37</span> <span class="fl">10.95</span> <span class="fl">13.08</span>
[<span class="dv">29</span>] <span class="fl">12.63</span> <span class="fl">12.88</span> <span class="fl">14.03</span></code></pre>
<p>Just as in the previous section, we can generate summary statistics using functions like <code>mean</code> and <code>sd</code> by running them on a specific variable:

</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(treadmill<span class="op">$</span>RunTime)
[<span class="dv">1</span>] <span class="fl">10.58613</span>
<span class="op">&gt;</span><span class="st"> </span><span class="kw">sd</span>(treadmill<span class="op">$</span>RunTime)
[<span class="dv">1</span>] <span class="fl">1.387414</span></code></pre>
<p>And now we know that the average running time for 1.5 miles for the subjects in the study was 10.6 minutes with a standard deviation (SD) of 1.39 minutes. But you should remember that the
mean and SD are only appropriate summaries if the distribution is roughly
<strong><em>symmetric</em></strong> (both sides of the distribution are approximately the same shape and length). The
<code>mosaic</code> package provides a useful function called <code>favstats</code> that provides
the mean and SD as well as the <strong><em>5 number summary</em></strong>: 
the minimum (<code>min</code>), the first quartile (<code>Q1</code>, the 25<sup>th</sup> percentile),
the median (50<sup>th</sup> percentile), the third quartile (<code>Q3</code>, the 75<sup>th</sup>
percentile), and the maximum (<code>max</code>). It also provides the number of
observations (<code>n</code>) which was 31, as noted above, and a count of whether any
missing values were encountered (<code>missing</code>), which was 0 here since all
subjects had measurements available on this variable.
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">favstats</span>(treadmill<span class="op">$</span>RunTime)
  min   Q1 median    Q3   max     mean       sd  n missing
 <span class="fl">8.17</span> <span class="fl">9.78</span>  <span class="fl">10.47</span> <span class="fl">11.27</span> <span class="fl">14.03</span> <span class="fl">10.58613</span> <span class="fl">1.387414</span> <span class="dv">31</span>       <span class="dv">0</span></code></pre>
<p>We are starting to get somewhere with understanding that the runners were
somewhat fit with worst runner covering 1.5 miles in 14 minutes
(the equivalent of a 9.3 minute mile)
and the best running at a 5.4 minute mile pace. The limited variation in the
results suggests that the sample was obtained from a restricted group with
somewhat common characteristics. When you explore the ages and weights of the
subjects in the Practice Problems in Section <a href="#section1-6"><strong>??</strong></a>, you will get even more
information about how similar all the subjects in this study were. Researchers often publish numerical summaries of this sort of demographic information to help readers understand the subjects that they studied and that their results might apply to.</p>
<p>A graphical display of these results will help us to assess the shape
of the distribution of run times – including considering the potential for the presence of a <strong><em>skew</em></strong> (whether the right or left tail of the distribution
is noticeably more spread out, with left skew meaning that the left tail
is more spread out than the right tail)  and <strong><em>outliers</em></strong> 
(unusual observations). A <strong><em>histogram</em></strong>  is a good place to start.
Histograms display connected bars with counts of observations defining
the height of bars based on a set of bins of values of the quantitative variable.
We will apply the <code>hist</code> function to the <code>RunTime</code> variable, which produces
Figure <a href="acknowledgments.html#fig:Figure1-5">0.5</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">hist</span>(treadmill<span class="op">$</span>RunTime)</code></pre>

<div class="figure"><span id="fig:Figure1-5"></span>
<img src="01-preface_files/figure-html/Figure1-5-1.png" alt="Histogram of Run Times (minutes) of \(n\)=31 subjects in Treadmill study, bar heights are counts." width="480" />
<p class="caption">
Figure 0.5: Histogram of Run Times (minutes) of <span class="math inline">\(n\)</span>=31 subjects in Treadmill study, bar heights are counts.
</p>
</div>
<p>You can save this plot by clicking on the <strong>Export</strong> button found above
the plot, followed by <strong>Copy to Clipboard</strong> and clicking on the
<strong>Copy Plot</strong> button. Then if you open your
favorite word-processing program, you should be able to paste it into a
document for writing reports that include the figures. You can see the first
parts of this process in the screen grab in Figure <a href="acknowledgments.html#fig:Figure1-6">0.6</a>. You can also directly save the figures as separate files using
<strong>Save as Image</strong> or <strong>Save as PDF</strong> and then insert them into your word
processing documents.</p>

<div class="figure"><span id="fig:Figure1-6"></span>
<img src="chapter1_files/image010.png" alt="RStudio while in the process of copying the histogram." width="960" />
<p class="caption">
Figure 0.6: RStudio while in the process of copying the histogram.
</p>
</div>
<p>The function <code>hist</code> defaults into providing a histogram on the <strong><em>frequency</em></strong>
(count) scale. In most R functions, there are the default options that will
occur if we don’t make any specific choices but we
can override the default options if we desire. One option we can modify here is
to add labels to the bars to be able to see exactly how many observations fell
into each bar. Specifically, we can turn the <code>labels</code> option “on” by making it true (“T”) by adding <code>labels=T</code> to the previous call to the <code>hist</code> function, separated by a comma. Note that we will use the <code>=</code> sign only for changing options within functions.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">hist</span>(treadmill<span class="op">$</span>RunTime, <span class="dt">labels=</span>T)</code></pre>

<div class="figure"><span id="fig:Figure1-7"></span>
<img src="01-preface_files/figure-html/Figure1-7-1.png" alt="Histogram of Run Times with counts in bars labeled." width="384" />
<p class="caption">
Figure 0.7: Histogram of Run Times with counts in bars labeled.
</p>
</div>
<p>Based on this histogram, it does not appear that there any outliers in the responses
since there are no bars that are separated from the other observations. However,
the distribution does not look symmetric and there might be a skew to the
distribution. Specifically, it appears to be <strong><em>skewed right</em></strong> (the right tail is longer than the left). But histograms can sometimes mask features of
the data set by binning observations and it is hard to find the percentiles
accurately from the plot.</p>
<p>When assessing outliers and skew, the <strong><em>boxplot</em></strong>
(or <em>Box and Whiskers</em> plot) can also be helpful (Figure <a href="acknowledgments.html#fig:Figure1-8">0.8</a>) to describe the
shape of the distribution as it displays the 5-number summary and will also indicate
observations that are “far” above the middle of the observations.

R’s <code>boxplot</code> function uses the standard rule to indicate an observation as a
<strong><em>potential outlier</em></strong> if it falls more than 1.5 times the <strong><em>IQR</em></strong>
(Inter-Quartile Range, calculated as Q3 – Q1) below Q1 or above Q3.

The potential outliers
are plotted with circles and the <em>Whiskers</em> (lines that extend from Q1 and Q3 typically to
the minimum and maximum) are shortened to only go as far as
observations that are within <span class="math inline">\(1.5*\)</span>IQR of the upper and lower quartiles. The <em>box</em>
part of the boxplot is a box that goes from Q1 to Q3 and the median is displayed as a line
somewhere inside the box.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Looking back at the summary statistics above, Q1=9.78 and Q3=11.27, providing an IQR of:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span>IQR &lt;-<span class="st"> </span><span class="fl">11.27</span> <span class="op">-</span><span class="st"> </span><span class="fl">9.78</span>
<span class="op">&gt;</span><span class="st"> </span>IQR
[<span class="dv">1</span>] <span class="fl">1.49</span></code></pre>
<p>One observation (the maximum value of 14.03) is indicated as a potential outlier
based on this result by being larger than Q3 <span class="math inline">\(+1.5*\)</span>IQR, which was 13.505:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="fl">11.27</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.5</span><span class="op">*</span>IQR
[<span class="dv">1</span>] <span class="fl">13.505</span></code></pre>
<p>The boxplot also shows a slight indication of a right skew (skew towards
larger values) with the distance from the minimum to the median being smaller than the
distance from the median to the maximum. Additionally, the distance from Q1 to
the median is smaller than the distance from the median to Q3. It is modest skew,
but worth noting.</p>

<div class="figure"><span id="fig:Figure1-8"></span>
<img src="01-preface_files/figure-html/Figure1-8-1.png" alt="Boxplot of 1.5 mile Run Times." width="384" />
<p class="caption">
Figure 0.8: Boxplot of 1.5 mile Run Times.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">boxplot</span>(treadmill<span class="op">$</span>RunTime)</code></pre>
<p>While the default boxplot is fine, it fails to provide good graphical labels,
especially on the y-axis. Additionally, there is no title on the plot. The
following code provides some enhancements to the plot by using the <code>ylab</code> and
<code>main</code> options in the call to <code>boxplot</code>, with the results displayed in
Figure <a href="acknowledgments.html#fig:Figure1-9">0.9</a>. When we add text to plots, it will be contained within quotes and
be assigned into the options <code>ylab</code> (for y-axis) or <code>main</code>
(for the title) here to put it into those locations.</p>

<div class="figure"><span id="fig:Figure1-9"></span>
<img src="01-preface_files/figure-html/Figure1-9-1.png" alt="Boxplot of Run Times with improved labels." width="384" />
<p class="caption">
Figure 0.9: Boxplot of Run Times with improved labels.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">boxplot</span>(treadmill<span class="op">$</span>RunTime, <span class="dt">ylab=</span><span class="st">&quot;1.5 Mile Run Time (minutes)&quot;</span>, 
          <span class="dt">main=</span><span class="st">&quot;Boxplot of the Run Times of n=31 participants&quot;</span>)</code></pre>
<p>Throughout the book, we will often use extra options to make figures that
are easier for you to understand. There
are often simpler versions of the functions that will suffice but the extra
work to get better labeled figures is often worth it. I guess the point is that
“a picture is worth a thousand words” but in data visualization, that is only
true if the reader can understand what is being displayed. It is also important
to think about the quality of the information that is being displayed,
regardless of how pretty the graphic might be. So maybe it is better to say
“a picture can be worth a thousand words” if it is well-labeled?</p>
<p>All the previous results were created by running the R code and then copying the
results from either the console or by copying the figure and then pasting the results
into the typesetting program. There is another way
to use RStudio where you can have it compile the results (both output and
figures) directly into a document together with the code that generated it,
using what is called R Markdown (<a href="http://shiny.rstudio.com/articles/rmarkdown.html" class="uri">http://shiny.rstudio.com/articles/rmarkdown.html</a>).
It adds some additional setup
complexity we want to avoid for now but is basically what we used to prepare this book.
The main reason to mention this is that you will see a
change in formatting of the R code and output from here forward as you will no
longer see the command prompt (“&gt;”) with the code. The output will be
flagged by having two “##”’s before it. For example, the summary statistics for
the <em>RunTime</em> variable from <code>favstats</code> function would look like:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(treadmill<span class="op">$</span>RunTime)</code></pre>
<pre><code>##   min   Q1 median    Q3   max     mean       sd  n missing
##  8.17 9.78  10.47 11.27 14.03 10.58613 1.387414 31       0</code></pre>
<p>Statisticians (and other scientists) are starting to use these methods
because they provide what is called “Reproducible
research” <span class="citation">(Gandrud <a href="#ref-Gandrud2015" role="doc-biblioref">2015</a>)</span> where all the code and output it produced are
available in a single place. This allows different researchers to run and verify
results or the original researchers to revisit their earlier work at a later
date and recreate all their results exactly. Scientific publications are currently
encouraging researchers to work in this way and may someday require it. In this
book, we focus on the R code and show the results from running it, but you may
want to consider exploring these alternative options. Ask your instructor to show
you this way of working and see if you like it better than copying and pasting everything.</p>
<p>Finally, when you are done with your work and attempt to exit out of RStudio,
it will
ask you to save your workspace. <strong><em>DO NOT DO THIS!</em></strong> It will just create a cluttered
workspace and could even cause you to get incorrect results. If you
save your R code (and edit it to only contain the parts of it that worked) via the
script window, you can re-create any results by simply
re-running that code. If you find that you have lots of “stuff” in your
workspace because you accidentally saved your workspace, just run <code>rm(list = ls())</code>.
It will delete all the data sets from your workspace.</p>
<p>##Chapter summary {#section1-4}</p>
<p>This chapter covered getting R and RStudio downloaded and some basics of working with
R via RStudio. You should be able to read a data set into R and run some basic
functions, all done using the RStudio interface. If you are struggling with
this, you should seek additional help with these technical issues so that you
are ready for more complicated statistical methods that are going to be
encountered in the following chapters. For most assignments, we will give you a
seed of the basic R code that you need and then you will modify it to work on
your data set of interest. As mentioned previously, the way everyone learns R is
by starting with some example code that does most of what you want to do and
then you modify it. If you can complete the Practice Problems that follow, you
are well on your way to learning to use R.</p>
<p>The statistical methods in this chapter were minimal and all should have been
review. They involved a quick reminder of summarizing the center, spread, and
shape of distributions using numerical summaries of the mean and SD and/or the
min, Q1, median, Q3, and max and the histogram and boxplot as graphical
summaries. We revisited the ideas of symmetry and skew. But the main point was
really to get a start on using R to provide results you should be familiar with
from your previous statistics experience(s).</p>
<div id="section1-5" class="section level2">
<h2><span class="header-section-number">0.1</span> Summary of important R code</h2>
<p>To help you learn and use R, there is a section highlighting the most important
R code used near the end of each
chapter. The bold text will never change but the
lighter and/or ALL CAPS text (red in the online or digital version) will need
to be customized to your particular application. The sub-bullet for each
function will discuss the use of the function and pertinent options or packages
required. You can use this as a guide to finding the function names and some
hints about options that will help you to get the code to work. You can also
revisit the worked examples using each of the functions.</p>
<ul>
<li><p><font color='red'>FILENAME</font> <code>&lt;-</code> <strong>read_csv(</strong><font color='red'>“path to csv file/FILENAME.csv”</font><strong>)</strong></p>
<ul>
<li><p>Can be generated using “Import Dataset” button or by modifying this text.</p></li>
<li><p>Requires the <code>readr</code> package to be loaded (<code>require(readr)</code>) when using
the code directly.</p></li>
<li><p>Imports a text file saved in the CSV format.</p></li>
</ul></li>
<li><p><font color='red'>DATASETNAME</font><strong>$</strong><font color='red'>VARIABLENAME</font></p>
<ul>
<li>To access a particular variable in a tibble called DATASETNAME, use
a $ and then the VARIABLENAME.</li>
</ul></li>
<li><p><strong>head(</strong><font color='red'>DATASETNAME</font><strong>)</strong></p>
<ul>
<li>Provides a list of the first few rows of the data set for all the
variables in it. </li>
</ul></li>
<li><p><strong>tail(</strong><font color='red'>DATASETNAME</font><strong>)</strong></p>
<ul>
<li>Provides a list of the last few rows of the data set for all the
variables in it. </li>
</ul></li>
<li><p><strong>mean(</strong><font color='red'>DATASETNAME</font><strong>$</strong><font color='red'>VARIABLENAME</font><strong>)</strong></p>
<ul>
<li>Calculates the mean of the observations in a variable.
</li>
</ul></li>
<li><p><strong>sd(</strong><font color='red'>DATASETNAME</font><strong>$</strong><font color='red'>VARIABLENAME</font><strong>)</strong></p>
<ul>
<li>Calculates the standard deviation of the observations in a variable.
</li>
</ul></li>
<li><p><strong>favstats(</strong><font color='red'>DATASETNAME</font>$<font color='red'>VARIABLENAME</font><strong>)</strong></p>
<ul>
<li><p>Requires the <code>mosaic</code> package to be loaded (<code>require(mosaic</code>) after
installing the package).</p></li>
<li><p>Provides a suite of numerical summaries of the observations in a variable.
</p></li>
</ul></li>
<li><p><strong>hist(</strong><font color='red'>DATASETNAME</font><strong>$</strong><font color='red'>VARIABLENAME</font><strong>)</strong></p>
<ul>
<li>Makes a histogram. </li>
</ul></li>
<li><p><strong>boxplot(</strong><font color='red'>DATASETNAME</font><strong>$</strong><font color='red'>VARIABLENAME</font><strong>)</strong></p>
<ul>
<li>Makes a boxplot. </li>
</ul></li>
</ul>
<p>##Practice problems {#section1-6}</p>
<p>In each chapter, the last section contains some questions for you to complete
to make sure you understood the
material. You can download the code to answer questions 1.1 to 1.5 below at
<a href="http://www.math.montana.edu/courses/s217/documents/Ch1.Rmd" class="uri">http://www.math.montana.edu/courses/s217/documents/Ch1.Rmd</a>. But to practice
learning R, it would be most useful for you to try to accomplish the requested tasks
yourself and then only refer to the provided R code if/when you struggle.
These questions provide a great venue to check your learning, often to see the
methods applied to another data set, and for something to discuss in study groups,
with your instructor, and at the Math Learning Center.</p>
<p>1.1. Read in the treadmill data set
discussed previously and find the mean and SD of the Ages (<code>Age</code> variable) and Body
Weights (<code>BodyWeight</code> variable). In studies involving human subjects, it is
common to report a
summary of characteristics of the subjects. Why does this matter? Think about
how your interpretation of any study of the fitness of subjects would change if
the mean age (same spread) had been 20 years older or 35 years younger.</p>
<p>1.2. How does knowing about the
distribution of results for <em>Age</em> and <em>BodyWeight</em> help you understand the
results for the Run Times discussed previously?</p>
<p>1.3. The mean and SD are most useful
as summary statistics only if the distribution is relatively symmetric. Make a
histogram of <em>Age</em> responses and
discuss the shape of the distribution (is it skewed right, skewed left,
approximately symmetric?; are there outliers?). Approximately what range of
ages does this study pertain to?</p>
<p>1.4. The weight responses are in
kilograms and you might prefer to see them in pounds. The conversion is
lbs=2.205<code>*</code>kgs. Create a new variable in the <code>treadmill</code>
tibble called <em>BWlb</em> using this code:</p>
<pre class="sourceCode r"><code class="sourceCode r">treadmill<span class="op">$</span>BWlb &lt;-<span class="st"> </span><span class="fl">2.205</span><span class="op">*</span>treadmill<span class="op">$</span>BodyWeight</code></pre>
<p>and find the mean and SD of the new variable (<em>BWlb</em>).</p>
<p>1.5. Make histograms and boxplots of
the original <em>BodyWeight</em> and new <em>BWlb</em> variables. Discuss aspects of the
distributions that changed and those that remained the same with the
transformation from kilograms to pounds. What does this tell you about changing the units of a variable in terms of its distribution?</p>

<p>#(R)e-Introduction to statistics {#chapter2}</p>
<p>The previous material served to get us started in R and to get a quick review of same basic
descriptive statistics. Now we will begin to engage some new material and
exploit the power of R to do some statistical inference. Because inference is
one of the hardest topics to master in statistics, we will also review some
basic terminology that is required to move forward in learning more
sophisticated statistical methods. To keep this “review” as short as possible,
we will not consider every situation you learned in introductory statistics and
instead focus exclusively on the situation where we have a quantitative
response variable measured on two groups, adding a new graphic called a “bean
plot”  to help us see the differences in the observations in the groups.</p>
<p>##Histograms, boxplots, and density curves {#section2-1}</p>
<p>Part of learning statistics is learning to correctly use the terminology, some of which is used colloquially
differently than it is used in formal statistical settings. The most commonly
“misused” statistical term is <strong><em>data</em></strong>.   In statistical parlance, we want to note the plurality of
data. Specifically, <strong><em>datum</em></strong> is a single measurement, possibly on multiple random
variables, and so it is appropriate to say that “<strong>a datum is…</strong>”.
Once we move to discussing data, we are now referring to more than one
observation, again on one, or possibly more than one, random variable, and
so we need to use “<strong>data are…</strong>” when talking about our observations. We want
to distinguish our use of the term “data” from its more
colloquial<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> usage that often involves treating it as singular.
In a statistical setting
“data” refers to measurements of our cases or units. When we summarize the
results of a study (say providing the mean and SD), that information is not
“data”. We used our data to generate that information. Sometimes we also use
the term “data set” to refer to all our observations and this is a singular
term to refer to the group of observations and this makes it really easy to
make mistakes on the usage of “data”<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>.</p>
<p>It is also really important to note that <strong><em>variables</em></strong> have to vary –
if you measure the level of education of your subjects but all are high school graduates, then you do
not have a “variable”. You may not know if you have real variability in a
“variable” until you explore the results you obtained.</p>
<p>The last, but probably most important, aspect of data is the context
of the measurement. The “who, what, when, and where” of the collection
of the observations is critical to the
sort of conclusions we can make based on the results. The information on the
study design provides information required to assess the scope of inference of
the study.  Generally, remember to think about the research questions the
researchers were trying to answer and whether their study actually would answer
those questions. There are no formulas to help us sort some of these things
out, just critical thinking about the context of the measurements.</p>
<p>To make this concrete, consider the data collected from a study
<span class="citation">(Plaster <a href="#ref-Plaster1989" role="doc-biblioref">1989</a>)</span> to investigate whether
perceived physical attractiveness had an impact on the sentences or perceived
seriousness of a crime that male jurors might give to female defendants. The
researchers showed the participants in the study (men who volunteered from a
prison) pictures of one of three young women. Each picture had previously been
decided to be either beautiful, average, or unattractive by the researchers.
Each “juror” was randomly assigned to one of three levels of this factor

(which is a categorical <em>predictor</em> or <em>explanatory</em> variable) 
and then each participant rated their
picture on a variety of traits such as how warm or sincere the woman appeared.
Finally, they were told the women had committed a crime (also randomly assigned
to either be told she committed a burglary or a swindle) and were asked to rate
the seriousness of the crime and provide a suggested length of sentence. We
will bypass some aspects of their research and just focus on differences in the
sentence suggested among the three pictures. To get a sense of these data,
let’s consider the first and last parts of the data set:</p>

<table style="width:100%;">
<caption><span id="tab:Table2-1">Table 0.1: </span> First 5 and last 7 rows of the Mock Jury data set.</caption>
<colgroup>
<col width="13%" />
<col width="16%" />
<col width="14%" />
<col width="10%" />
<col width="13%" />
<col width="18%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Subject</th>
<th align="center">Attr</th>
<th align="center">Crime</th>
<th align="center">Years</th>
<th align="center">Serious</th>
<th align="center">Independent</th>
<th align="center">Sincere</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">10</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">3</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">9</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">7</td>
<td align="center">9</td>
<td align="center">5</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">108</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">109</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">9</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">110</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">8</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">111</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">7</td>
<td align="center">4</td>
<td align="center">9</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">112</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">6</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">113</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">12</td>
<td align="center">9</td>
<td align="center">9</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">114</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">8</td>
<td align="center">8</td>
<td align="center">1</td>
<td align="center">5</td>
</tr>
</tbody>
</table>
<p>When working with data, we should always start with
summarizing the sample size. We will use <strong><em>n</em></strong>  for the
number of subjects in the sample and denote the population size (if
available) with <strong><em>N</em></strong>.  Here, the sample size is <strong><em>n=114</em></strong>. In
this situation, we do not have a random sample from a population 
(these were volunteers from the population of prisoners at the
particular prison) so we cannot make inferences from our sample to a larger group.
But we can assess whether there is a  <strong><em>causal effect</em></strong><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>: if sufficient evidence is found to conclude that there is some difference in
the responses across the treated groups, we can attribute those differences to
the treatments applied, since the groups should be same otherwise due to the
pictures being randomly assigned to the “jurors”. The story of the data set –
that it was collected on prisoners – becomes pretty important in thinking about
the ramifications of any results. Are male prisoners different from the
population of college males or all residents of a state such as Montana? If so,
then we should not assume that the detected differences, if detected, would
also exist in some other group of male subjects. The lack of a random sample 
makes it impossible to assume that this set of prisoners might be like other
prisoners. So there are definite limitations to the inferences in the following
results. But it is still interesting to see if the pictures caused a difference
in the suggested mean sentences, even though the inferences are limited to this
group of prisoners. If this had been an observational study (suppose that the
prisoners could select one of the three pictures), then we would have to avoid
any of the “causal” language that we can consider here because the pictures
were not randomly assigned to the subjects. Without random assignment,  the
explanatory variable of picture choice could be <strong><em>confounded</em></strong> 
with another characteristic of prisoners that was related to which picture they selected and
the rating they provided. Confounding is not the only reason to avoid causal
statements with non-random assignment but the inability to separate the effect
of other variables (measured or unmeasured) from the differences we are
observing means that our inferences in these situations need to be carefully
stated to avoid implying causal effects.</p>
<p>Instead of loading this data set into R using the “Import Dataset”
functionality, we can load an R

package that contains the data, making for easy access to this data set. The
package called <code>heplots</code> <span class="citation">(Fox and Friendly <a href="#ref-R-heplots" role="doc-biblioref">2018</a>)</span> contains a data set called <code>MockJury</code>
that contains the results of the study.

We also rely on the R package called
<code>mosaic</code> <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaic" role="doc-biblioref">2019</a>)</span> that was introduced previously. First (but only once),
you need to install both packages, which can
be done either using the Packages tab in the lower right panel of RStudio or
using the <code>install.packages</code> function with quotes around the package name:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">install.packages</span>(<span class="st">&quot;heplots&quot;</span>)</code></pre>
<p>After making sure that both packages are installed, we use the <code>require</code>
function around the package name (no quotes now!) to load the package, something that
you need to do any time you want to use features of a package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(heplots)
<span class="kw">require</span>(mosaic)</code></pre>
<p>When you are loading a package, R might mention a need to install other packages. If the output says that it needs a package that is
unavailable, then follow the same process noted above to install that package
and then repeat trying to load the package you wanted. These are called package “dependencies” and are due to one package developer relying on functions that already exist in another package.</p>
<p>To load the data set that is available in an active package, we use the
<code>data</code> function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(MockJury)</code></pre>
<p>Some data sets, like this one, will load in a different data set framework
called a “data frame” (its class in R is “data.frame”). You can leave it in this
format but for consistency of output, we will change it to a <em>tibble</em> using the
<code>as.tibble</code> function in the code that follows. Now there will be a tibble called
<code>MockJury</code> available for us to
analyze and some information about it in the <strong>Environment</strong> tab. Again, we
can find out more about the data set in a couple of ways. First,
we can use the <code>View</code> function to provide a spreadsheet type of display
in the upper left panel. Second, we can use the <code>head</code> and <code>tail</code>
functions to print out the beginning and end of the data set.<br />
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(tibble)
MockJury&lt;-<span class="kw">as.tibble</span>(MockJury)
<span class="kw">View</span>(MockJury)
<span class="kw">head</span>(MockJury)</code></pre>
<pre><code>##        Attr    Crime Years Serious exciting calm independent sincere warm
## 1 Beautiful Burglary    10       8        6    9           9       8    5
## 2 Beautiful Burglary     3       8        9    5           9       3    5
## 3 Beautiful Burglary     5       5        3    4           6       3    6
## 4 Beautiful Burglary     1       3        3    6           9       8    8
## 5 Beautiful Burglary     7       9        1    1           5       1    8
## 6 Beautiful Burglary     7       9        1    5           7       5    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 1       9        9    9           6      9             9     5     9
## 2       9        9    4           9      5             5     5     7
## 3       7        4    2           4      5             4     5     5
## 4       9        9    9           9      9             9     9     9
## 5       8        9    4           7      9             9     8     7
## 6       8        9    5           8      9             9     9     9</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(MockJury)</code></pre>
<pre><code>##        Attr   Crime Years Serious exciting calm independent sincere warm
## 109 Average Swindle     3       2        7    6           9       9    6
## 110 Average Swindle     2       1        8    8           8       8    8
## 111 Average Swindle     7       4        1    6           9       1    1
## 112 Average Swindle     6       3        5    3           5       2    4
## 113 Average Swindle    12       9        1    9           9       1    1
## 114 Average Swindle     8       8        1    9           1       5    1
##     phyattr sociable kind intelligent strong sophisticated happy ownPA
## 109       4        7    6           8      6             5     7     2
## 110       8        9    9           9      9             9     9     6
## 111       1        9    4           1      1             1     1     9
## 112       1        4    9           3      3             9     5     3
## 113       1        9    1           9      9             1     9     1
## 114       1        9    1           1      9             5     1     1</code></pre>
<p>When data sets are loaded from packages, there is often extra documentation
available about the data set which can be accessed using the <code>help</code> function. In
this case, it will bring up a screen with information about the study and each
variable that was measured.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">help</span>(MockJury)</code></pre>
<p>The <code>help</code> function is also useful with functions in R to help you
understand options and, at the bottom of the help,
see examples of using the function. </p>
<p>With many variables in a data set, it is often useful to get some
quick information about all of them; the <code>summary</code> function provides
useful information whether the variables are categorical or
quantitative and notes if any values were missing. </p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(MockJury)</code></pre>
<pre><code>##            Attr         Crime        Years           Serious     
##  Beautiful   :39   Burglary:59   Min.   : 1.000   Min.   :1.000  
##  Average     :38   Swindle :55   1st Qu.: 2.000   1st Qu.:3.000  
##  Unattractive:37                 Median : 3.000   Median :5.000  
##                                  Mean   : 4.693   Mean   :5.018  
##                                  3rd Qu.: 7.000   3rd Qu.:6.750  
##                                  Max.   :15.000   Max.   :9.000  
##     exciting          calm        independent       sincere     
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:3.000   1st Qu.:4.250   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.000   Median :6.500   Median :6.500   Median :5.000  
##  Mean   :4.658   Mean   :5.982   Mean   :6.132   Mean   :4.789  
##  3rd Qu.:6.000   3rd Qu.:8.000   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##       warm         phyattr        sociable          kind      
##  Min.   :1.00   Min.   :1.00   Min.   :1.000   Min.   :1.000  
##  1st Qu.:2.00   1st Qu.:2.00   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.00   Median :5.00   Median :7.000   Median :5.000  
##  Mean   :4.57   Mean   :4.93   Mean   :6.132   Mean   :4.728  
##  3rd Qu.:7.00   3rd Qu.:8.00   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.00   Max.   :9.00   Max.   :9.000   Max.   :9.000  
##   intelligent        strong      sophisticated       happy      
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:4.000   1st Qu.:4.000   1st Qu.:3.250   1st Qu.:3.000  
##  Median :7.000   Median :6.000   Median :5.000   Median :5.000  
##  Mean   :6.096   Mean   :5.649   Mean   :5.061   Mean   :5.061  
##  3rd Qu.:8.750   3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##      ownPA      
##  Min.   :1.000  
##  1st Qu.:5.000  
##  Median :6.000  
##  Mean   :6.377  
##  3rd Qu.:9.000  
##  Max.   :9.000</code></pre>
<p>If we take a few moments to explore the output we can discover some
useful aspects of the data set. The output is organized by variable,
providing summary information based on the type of
variable, either counts by category for categorical variables (<code>Attr</code>
and <code>Crime</code>) or the 5-number summary plus the mean for quantitative
variables. If present, you would also get a count of missing values that are
called “NAs” in R.  For the first variable, called <code>Attr</code> in the data.frame
and that we might more explicitly name <em>Attractiveness</em>, we find counts of the
number of subjects shown each picture: <span class="math inline">\(37\)</span> out of <span class="math inline">\(114\)</span> viewed the
“Unattractive” picture, <span class="math inline">\(38\)</span> viewed “Average”, and <span class="math inline">\(39\)</span> viewed “Beautiful”.
We can also see that suggested prison sentences (variable
<code>Years</code>) ranged from 1 year to 15 years with a median of 3 years.
It seems that all the other variables except for <em>Crime</em> (type of crime
that they were told the pictured woman committed) contained responses
between 1 and 9 based on rating scales from 1 = low to 9 = high.</p>
<p>To accompany the numerical summaries, histograms and boxplots can
provide some initial information on the shape of the distribution of
the responses for the suggested sentences in <em>Years</em>. Figure <a href="acknowledgments.html#fig:Figure2-1">0.10</a>
contains the histogram
and boxplot of <em>Years</em>, ignoring any information on which picture the
“jurors” were shown. The calls to the two plotting functions are
enhanced slightly to add better labels.</p>

<div class="figure"><span id="fig:Figure2-1"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-1-1.png" alt="Histogram and boxplot of suggested sentences in years." width="576" />
<p class="caption">
Figure 0.10: Histogram and boxplot of suggested sentences in years.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">labels=</span>T, <span class="dt">main=</span><span class="st">&quot;Histogram of Years&quot;</span>)
<span class="kw">boxplot</span>(MockJury<span class="op">$</span>Years, <span class="dt">ylab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Boxplot of Years&quot;</span>)</code></pre>
<p>The distribution appears to have a strong right skew with three
observations at 15 years flagged as potential
outliers. You can only tell that there are three observations and that they are
at 15 by looking at both plots – the bar around 15 years in the histogram has a
count of three and the boxplot only shows a single point at 15 which is
actually three tied points at exactly 15 years plotted on top of each other (we
call this “overplotting”). These three observations really seem to be the upper
edge of the overall pattern of a strongly right skewed distribution, so even
though they are flagged in the boxplot, they really are not outside the pattern we observed so are not outliers. In real data sets, outliers are commonly encountered and the
first step is to verify that they were not errors in recording (if so, fixing or removing them is easily justified). If they cannot be easily dismissed or fixed, the next step
is to study their impact on the statistical analyses performed, potentially
considering reporting results with and without the influential observation(s)
in the results. If the analysis is unaffected by the “unusual” observations,
then it matters little whether they are dropped or not. If they do affect the
results, then reporting both versions of results allows the reader to judge the
impacts for themselves. It is important to remember that sometimes the outliers
are the most interesting part of the data set. </p>
<p>Often when statisticians think of distributions of data, we think
of the smooth underlying
shape that led to the data set that is being displayed in the histogram.
Instead of binning up observations and making bars in the histogram, we can
estimate what is called a <strong><em>density curve</em></strong> as a smooth curve
that represents the observed distribution of the responses. Density curves can
sometimes help us see features of the data sets more clearly. </p>
<p>To understand the density curve, it is useful to initially see
the histogram and density curve together. The height of the density curve is scaled
so that the total area under the curve<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> is 1. To make a comparable histogram, the
y-axis needs to be scaled so that the histogram is also on the “density”
scale which makes the bar heights adjust so that the proportion of the
total data set in each bar is represented by the area in each bar
(remember that area is height times width). So the height depends on the
width of the bars and the total area across all the bars has to be 1. In the
<code>hist</code> function, the <code>freq=F</code> option does this required re-scaling to get
density-scaled histogram bars. The
density curve is added to the histogram using the R code of
<code>lines(density())</code>, producing the result in Figure <a href="acknowledgments.html#fig:Figure2-2">0.11</a> with
added modifications of options for <code>lwd</code> (line width) and <code>col</code> (color)
to make the plot more interesting. You can see how the density curve
somewhat matches the histogram bars but deals with the bumps up and down
and edges a little differently. We can pick out the strong right skew using
either display and will rarely make both together.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Histogram of Years&quot;</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(MockJury<span class="op">$</span>Years), <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure2-2"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-2-1.png" alt="Histogram and density curve of Years data." width="480" />
<p class="caption">
Figure 0.11: Histogram and density curve of Years data.
</p>
</div>
<p>Histograms can be sensitive to the choice of the number of bars and
even the cut-offs used to define the bins for a given number of bars.
Small changes in the definition of cut-offs for the bins can have
noticeable impacts on the shapes observed but
this does not impact density curves. We are not going to tinker with the
default choices for bars in histogram as they are reasonably selected, but we
can add information on the original observations being included in each bar to
better understand the choices that <code>hist</code> is making. In the previous
display, we can add what is called a <strong><em>rug</em></strong> to the plot, where a tick
mark is made on the x-axis for each observation.  Because the responses
were provided as whole years (1, 2, 3, …, 15), we need to use a graphical
technique called <strong><em>jittering</em></strong> to add a little noise<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> to each observation so all the
observations at each year value do not
plot as a single line.  In Figure <a href="acknowledgments.html#fig:Figure2-3">0.12</a>, the added tick marks
on the x-axis show the approximate locations of the original observations.
We can see how there are 3 observations at 15 (all were 15 and the noise
added makes it possible to see them all). The limitations of the
histogram arise around the 10 year sentence
area where there are many responses at 10 years and just one at both 9 and 11 years,
but the histogram bars sort of miss this aspect of the data set. The
density curve did show a small bump at 10 years. Density curves are, however,
not perfect and this one shows area for sentences less than 0 years which is
not possible here.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;Histogram of Years with density curve and rug&quot;</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(MockJury<span class="op">$</span>Years), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">rug</span>(<span class="kw">jitter</span>(MockJury<span class="op">$</span>Years), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<div class="figure"><span id="fig:Figure2-3"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-3-1.png" alt="Histogram with density curve and rug plot of the jittered responses." width="480" />
<p class="caption">
Figure 0.12: Histogram with density curve and rug plot of the jittered responses.
</p>
</div>
<p>The graphical tools we’ve just discussed are going to help us move to comparing the
distribution of responses across more than one group. We will have two displays
that will help us make these comparisons. The simplest is
the <strong><em>side-by-side boxplot</em></strong>, where a boxplot is displayed for each group
of interest using the same y-axis scaling.  In R, we can use its <strong><em>formula</em></strong>
notation to see if the response (<code>Years</code>) differs based on the group
(<code>Attr</code>) by using something like <code>Y~X</code> or, here, <code>Years~Attr</code>.
We also need to tell R where to find the variables – use the last option in the command, <code>data=DATASETNAME</code> , to inform R of the tibble to look in
to find the variables. In this example, <code>data=MockJury</code>. We will use
the formula and <code>data=...</code> options in almost every function we use
from here forward. Figure <a href="acknowledgments.html#fig:Figure2-4">0.13</a> contains the side-by-side
boxplots showing right skew for all the groups, slightly higher median and
more variability for the <em>Unattractive</em> group along with some potential outliers indicated in two of the three groups.</p>

<div class="figure"><span id="fig:Figure2-4"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-4-1.png" alt="Side-by-side boxplot of Years based on picture groups." width="480" />
<p class="caption">
Figure 0.13: Side-by-side boxplot of Years based on picture groups.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<p>The “~” (which is read as the <em>tilde</em> symbol, which you can find in the
upper left corner of your keyboard) notation will be used in two ways this
semester.  The formula use in R employed previously declares that the
response variable here is <em>Years</em> and the explanatory variable is <em>Attr</em>.
The other use for “~” is as shorthand for “is distributed as” and is used in
the context of <span class="math inline">\(Y\sim N(0,1)\)</span>, which translates (in statistics) to defining the
random variable <em>Y</em> as following a Normal distribution<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>

with mean 0
and standard deviation of 1. In the current situation, we could ask whether
the <code>Years</code> variable seems like it may follow a normal distribution, in
other words, is <span class="math inline">\(\text{Years}\sim N(\mu,\sigma^2)\)</span>? Since the responses are right
skewed with some groups having outliers, it is not reasonable to assume that
the <em>Years</em> variable for any of the three groups may follow a normal
distribution (more later on the issues this creates!). Remember that
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters where
<span class="math inline">\(\mu\)</span> (“mu”) is our standard symbol for the <strong><em>population mean</em></strong>
and that <span class="math inline">\(\sigma\)</span> (“sigma”) is the symbol of the
<strong><em>population standard deviation</em></strong>.  </p>
<p>##Beanplots {#section2-2}</p>
<p>The other graphical display for comparing multiple groups we will use is a display called a <strong><em>beanplot</em></strong> <span class="citation">(Kampstra <a href="#ref-Kampstra2008" role="doc-biblioref">2008</a>)</span>.  Figure <a href="acknowledgments.html#fig:Figure2-5">0.14</a>
shows an example of a beanplot  that provides a side-by-side display that
contains the density curves, the original observations that generated the
density curve in a (jittered) rug-plot, the mean of each group, and the
overall mean of the entire data set. For each group, the density curves
are mirrored to aid in visual assessment of the shape of the distribution,
which makes a “bean” of sorts. This mirroring also
creates a shape that resembles a violin with skewed distributions so this
display has also been called a “violin plot”. The beanplot  includes bold
horizontal lines at the mean for each group and adds a lighter
dashed line for the overall mean to allow comparison of that global mean with the individual group means. All together this plot shows us information
on the center (mean), spread, and shape of the distributions of the responses.
Our inferences typically focus on the means of the groups and this plot allows
us to compare those across the groups while gaining information on the shapes
of the distributions of responses in each group.</p>
<p>To use the <code>beanplot</code> function  we need to install and then load the <code>beanplot</code>
package <span class="citation">(Kampstra <a href="#ref-R-beanplot" role="doc-biblioref">2014</a>)</span>.

The function works like the boxplot used previously
except that options
for <code>log</code>, <code>col</code>, and <code>method</code> need to be specified. Use these<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> options for any beanplots you make: 
<code>log="", col="bisque", method="jitter"</code>.</p>
<p>(ref:fig2-5) Beanplot of Years by picture group. Long, bold lines correspond
to mean of each group, dashed line for overall or global mean.</p>
<div class="figure"><span id="fig:Figure2-5"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-5-1.png" alt="(ref:fig2-5)" width="480" />
<p class="caption">
Figure 0.14: (ref:fig2-5)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(beanplot)
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre>
<p>Figure <a href="acknowledgments.html#fig:Figure2-5">0.14</a> reinforces the strong right skews that were also
detected in the boxplots previously. The three large sentences of 15 years
can now be clearly identified, with one in the <em>Beautiful</em> group and two in
the <em>Unattractive</em> group. The <em>Unattractive</em> group seems to have more high
observations than the other groups even though the <em>Beautiful</em> group had the
largest number of observations around 10 years. The mean sentence was highest
for the <em>Unattractive</em> group and the difference in the means between
<em>Beautiful</em> and <em>Average</em> was small.</p>
<p>In this example, it appears that the mean for <em>Unattractive</em> is larger
than the other two groups. But is this difference real? We will never
know the answer to that question, but we
can assess how likely we are to have seen a result as extreme or more
extreme than our result, assuming that there is no difference in the
means of the groups. And if the observed result is
(extremely) unlikely to occur, then we can reject the hypothesis that the
groups have the same mean and conclude that there is evidence of a real
difference. To start exploring whether there are differences in the means, we
need to have numerical values to compare. We can get means and standard
deviations by groups easily using the same formula notation with the <code>mean</code>
and <code>sd</code> functions if the <code>mosaic</code> package is loaded.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">mean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##     4.333333     3.973684     5.810811</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##     3.405362     2.823519     4.364235</code></pre>
<p>We can also use the <code>favstats</code> function to get those summaries and others by groups.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>##           Attr min Q1 median   Q3 max     mean       sd  n missing
## 1    Beautiful   1  2      3  6.5  15 4.333333 3.405362 39       0
## 2      Average   1  2      3  5.0  12 3.973684 2.823519 38       0
## 3 Unattractive   1  2      5 10.0  15 5.810811 4.364235 37       0</code></pre>
<p>Based on these results, we can see that there is an estimated difference of
almost 2 years in the mean sentence between <em>Average</em> and <em>Unattractive</em> groups. Because there are three groups being compared in this study, we will have to
wait until Chapter 3 and the One-Way ANOVA test to fully assess evidence
related to some difference among the three groups. For now, we are going to
focus on comparing the mean <em>Years</em> between <em>Average</em> and <em>Unattractive</em> groups
– which is a <strong><em>two independent sample mean</em></strong> situation and something you
should have seen before. Remember that the “independent” sample part of
this refers to observations that are independently observed for the two
groups as opposed to the paired sample situation that you may have
explored where one observation from the first group is related to an
observation in the second group (two measures on the same person (we
generically call this “repeated measures”)
or the famous “twin” studies with one twin assigned to each group). </p>
<p>Here we are going to use the “simple” two independent group scenario to
review some basic statistical concepts and connect two different
frameworks for conducting statistical inference: randomization and
parametric  inference techniques. <strong><em>Parametric</em></strong> statistical methods
involve making assumptions

about the distribution of the
responses and obtaining confidence intervals and/or p-values using a
<em>named</em> distribution (like the <span class="math inline">\(z\)</span> or <span class="math inline">\(t\)</span>-distributions). Typically these
results are generated using formulas and looking up areas under curves or
cutoffs using a table or a computer. <strong><em>Randomization</em></strong>-based statistical
methods use a computer to shuffle, sample, or simulate observations in ways
that allow you to obtain distributions of possible results to find areas and
cutoffs without resorting to using tables and named distributions.
Randomization methods are what are called <strong><em>nonparametric</em></strong> methods

that often make fewer assumptions (they are <strong><em>not free of assumptions</em></strong>!)
and so can handle a larger set of problems more easily than parametric
methods.

When the assumptions involved in the parametric procedures are
met by a data set, the randomization methods often provide very similar
results to those provided by the parametric techniques. To be a more
sophisticated statistical consumer, it is useful to have some knowledge
of both of these techniques for performing statistical inference and the fact that
they can provide similar results might deepen your understanding of both
approaches.</p>
<p>We will start with comparing the <em>Average</em> and <em>Unattractive</em> groups to
compare these two ways of doing inference. We could remove the <em>Beautiful</em>
group observations in a spreadsheet program and read that new data set
back into R, but it is actually pretty easy to use R to do data
management once the data set is loaded. To remove the observations that came
from the <em>Beautiful</em> group, we are going to generate a new variable
that we will call <code>NotBeautiful</code> that is true when observations came
from another group (<em>Average</em> or <em>Unattractive</em>) and false for observations
from the <em>Beautiful</em> group. To do this, we will apply the <strong><em>not equal</em></strong>
logical function (<code>!=</code> ) to the variable <code>Attr</code>, inquiring whether it
was different from the <code>"Beautiful"</code> level. You can see the content of the
new variable in the output:</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury<span class="op">$</span>NotBeautiful &lt;-<span class="st"> </span>MockJury<span class="op">$</span>Attr <span class="op">!=</span><span class="st"> &quot;Beautiful&quot;</span>
MockJury<span class="op">$</span>NotBeautiful</code></pre>
<pre><code>##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
##  [23]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [34]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [45]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [56]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [67]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
##  [78] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [89] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
## [100]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [111]  TRUE  TRUE  TRUE  TRUE</code></pre>
<p>This new variable is only FALSE for the <em>Beautiful</em> responses as we can see
if we compare some of the results from the original and new variable:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">tibble</span>(MockJury<span class="op">$</span>Attr, MockJury<span class="op">$</span>NotBeautiful))</code></pre>
<pre><code>## # A tibble: 6 x 2
##   `MockJury$Attr` `MockJury$NotBeautiful`
##   &lt;fct&gt;           &lt;lgl&gt;                  
## 1 Beautiful       FALSE                  
## 2 Beautiful       FALSE                  
## 3 Beautiful       FALSE                  
## 4 Beautiful       FALSE                  
## 5 Beautiful       FALSE                  
## 6 Beautiful       FALSE</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(<span class="kw">tibble</span>(MockJury<span class="op">$</span>Attr, MockJury<span class="op">$</span>NotBeautiful))</code></pre>
<pre><code>## # A tibble: 6 x 2
##   `MockJury$Attr` `MockJury$NotBeautiful`
##   &lt;fct&gt;           &lt;lgl&gt;                  
## 1 Average         TRUE                   
## 2 Average         TRUE                   
## 3 Average         TRUE                   
## 4 Average         TRUE                   
## 5 Average         TRUE                   
## 6 Average         TRUE</code></pre>
<p>To get rid of one of the responses that are in one of the groups, we need to learn a little bit about data
management in R. <strong><em>Brackets</em></strong> <code>([,])</code> are used to access and possibly modify the rows or
columns in a tibble with entries before the comma operating on rows and
entries after the comma on the columns. For example, if you want to see the
results for the 5<sup>th</sup> subject, you can reference the 5<sup>th</sup> row of the
tibble using <code>[5,]</code> after the tibble name:</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury[<span class="dv">5</span>,]</code></pre>
<pre><code>##        Attr    Crime Years Serious exciting calm independent sincere warm
## 5 Beautiful Burglary     7       9        1    1           5       1    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 5       8        9    4           7      9             9     8     7
##   NotBeautiful
## 5        FALSE</code></pre>
<p>We could just extract the <em>Years</em> response for the 5<sup>th</sup>
subject by incorporating information on the row and column of interest
(<code>Years</code> is the 3<sup>rd</sup> column):</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury[<span class="dv">5</span>,<span class="dv">3</span>]</code></pre>
<pre><code>## [1] 7</code></pre>
<p>In R, we can use logical vectors to keep any rows of the tibble where
the variable is true and drop any rows where it is false by placing the
logical variable in the first element of the brackets. The reduced version
of the data set should be saved with a different name such as <code>MockJury2</code>
that is used here to reduce the chances of confusing it with the previous
full data set:</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury2 &lt;-<span class="st"> </span>MockJury[MockJury<span class="op">$</span>NotBeautiful,]</code></pre>
<p>You will always want to check that the correct observations were dropped
either using <code>View(MockJury2)</code> or by doing a quick summary of the
<code>Attr</code> variable in the new tibble.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(MockJury2<span class="op">$</span>Attr)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##            0           38           37</code></pre>
<p>It ends up that R remembers the <em>Beautiful</em> category even though there are
0 observations in it now and that can cause us some problems. When we remove a
group of observations<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>, we sometimes need to clean up categorical variables to
just reflect the categories that are present. The <code>factor</code>

function
creates categorical variables based on the levels of the variables that are
observed and is useful to run here to clean up <code>Attr</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury2<span class="op">$</span>Attr &lt;-<span class="st"> </span><span class="kw">factor</span>(MockJury2<span class="op">$</span>Attr) 
<span class="kw">summary</span>(MockJury2<span class="op">$</span>Attr)</code></pre>
<pre><code>##      Average Unattractive 
##           38           37</code></pre>
<p>Now if we remake the boxplots and beanplots, they only contain results for
the two groups of interest here as seen in Figure <a href="acknowledgments.html#fig:Figure2-6">0.15</a>.</p>

<div class="figure"><span id="fig:Figure2-6"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-6-1.png" alt="Boxplot and beanplot of the Years responses on the reduced MockJury2 data set." width="552" />
<p class="caption">
Figure 0.15: Boxplot and beanplot of the <em>Years</em> responses on the reduced <code>MockJury2</code> data set.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2) 
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre>
<p>The two-sample mean techniques you learned in your previous course all
start with comparing the means the two groups. We can obtain the two
means using the <code>mean</code> function or directly obtain the difference
in the means using the <code>diffmean</code> function (both require the <code>mosaic</code>
package). The <code>diffmean</code> function provides
<span class="math inline">\(\bar{x}_\text{Unattractive} - \bar{x}_\text{Average}\)</span> where <span class="math inline">\(\bar{x}\)</span>
(read as “x-bar”) is the sample mean of observations in the subscripted
group. Note that there are two directions that you could compare the
means and this function chooses to take the mean from the second group
name <em>alphabetically</em> and subtract the mean from the first alphabetical group
name. It is always good to check the direction of this calculation as
having a difference of <span class="math inline">\(-1.84\)</span> years versus <span class="math inline">\(1.84\)</span> years could be important.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre>
<pre><code>##      Average Unattractive 
##     3.973684     5.810811</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre>
<pre><code>## diffmean 
## 1.837127</code></pre>

</div>
<div id="section2-3" class="section level2">
<h2><span class="header-section-number">0.2</span> Models, hypotheses, and permutations for the two sample mean situation</h2>

<p>There appears to be some evidence that the <em>Unattractive</em> group is
getting higher average lengths of sentences from the prisoner “jurors” than
the <em>Average</em> group, but we want to make sure that the difference is
real – that there is evidence to reject the assumption that the means
are the same “in the population”. First, a <strong><em>null hypothesis</em></strong><a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> which
defines a <strong><em>null model</em></strong><a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>

needs to be determined in terms of <strong><em>parameters</em></strong> (the true values in
the population). The research question should help you determine the form of the
hypotheses for the assumed population. In the two independent sample mean
problem, the interest is in testing a null hypothesis of <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span>
versus the alternative hypothesis of <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span>, where
<span class="math inline">\(\mu_1\)</span> is the parameter for the true mean of the first group and <span class="math inline">\(\mu_2\)</span>
is the parameter for the true mean of the second group. The alternative
hypothesis involves assuming a statistical model

for the <span class="math inline">\(i^{th}\ (i=1,\ldots,n_j)\)</span>
response from the <span class="math inline">\(j^{th}\ (j=1,2)\)</span> group, <span class="math inline">\(\boldsymbol{y}_{ij}\)</span>, that
involves modeling it as <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>,
where we assume that <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. For the moment,
focus on the models that either assume the means are the same (null) or
different (alternative),

which imply:</p>
<ul>
<li><p>Null Model: <span class="math inline">\(y_{ij} = \mu + \varepsilon_{ij}\)</span> There is <strong>no</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
<li><p>Alternative Model: <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span> There is <strong>a</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
</ul>
<p>Suppose we are considering the alternative model for the 4<sup>th</sup>
observation (<span class="math inline">\(i=4\)</span>) from the second group (<span class="math inline">\(j=2\)</span>), then the model for
this observation is <span class="math inline">\(y_{42} = \mu_2 +\varepsilon_{42}\)</span>, that defines the
response as coming from the true mean for the second group plus a
random error term for that observation, <span class="math inline">\(\varepsilon_{42}\)</span>. For, say, the
5<sup>th</sup> observation from the first group (<span class="math inline">\(j=1\)</span>), the model is
<span class="math inline">\(y_{51} = \mu_1 +\varepsilon_{51}\)</span>. If we were working with the null model,
the mean is always the same (<span class="math inline">\(\mu\)</span>) – the group specified does not change
the mean we use for that observation.</p>
<p>It can be helpful to think about the null and alternative models graphically.

By assuming the null hypothesis is true (means are equal) and that the random
errors around the mean follow a normal distribution,

we assume that the truth
is as displayed in the left panel of Figure <a href="acknowledgments.html#fig:Figure2-7">0.16</a> – two
normal distributions with the same mean and variability. The alternative
model allows the two groups to potentially have different means, such as
those displayed in the right panel of Figure <a href="acknowledgments.html#fig:Figure2-7">0.16</a> where the
second group has a larger mean. Note that in this scenario, we assume that
the observations all came from the same distribution except that they had
different means. Depending on the statistical procedure we are using, we
basically are going to assume that the observations (<span class="math inline">\(y_{ij}\)</span>) either were
generated as samples from the null or alternative model. You can imagine
drawing observations at random from the pictured distributions. For hypothesis
testing, the null model

is assumed to be true and then the unusualness of
the actual result is assessed relative to that assumption. In hypothesis
testing, we have to decide if we have enough evidence to reject the assumption
that the null model (or hypothesis) is true. If we reject the null hypothesis,
then we would conclude that the other model considered (the alternative
model)

is more reasonable. The researchers obviously would have hoped to
encounter some sort of noticeable difference in the sentences provided for the
different pictures and have been able to find enough evidence to reject the null
model where the groups “look the same”.</p>
<p>(ref:fig2-7) Illustration of the assumed situations under the null (left)
and a single possibility that could occur if the alternative were true
(right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis.</p>
<div class="figure"><span id="fig:Figure2-7"></span>
<img src="chapter2_files/image015.png" alt="(ref:fig2-7)"  />
<p class="caption">
Figure 0.16: (ref:fig2-7)
</p>
</div>
<p>In statistical inference, null hypotheses (and their
implied models) are set
up as “straw men” with every interest in rejecting them even though we assume
they are true to be able to assess the evidence <span class="math inline">\(\underline{\text{against them}}\)</span>.
Consider the original study design here, the pictures were randomly assigned to
the subjects. If the null hypothesis were true, then we would have no difference
in the population means of the groups. And this would apply if we had done a
different random assignment  of the pictures to the subjects. So let’s try this:
assume that the null hypothesis is true and randomly re-assign the treatments
(pictures) to the observations that were obtained. In other words, keep the
sentences (<em>Years</em>) the same and shuffle the group labels randomly. The
technical term for this is doing a <strong><em>permutation</em></strong>  (a random shuffling of
a grouping variable relative to the observed responses). If the null is true
and the means
in the two groups are the same, then we should be able to re-shuffle the
groups to the observed sentences (<em>Years</em>) and get results similar to those we
actually observed. If the null is false and the means are really different in
the two groups, then what we observed should differ from what we get under
other random permutations. The differences between the two groups should be
more noticeable in the observed data set than in (most) of the shuffled data
sets. It helps to see an example of a permutation of the labels to understand
what this means here.</p>
<p>In the <code>mosaic</code> package, the <code>shuffle</code> function allows us to easily perform
a  permutation<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>. Just one time, we can explore what a permutation of the
treatment labels could look like in the <code>PermutedAttr</code> variable below. Note
that the <code>Years</code> are held in the same place while the group labels are shuffled.</p>

<pre class="sourceCode r"><code class="sourceCode r">Perm1 &lt;-<span class="st"> </span><span class="kw">with</span>(MockJury2, <span class="kw">tibble</span>(Years, Attr, <span class="dt">PermutedAttr=</span><span class="kw">shuffle</span>(Attr)))
<span class="co">#To force the tibble to print out all rows in data set - not used often</span>
<span class="kw">data.frame</span>(Perm1) </code></pre>
<pre><code>##    Years         Attr PermutedAttr
## 1      1 Unattractive Unattractive
## 2      4 Unattractive      Average
## 3      3 Unattractive      Average
## 4      2 Unattractive      Average
## 5      8 Unattractive      Average
## 6      8 Unattractive      Average
## 7      1 Unattractive Unattractive
## 8      1 Unattractive Unattractive
## 9      5 Unattractive      Average
## 10     7 Unattractive Unattractive
## 11     1 Unattractive      Average
## 12     5 Unattractive Unattractive
## 13     2 Unattractive Unattractive
## 14    12 Unattractive      Average
## 15    10 Unattractive      Average
## 16     1 Unattractive      Average
## 17     6 Unattractive Unattractive
## 18     2 Unattractive      Average
## 19     5 Unattractive Unattractive
## 20    12 Unattractive Unattractive
## 21     6 Unattractive      Average
## 22     3 Unattractive      Average
## 23     8 Unattractive      Average
## 24     4 Unattractive Unattractive
## 25    10 Unattractive Unattractive
## 26    10 Unattractive      Average
## 27    15 Unattractive Unattractive
## 28    15 Unattractive      Average
## 29     3 Unattractive      Average
## 30     3 Unattractive      Average
## 31     3 Unattractive Unattractive
## 32    11 Unattractive      Average
## 33    12 Unattractive      Average
## 34     2 Unattractive Unattractive
## 35     1 Unattractive Unattractive
## 36     1 Unattractive Unattractive
## 37    12 Unattractive      Average
## 38     5      Average Unattractive
## 39     5      Average Unattractive
## 40     4      Average Unattractive
## 41     3      Average Unattractive
## 42     6      Average      Average
## 43     4      Average      Average
## 44     9      Average      Average
## 45     8      Average Unattractive
## 46     3      Average      Average
## 47     2      Average Unattractive
## 48    10      Average      Average
## 49     1      Average Unattractive
## 50     1      Average Unattractive
## 51     3      Average Unattractive
## 52     1      Average      Average
## 53     3      Average      Average
## 54     5      Average      Average
## 55     8      Average Unattractive
## 56     3      Average      Average
## 57     1      Average      Average
## 58     1      Average Unattractive
## 59     1      Average      Average
## 60     2      Average      Average
## 61     2      Average Unattractive
## 62     1      Average      Average
## 63     1      Average Unattractive
## 64     2      Average Unattractive
## 65     3      Average Unattractive
## 66     4      Average Unattractive
## 67     5      Average      Average
## 68     3      Average Unattractive
## 69     3      Average      Average
## 70     3      Average      Average
## 71     2      Average Unattractive
## 72     7      Average Unattractive
## 73     6      Average Unattractive
## 74    12      Average Unattractive
## 75     8      Average      Average</code></pre>
<p>If you count up the number of subjects in each group by counting the number
of times each label (Average, Unattractive) occurs, it is the same in both the
<code>Attr</code> and <code>PermutedAttr</code> columns. Permutations involve randomly
re-ordering the values of a variable – here the <code>Attr</code> group labels – without
changing the content of the variable.

This result can also be generated using
what is called <strong><em>sampling without replacement</em></strong>: sequentially select <span class="math inline">\(n\)</span> labels
from the original variable (<em>Attr</em>), removing each observed label and making sure that each of the
original <code>Attr</code> labels is selected once and only once. The new, randomly
selected order of selected labels provides the permuted labels. Stepping
through the process helps to understand how it works: after the initial random
sample of one label, there would <span class="math inline">\(n - 1\)</span> choices possible; on the <span class="math inline">\(n^{th}\)</span>
selection, there would only be one label remaining to select. This makes sure
that all original labels are re-used but that the order is random. Sampling
without replacement is like picking names out of a hat, one-at-a-time, and not
putting the names back in after they are selected. It is an exhaustive process
for all the original observations. <strong><em>Sampling with replacement</em></strong>, in contrast,
involves sampling from the specified list with each observation having an equal
chance of selection for each sampled observation – in other words, observations
can be selected more than once. This is like picking <span class="math inline">\(n\)</span> names out of a hat that
contains <span class="math inline">\(n\)</span> names, except that every time a name is selected, it goes back into
the hat – we’ll use this technique in Section <a href="acknowledgments.html#section2-8">0.7</a>
to do what is called <strong><em>bootstrapping</em></strong>.

Both sampling mechanisms can be
used to generate inferences but each has particular situations
where they are most useful. For hypothesis testing,

we will use permutations 
(sampling without replacement) as its mechanism most closely matches the null hypotheses we will be testing.</p>
<p>The comparison of the beanplots  for the real data set and permuted version of
the labels is what is really interesting (Figure <a href="acknowledgments.html#fig:Figure2-8">0.17</a>). The
original difference in the sample means of the two groups was 1.84 years
(<em>Unattractive</em> minus <em>Average</em>). The sample means are the <strong><em>statistics</em></strong>
that estimate the parameters for the true means of the two groups. In the
permuted data set, the difference in the means is 1.15 years in the opposite
direction (Average had a higher mean than Unattractive in the permuted data).</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years<span class="op">~</span>PermutedAttr, <span class="dt">data=</span>Perm1)</code></pre>
<pre><code>##      Average Unattractive 
##     5.447368     4.297297</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(Years<span class="op">~</span>PermutedAttr, <span class="dt">data=</span>Perm1)</code></pre>
<pre><code>##  diffmean 
## -1.150071</code></pre>
<p>(ref:fig2-8) Boxplots of Years responses versus actual treatment groups and
permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set.</p>
<div class="figure"><span id="fig:Figure2-8"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-8-1.png" alt="(ref:fig2-8)" width="672" />
<p class="caption">
Figure 0.17: (ref:fig2-8)
</p>
</div>
<p>These results suggest that the observed difference was larger than what we got
when we did a single permutation  although it was only a little bit larger than
a difference we could observe in permutations if we ignore the difference in
directions. Conceptually, permuting observations between group labels is
consistent with the null hypothesis – this is a technique to generate results
that we might have gotten if the null hypothesis were true since the responses
are the same in the two groups if the null is true. We just need to repeat the
permutation process many times and track how unusual our observed result is
relative to this distribution of potential responses if the null were true.
If the observed differences are unusual relative to the results under
permutations, then there is evidence against the null hypothesis, the null
hypothesis should be rejected (Reject <span class="math inline">\(H_0\)</span>), and a conclusion should be made,
in the direction of the alternative hypothesis, that there is evidence that the
true means differ. If the observed differences are similar to (or at least not
unusual relative to) what we get under random shuffling under the null model,
we would have a tough time concluding that there is any real difference between
the groups based on our observed data set.</p>
</div>
<div id="section2-4" class="section level2">
<h2><span class="header-section-number">0.3</span> Permutation testing for the two sample mean situation</h2>
<p>In any testing situation, you must define some function of the observations that
gives us a single number that addresses our question of interest. This quantity
is called a <strong><em>test statistic</em></strong>. These often take on complicated forms and
have names like <span class="math inline">\(t\)</span> or <span class="math inline">\(z\)</span> statistics that relate to their parametric

(named)
distributions so we know where to look up
<strong><em>p-values</em></strong><a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>. In randomization settings, they can
have simpler forms because we use the data set to find the
distribution of the statistic and don’t need to rely on a
named distribution. We will label our test statistic <strong><em>T</em></strong>
(for <strong>T</strong>est statistic) unless the test statistic has a commonly
used name. Since we are interested in comparing the means of the two groups, we
can define</p>
<p><span class="math display">\[T=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average},\]</span></p>
<p>which coincidentally is what the <code>diffmean</code> function provided us previously.
We label our <strong><em>observed test statistic</em></strong> (the one from the original data
set) as</p>
<p><span class="math display">\[T_{obs}=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average},\]</span></p>
<p>which happened to be 1.84 years here. We will compare this result to the results
for the test statistic that we obtain from permuting the group labels. To
denote permuted results, we will add a * to the labels:</p>
<p><span class="math display">\[T^*=\bar{x}_{\text{Unattractive}^*}-\bar{x}_{\text{Average}^*}.\]</span></p>
<p>We then compare the <span class="math inline">\(T_{obs}=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average} = 1.84\)</span>
to the distribution of results that are possible for the permuted results (<span class="math inline">\(T^*\)</span>)
which corresponds to assuming the null hypothesis is true.</p>
<p>We need to consider lots of permutations to do a permutation test.

In contrast to
your introductory statistics course where, if you did this, it was just a click
away, we are going to learn what was going on under the hood. Specifically, we
need a <strong><em>for loop</em></strong> in R to be able to repeatedly generate the permuted data
sets and record <span class="math inline">\(T^*\)</span> for each one. Loops are a basic programming task that make
randomization methods possible as well as potentially simplifying any repetitive
computing task. To write a “for loop”, we need to choose how many times we want
to do the loop (call that <code>B</code>) and decide on a counter to keep track of where
we are at in the loops (call that <code>b</code>, which goes from 1 up to <code>B</code>). The
simplest loop just involves printing out the index, <code>print(b)</code> at each step.
This is our first use of curly braces, { and}, that are used to group the code
we want to repeatedly run as we proceed through the loop. By typing the following
code in the script window and then highlighting it all and hitting the run button,
R will go through the loop 5 times, printing out the counter:</p>
<pre><code>B &lt;- 5
for (b in (1:B)){
  print(b)
}</code></pre>
<p>Note that when you highlight and run the code, it will look about the same with
“+” printed after the first line to indicate that all the code is connected when
it appears in the console, looking like this:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="cf">for</span>(b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
<span class="op">+</span><span class="st">   </span><span class="kw">print</span>(b)
<span class="op">+</span><span class="st"> </span>}</code></pre>
<p>When you run these three lines of code, the console will show you the following
output:</p>
<pre class="sourceCode r"><code class="sourceCode r">[<span class="dv">1</span>] <span class="dv">1</span>
[<span class="dv">1</span>] <span class="dv">2</span>
[<span class="dv">1</span>] <span class="dv">3</span>
[<span class="dv">1</span>] <span class="dv">4</span>
[<span class="dv">1</span>] <span class="dv">5</span></code></pre>
<p>Instead of printing the counter, we want to use the loop to repeatedly compute
our test statistic across B random permutations of the observations. The
<code>shuffle</code> function performs permutations of the group labels relative to
responses and the <code>diffmean</code> difference in the two group means in the permuted
data set. For a single permutation, the combination of shuffling <code>Attr</code> and
finding the difference in the means, storing it in a variable called <code>Ts</code> is:</p>
<pre class="sourceCode r"><code class="sourceCode r">Ts &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
Ts</code></pre>
<pre><code>##  diffmean 
## -0.616643</code></pre>
<p>And putting this inside the <code>print</code> function allows us to find the test
statistic under 5 different permutations easily:</p>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">5</span>
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Ts &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
  <span class="kw">print</span>(Ts)
}</code></pre>
<pre><code>##   diffmean 
## -0.8300142 
##   diffmean 
## -0.1365576 
##    diffmean 
## -0.08321479 
##  diffmean 
## 0.5035562 
## diffmean 
## 1.677098</code></pre>
<p>Finally, we would like to store the values of the test statistic instead of
just printing them out on each pass through the loop. To do this, we need to
create a variable to store the results, let’s call it <code>Tstar</code>. We know that
we need to store <code>B</code> results so will create a vector<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> of length B, which
contains B elements, full of missing values (NA) using the <code>matrix</code> function with the <code>nrow</code> option specifying the number of elements:</p>
<pre class="sourceCode r"><code class="sourceCode r">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
Tstar</code></pre>
<pre><code>##      [,1]
## [1,]   NA
## [2,]   NA
## [3,]   NA
## [4,]   NA
## [5,]   NA</code></pre>
<p>Now we can run our loop B times and store the results in <code>Tstar</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years <span class="op">~</span><span class="st"> </span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
}
Tstar</code></pre>
<pre><code>##             [,1]
## [1,] -0.08321479
## [2,]  0.23684211
## [3,] -0.24324324
## [4,] -0.61664296
## [5,]  0.66358464</code></pre>
<p>Five permutations are still not enough to assess whether our <span class="math inline">\(T_{obs}\)</span>
of 1.84 is unusual and we need to do many permutations to get an accurate
assessment of the possibilities under the null hypothesis.

It is common practice
to consider something like 1,000 permutations. The <code>Tstar</code> vector when we set
<em>B</em> to be large, say <code>B=1000</code>, contains the permutation distribution  for the
selected test statistic under<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> the null
hypothesis – what is called the <strong><em>null distribution</em></strong> of the statistic. The
null distribution is the distribution of possible values of a statistic
under the null hypothesis. We want to visualize this distribution and use it to
assess how unusual our <span class="math inline">\(T_{obs}\)</span> result of 1.84 years was relative to all the
possibilities under permutations (under the null hypothesis). So we repeat the
loop, now with <span class="math inline">\(B=1000\)</span> and generate a histogram, density curve, and summary
statistics of the results:</p>
<p>(ref:fig2-9) Histogram (left, with counts in bars) and density curve (right) of
values of test statistic for 1,000 permutations.</p>
<div class="figure"><span id="fig:Figure2-9"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-9-1.png" alt="(ref:fig2-9)" width="960" />
<p class="caption">
Figure 0.18: (ref:fig2-9)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
}
<span class="kw">hist</span>(Tstar, <span class="dt">label=</span>T)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Tstar)</code></pre>
<pre><code>##        min         Q1     median        Q3      max       mean        sd
##  -2.910384 -0.5099573 0.07681366 0.6102418 2.530583 0.04694168 0.8497364
##     n missing
##  1000       0</code></pre>
<p>Figure <a href="acknowledgments.html#fig:Figure2-9">0.18</a> contains visualizations of <span class="math inline">\(T^*\)</span> and the <code>favstats</code>
summary provides the related numerical summaries. Our observed <span class="math inline">\(T_{obs}\)</span>
of 1.84 seems fairly unusual relative to these results with only
14 <span class="math inline">\(T^*\)</span> values over 2 based on the
histogram. We need to make more specific comparisons of the permuted results
versus our observed result to be able to clearly decide whether our observed
result is really unusual.</p>
<p>To make the comparisons more concrete, first we can enhance the previous graphs
by adding the value of the test statistic from the real data set, as shown in
Figure <a href="acknowledgments.html#fig:Figure2-10">0.19</a>, using the <code>abline</code> function to draw a vertical
line at our <span class="math inline">\(T_{obs}\)</span> value specified in the <code>v</code> (for vertical) option.</p>
<p>(ref:fig2-10) Histogram (left) and density curve (right) of
values of test statistic for 1,000 permutations with bold vertical line for
value of observed test statistic.</p>
<div class="figure"><span id="fig:Figure2-10"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-10-1.png" alt="(ref:fig2-10)" width="960" />
<p class="caption">
Figure 0.19: (ref:fig2-10)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="fl">1.837</span>
<span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre>
<p>Second, we can calculate the exact number of permuted results that were as
large or larger
than what we observed. To calculate the proportion of the 1,000 values that were
as large or larger than what we observed, we will use the <code>pdata</code> function.

To use this
function, we need to provide the distribution of values to compare to the cut-off
(<code>Tstar</code>), the cut-off point (<code>Tobs</code>), and whether we want calculate the
proportion that are below (left of) or above (right of) the cut-off
(<code>lower.tail=F</code> option provides the proportion of values above the cutoff
of interest).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.02</code></pre>
<p>The proportion of 0.02 tells us that 20 of the 1,000 permuted results
(2%) were as large or larger than what we observed. This type of
work is how we can
generate <strong><em>p-values</em></strong> using permutation distributions.


P-values,

as you should
remember, are the probability of getting a result as extreme as or more extreme
than what we observed, <span class="math inline">\(\underline{\text{given that the null is true}}\)</span>. Finding
only 20
permutations of 1,000 that were larger than our observed result suggests that it
is hard to find a result like what we observed if there really were no difference,
although it is not impossible.</p>
<p>When testing hypotheses for two groups, there are two types of alternative
hypotheses, one-sided or two-sided. <strong><em>One-sided tests</em></strong> involve only considering
differences in one-direction (like <span class="math inline">\(\mu_1 &gt; \mu_2\)</span>) and are performed when
researchers can decide <strong><em>a priori</em></strong><a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a> which group should have a larger mean
if there is going to be any sort of difference. In this situation, we did not
know enough about the potential impacts of the pictures to know which group should
be larger than the other so should do a two-sided test. It is important to
remember that you can’t look at the responses to decide on the hypotheses. It is
often safer and more <strong><em>conservative</em></strong><a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> to start with a
<strong><em>two-sided alternative</em></strong> (<span class="math inline">\(\mathbf{H_A: \mu_1 \ne \mu_2}\)</span>). To do a 2-sided
test, find the area larger than what we observed as above. We also need to add
the area in the other tail (here the left tail) similar to what we observed in the
right tail. Some people suggest doubling the area in one tail but we will collect
information on the number that were more as or more extreme than the same
value in the other
tail. In other words, we count the proportion over 1.84 and below -1.84. So
we need to also find how many of the permuted results were smaller than or equal
to -1.84 years
to add to our previous proportion. Using <code>pdata</code> with <code>-Tobs</code> as the cut-off
and <code>lower.tail=T</code> provides this result:
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(Tstar, <span class="op">-</span>Tobs, <span class="dt">lower.tail=</span>T)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.014</code></pre>
<p>So the p-value to test our null hypothesis of no difference in the true means
between the groups is 0.02 + 0.014, providing a p-value of 0.034.
Figure <a href="acknowledgments.html#fig:Figure2-11">0.20</a> shows both cut-offs on the histogram and density curve.</p>
<p>(ref:fig2-11) Histogram and density curve of values of test statistic for 1,000
permutations with bold lines for value of observed test statistic (1.84) and its
opposite value (-1.84) required for performing the two-sided test.</p>
<div class="figure"><span id="fig:Figure2-11"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-11-1.png" alt="(ref:fig2-11)" width="960" />
<p class="caption">
Figure 0.20: (ref:fig2-11)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre>
<p>In general, the <strong><em>one-sided test p-value</em></strong>

is the proportion of the
permuted results that are as extreme or more extreme than observed in the
direction of the <em>alternative</em>
hypothesis (lower or upper tail, remembering that this also depends on the
direction of the difference taken). For the 2-sided test, the p-value

is the
proportion of the permuted results that are <em>less than or equal to the negative
version of the observed statistic and greater than or equal to the positive
version of the observed statistic</em>. Using absolute
values (| |), we can simplify this: the <strong><em>two-sided p-value</em></strong> is the
<em>proportion of the |permuted statistics| that are as large or larger than
|observed statistic|</em>.
This will always work and finds areas in both tails regardless of whether the
observed statistic is positive or negative. In R, the <code>abs</code> function provides the
<strong><em>absolute value</em></strong> and we can again use <code>pdata</code> to find our p-value in one line
of code:
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.034</code></pre>
<p>We will encourage you to think through what might constitute strong evidence
against your null hypotheses and then discuss how strong you feel the evidence
is against the null hypothesis in the p-value that you obtained. Basically,
p-values present a measure of evidence against the null hypothesis,

with smaller
values presenting more evidence against the null. They range from 0 to 1 and you
should interpret them on a graded scale from strong evidence (close to 0) to no
evidence (close 1). We will discuss the use of a fixed <strong><em>significance level</em></strong>
below as it is still commonly used in many fields and is necessary to think
about the theory of hypothesis testing, but, for the moment,
we can conclude that there is moderate evidence against the null hypothesis
presented by having a p-value of 0.034 because our observed result is somewhat
rare relative to what we would expect if the null hypothesis was true. And so
we would likely <strong><em>reject the null hypothesis</em></strong> and conclude (in the direction
of the alternative) that there is a difference in the population means in the
two groups.</p>
<p>Before we move on, let’s note some interesting features of the permutation
distribution of the difference in the sample means shown in
Figure <a href="acknowledgments.html#fig:Figure2-11">0.20</a>. </p>
<ol style="list-style-type: decimal">
<li><p>It is basically centered at 0. Since we are performing permutations assuming
the null model is true, we are assuming that <span class="math inline">\(\mu_1 = \mu_2\)</span> which implies that
<span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>. This also suggests that 0 should be the center of the
permutation distribution and it was.</p></li>
<li><p>It is approximately normally distributed. This is due to the <strong><em>Central Limit Theorem</em></strong><a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>, where the
<strong><em>sampling distribution</em></strong> (distribution of all possible results for samples
of this size) of the difference in sample means (<span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) becomes
more normally distributed as the sample sizes increase. With 38 and 37
observations in the groups, we are likely to have a relatively normal looking
distribution of the difference in the sample means. This result will allow us to
use a parametric

method to approximate this sampling distribution under the null
model if some assumptions are met, as we’ll discuss below.</p></li>
<li><p>Our observed difference in the sample means (1.84 years) is a fairly unusual
result relative to the rest of these results but there are some permuted data
sets that produce more extreme differences in the sample means. When the
observed differences are really large, we may not see any permuted results
that are as extreme as what we observed. When <code>pdata</code> gives you 0, the p-value

should be reported to be smaller than 0.0001 (<strong>not 0!</strong>) since it happened
in less than 1 in 1,000 tries but does occur once – in the actual data set.</p></li>
<li><p>Since our null model is not specific about the direction of the difference,
considering a result like ours but in the other direction (-1.84 years) needs to
be included. The observed result seems to put about the same area in both tails
of the distribution but it is not exactly the same. The small difference in the
tails is a useful aspect of this approach compared to the parametric method
discussed below as it accounts for potential asymmetry in the sampling distribution.</p></li>
</ol>
<p>Earlier, we decided that the p-value provided moderate evidence against
the null hypothesis and that we should reject the null. In this
course, you will often be allowed to use your own judgment about an appropriate
significance level in a particular situation (in other words, if we forget to
tell you an <span class="math inline">\(\alpha\)</span> -level, you can still make a decision based on how strong
you feel the evidence was against the null based on the p-value). Remembering
that the p-value is the probability

you would observe a result like you did (or more extreme), assuming the null
hypothesis is true; this tells you that the smaller the p-value is, the more
evidence you have against the null. Figure <a href="acknowledgments.html#fig:FigurePValStr">0.21</a> provides a
diagram of some suggestions for the graded p-value interpretation that you can
use. The next section provides a more formal
review of the hypothesis testing

infrastructure, terminology, and some of
things that can happen when testing hypotheses. P-values have been (validly)

criticized for the inability of studies to be reproduced, for the bias in
publications to only include studies that have small p-values, and for the lack of
thought that often accompanies using a fixed significance level. To alleviate
some of these criticisms, we recommend reporting the strength of evidence of the
result based on the p-value and also reporting and discussing the size of the
estimated results (with a measure of precision of the estimated difference).</p>
<p>(ref:figPValStr) Graphic suggesting potential interpretations of strength of
evidence based on gradient of p-values.</p>
<div class="figure"><span id="fig:FigurePValStr"></span>
<img src="chapter2_files/pvalueStrengths.png" alt="(ref:figPValStr)" width="800" />
<p class="caption">
Figure 0.21: (ref:figPValStr)
</p>
</div>
</div>
<div id="section2-5" class="section level2">
<h2><span class="header-section-number">0.4</span> Hypothesis testing (general)</h2>
<p>In hypothesis testing

(sometimes more explicitly called “Null Hypothesis
Significance Testing” or NHST), it is formulated to answer a specific question about
a population or true parameter(s) using a statistic based on a data set.
In your previous statistics course, you (hopefully) considered one-sample
hypotheses about population means and proportions and the two sample mean
situation we are focused on here. Our hypotheses relate to trying to answer
the question about whether the population mean sentences between the two
groups are different, with an initial assumption of no difference.</p>
<p>NHST is much like a criminal trial with a jury where you are in the role
of a jury member. Initially, the defendant
is assumed innocent. In our situation, the true means are assumed to be
equal between the groups. Then evidence is presented and, as a juror,
you analyze it. In statistical hypothesis testing,

data are collected and
analyzed. Then you have to decide if we had “enough” evidence to reject
the initial assumption (“innocence” that is initially assumed). To make
this decision, you want to have thought about and decided on the standard of
evidence required to reject the initial assumption. In criminal cases,
“beyond a reasonable doubt” is used. Wikipedia’s definition
(<a href="https://en.wikipedia.org/wiki/Reasonable_doubt" class="uri">https://en.wikipedia.org/wiki/Reasonable_doubt</a>) suggests that this
standard is that “there can still be a doubt, but only to the extent
that it would not affect a reasonable person’s belief regarding whether
or not the defendant is guilty”. In civil trials, a lower standard called
a “preponderance of evidence” is used. Based on that defined and pre-decided
(<em>a priori</em>) measure, you decide that the defendant is guilty or not guilty.
In statistics, we can compare our p-value

to a significance level, <span class="math inline">\(\alpha\)</span>,
which is most of the time selected to be 5%. If our p-value is less than
<span class="math inline">\(\alpha\)</span>, we reject the null hypothesis. The choice of the significance level
is like the variation in standards of evidence between criminal and civil
trials – and in all situations everyone should know the standards required
for rejecting the initial assumption before any information is “analyzed”.
Once someone is found guilty, then there is the matter of sentencing which
is related to the impacts (“size”) of the crime. In statistics, this is
similar to the estimated size of differences and the related judgments
about whether the differences are practically important or not. If the
crime is proven beyond a reasonable doubt but it is a minor crime, then
the sentence will be small. With the same level of evidence and a more
serious crime, the sentence will be more dramatic.</p>
<p>There are some important aspects of the testing process to note that inform
how we interpret statistical hypothesis test results. When someone is found
“not guilty”, it does not mean “innocent”, it just means that there was not
enough evidence to find the person guilty “beyond a reasonable doubt”.
Not finding enough evidence to reject the null hypothesis does not imply
that the true means are equal, just that there was not enough evidence to
conclude that they were different. There are many potential reasons why we
might fail to reject the null, but the most common one is that our sample
size was too small (which is related to having too little evidence). Other
reasons include simply the variation in taking a random sample  from the
population(s). This randomness in samples and the differences in the sample
means also implies that p-values are random and can easily vary if the data set
had been slightly different. This also relates to the suggestion of using a
graded interpretation of p-values – if the p-value is an estimated quantity,
is there really any difference between p-values of 0.049 and 0.051? We
probably shouldn’t think there is a big difference in results for these two
p-values even though the standard NHST reject/fail to reject the null approach
considers these as completely different results. So where does that leave us?
Interpret the p-values

using strength of evidence against the null hypothesis,
remembering that smaller (but not really small) p-values can still be
interesting. And if you think the p-value is small enough, then you can reject
the null hypothesis.</p>
<p>Throughout this material, we will continue to re-iterate the distinctions
between parameters and statistics and want you to be clear about the
distinctions between estimates based on the sample and inferences for the
population or true values of the parameters of interest. Remember that
statistics are summaries of the sample information and parameters are
characteristics of populations (which we rarely know). In the two-sample
mean situation, the sample means are always at least a little different
– that is not an interesting conclusion. What is interesting is whether
we have enough evidence to feel like we have proven that the population or true means
differ “beyond a reasonable doubt”.</p>
<p>The scope of any inferences is constrained based on whether there is a
<strong><em>random sample</em></strong> (RS) and/or <strong><em>random assignment</em></strong> (RA).   
Table <a href="acknowledgments.html#tab:Table2-2">0.2</a> contains the four possible combinations of these
two characteristics of a given study. Random assignment of treatment levels to
subjects allows for causal
inferences for differences that are observed – the difference in treatment
levels is said to cause differences in the mean responses. Random sampling (or
at least some sort of representative sample) allows inferences to be made to
the population of interest. If we do not have RA, then causal inferences cannot
be made. If we do not have a representative sample, then our inferences are
limited to the sampled subjects.
</p>


<table>
<caption><span id="tab:Table2-2">Table 0.2: </span> Scope of inference summary.</caption>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Random<br />
Sampling/Random<br />
Assignment</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– Yes (controlled
experiment)</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– No (observational study)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Random Sampling (RS)<br />
– Yes (or some method<br />
that results in a<br />
representative sample of<br />
population of<br />
interest)</strong></td>
<td align="left">Because we have RS, we can<br />
generalize inferences to the<br />
population the RS was taken<br />
from. Because we have<br />
RA we can assume the groups<br />
were equivalent on all
aspects<br />
except for the treatment<br />
and can establish causal
inference.<br />
</td>
<td align="left">Can generalize inference to<br />
population the RS was taken<br />
from but cannot establish<br />
causal inference (no RA<br />
– cannot isolate treatment<br />
variable as only difference<br />
among groups, could be<br />
confounding variables).</td>
</tr>
<tr class="even">
<td align="left"><strong>Random Sampling (RS)<br />
– No (usually a<br />
convenience sample)</strong></td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Can establish causal<br />
inference due to RA
<span class="math inline">\(\rightarrow\)</span><br />
the inference from this type
of<br />
study applies only to the
sample.</td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Cannot establish causal<br />
inference due to lack of RA
of<br />
the treatment.</td>
</tr>
</tbody>
</table>

<p>A simple example helps to clarify how the scope of inference can change
based on the study design.  Suppose
we are interested in studying the GPA of students. If we had taken a random
sample from, say, the STAT 217 students in a given semester, our scope of
inference would be the population of STAT 217 students in that semester. If we had
taken a random sample  from the entire MSU population, then the inferences would
be to the entire MSU population in that semester. These are similar types of
problems but the two populations are very different and the group you are trying
to make conclusions about should be noted carefully in your results – it does
matter! If we did not have a representative sample, say the students could
choose to provide this information or not, then we can only make inferences to
volunteers. These volunteers might differ in systematic ways from the entire
population of STAT 217 students so we cannot safely extend our inferences beyond
the group that volunteered.</p>
<p>To consider the impacts of RA versus results from purely observational
studies, we need to be
comparing groups. Suppose that we are interested in differences in the mean
GPAs for different sections of STAT 217 and that we take a random sample  of
students from each section and compare the results and find evidence of some
difference. In this scenario, we can conclude that there is some difference in
the population of STAT 217 students but we can’t say that being in different
sections caused the differences in the mean GPAs. Now suppose that we randomly
assigned every STAT 217 student to get extra training in one of three different
study techniques and found evidence of differences among the training methods.
We could conclude that the training methods caused the differences in these
students. These conclusions would only apply to STAT 217 students and could
not be generalized to a larger population of students. If we took a random
sample of STAT 217 students (say only 10 from each section) and then randomly
assigned them to one of three training programs and found evidence of
differences, then we can say that the training programs caused the differences.
But we can also say that we have evidence that those differences pertain to the
population of STAT 217 students. This seems similar to the scenario where all
STAT 217 students participated in the training programs except that by using random
sampling, only a fraction of the population needs to actually be studied to
make inferences to the entire population of interest – saving time and money.</p>
<p>A quick summary of the terminology of hypothesis testing

is useful at this
point. The <strong><em>null hypothesis</em></strong> (<span class="math inline">\(H_0\)</span>) states that there is no difference
or no relationship in the population. This is the statement of no effect or
no difference and the claim that we are trying to find evidence against. In
this chapter, <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2\)</span>. When doing two-group problems, you
always need to specify which group is 1 and which one is 2 because the order
does matter. The <strong><em>alternative hypothesis</em></strong> (<span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_A\)</span>) states a
specific difference between parameters. This is the research hypothesis and
the claim about the population that we hope to demonstrate is more reasonable
to conclude than the null hypothesis. In the two-group situation, we can have
<strong><em>one-sided alternatives</em></strong> <span class="math inline">\(H_A: \mu_1 &gt; \mu_2\)</span> (greater than) or
<span class="math inline">\(H_A: \mu_1 &lt; \mu_2\)</span> (less than) or, the more common, <strong><em>two-sided
alternative</em></strong> <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span> (not equal to). We usually default to
using two-sided tests because we often do not know enough to know the
direction of a difference in advance, especially in more complicated
situations. The <strong><em>sampling distribution under the null</em></strong> is the
distribution of all possible values of a statistic under the assumption that
<span class="math inline">\(H_0\)</span> is true. It
is used to calculate the <strong><em>p-value</em></strong>,

the probability of obtaining a
result as extreme or more extreme than what we observed given that the null
hypothesis is true. We will find sampling distributions

using
<strong><em>nonparametric</em></strong>

approaches (like the permutation  approach used previously)
and <strong><em>parametric</em></strong>

methods (using “named” distributions like the
<span class="math inline">\(t\)</span>, F, and <span class="math inline">\(\chi^2\)</span>).</p>
<p>Small p-values are evidence against the null hypothesis

because the observed
result is unlikely due to chance if <span class="math inline">\(H_0\)</span> is true. Large p-values provide
little to no evidence against <span class="math inline">\(H_0\)</span> but do not allow us to conclude that the null
hypothesis is correct – just that we didn’t find enough evidence to think it
was wrong. The <strong><em>level of significance</em></strong> is an <em>a priori</em> definition of
how small the p-value needs to be to provide “enough” (sufficient) evidence
against <span class="math inline">\(H_0\)</span>. This is most useful to prevent sliding the standards after
the results are found. We can compare the p-value to the level of significance to
decide if the p-value is small enough to constitute sufficient evidence to
reject the null hypothesis. We use <span class="math inline">\(\alpha\)</span> to denote the level of
significance and most typically use 0.05 which we refer to as the 5%
significance level. We can compare the p-value to this level and make a
decision, focusing our interpretation on the strength of evidence we found
based on the p-value from very strong to none.
If we are using the strict version of NHST, the two options for <em>decisions</em> are
to either <em>reject the null hypothesis</em>
if the p-value <span class="math inline">\(\le \alpha\)</span> or <em>fail to reject the null hypothesis</em> if the
p-value <span class="math inline">\(&gt; \alpha\)</span>. When interpreting hypothesis testing results, remember
that the p-value is a measure of how unlikely the observed outcome was,
assuming that the null hypothesis is true. It is <strong>NOT</strong> the probability of
the data or the probability of either hypothesis being true. The p-value,
simply, is a measure of evidence against the null hypothesis.</p>
<p>Although we want to use graded evidence to interpret p-values, there
is one situation where thinking about comparisons to fixed <span class="math inline">\(\alpha\)</span> levels is
useful for understanding and studying statistical hypothesis testing. The
specific definition of <span class="math inline">\(\alpha\)</span> is that it is the probability of rejecting
<span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true, the probability of what is called a
<strong><em>Type I error</em></strong>.  Type I errors are also called <strong><em>false rejections</em></strong> or
<strong><em>false detections</em></strong>. In the two-group mean situation, a Type I error would
be concluding that there
is a difference in the true means between the groups when none really exists
in the population. In the courtroom setting, this is like falsely finding
someone guilty. We don’t want to do this very often, so we use small values
of the significance level, allowing us to control the rate of Type I errors
at <span class="math inline">\(\alpha\)</span>.  We also have to worry about <strong>Type II errors</strong>,  which are failing
to reject the null hypothesis when it’s false. In a courtroom, this is the same
as failing to convict a truly guilty person. This most often occurs due to a
lack of evidence that could be due to a small sample size or merely just an
unusual sample from the population. You can use the Table <a href="acknowledgments.html#tab:Table2-3">0.3</a>
to help you remember all the possibilities.</p>
<p>
</p>
<p>(ref:tab2-3) Table of decisions and truth scenarios in a hypothesis
testing situation. But we never know the truth in a real situation.</p>
<table>
<caption><span id="tab:Table2-3">Table 0.3: </span> (ref:tab2-3)</caption>
<colgroup>
<col width="34%" />
<col width="32%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>True</strong></th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>False</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>FTR</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Correct decision</td>
<td align="left">Type II error</td>
</tr>
<tr class="even">
<td align="left"><strong>Reject</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Type I error</td>
<td align="left">Correct decision</td>
</tr>
</tbody>
</table>
<p>In comparing different procedures or in planning studies, there is an
interest in studying the rate or
probability of Type I and II errors. The probability of a Type I error was
defined previously as <span class="math inline">\(\alpha\)</span>, the significance level. The <strong><em>power</em></strong> of a
procedure is the probability of rejecting the null hypothesis when it is false.   
Power is defined as</p>
<p><span class="math display">\[\text{Power} = 1 - \text{Probability(Type II error) } = 
\text{Probability(Reject } H_0 | H_0 \text{ is false),}\]</span></p>
<p>
</p>
<p>or, in words, the probability of detecting a difference when it actually
exists. We want to use a statistical procedure that controls the Type I error
rate at the pre-specified level and has high power to detect false null
hypotheses. Increasing the sample size is one of the most commonly used
methods for increasing the power  in a given situation. Sometimes we can choose
among different procedures and use the power of the procedures to help us make
that selection. Note that there are many ways <span class="math inline">\(H_0\)</span> false and the power changes
based on how false the null hypothesis actually is. To make this concrete,
suppose that the true mean sentences differed by either 1 or 20 years in
previous example. The chances of rejecting the null hypothesis are much larger
when the groups actually differ by 20 years than if they differ by just 1 year,
given the same sample size. For a given difference in the true means, the larger
the sample, the higher the power  of the study to actually find evidence of a
difference in the groups.</p>
<p>After making a decision (was there enough evidence to reject the null
or not), we want to make the conclusions specific to the problem of interest.
If we reject <span class="math inline">\(H_0\)</span>, then we can conclude that there was sufficient evidence at
the <span class="math inline">\(\alpha\)</span>-level that the null hypothesis is wrong (and the results point in
the direction of the alternative). If we fail to reject <span class="math inline">\(H_0\)</span> (FTR <span class="math inline">\(H_0\)</span>), then
we can conclude that there was insufficient evidence at the <span class="math inline">\(\alpha\)</span>-level to say
that the null hypothesis is wrong. We are <strong>NOT</strong> saying that the null is
correct and we <strong>NEVER</strong> accept the null hypothesis. We just failed to find
enough evidence to say it’s wrong. If we find sufficient evidence to reject the
null, then we need to revisit the method of data collection and design of the
study to discuss scope of inference.  Can we discuss causality (due to RA) and/or
make inferences to a larger group than those in the sample (due to RS)?</p>
<p>To perform a hypothesis test, there are some steps to remember to
complete to make sure you have thought through all the aspects of the results.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><strong>Outline of 6+ steps to perform a Hypothesis Test</strong></td>
</tr>
<tr class="even">
<td align="left">Isolate the claim to be proved, method to use (define a test statistic T),
and significance level.</td>
</tr>
<tr class="odd">
<td align="left">1. Write the null and alternative hypotheses,</td>
</tr>
<tr class="even">
<td align="left">2. Plot the data and assess the “Validity Conditions” for the procedure being
used (discussed below),</td>
</tr>
<tr class="odd">
<td align="left">3. Find the value of the appropriate test statistic,</td>
</tr>
<tr class="even">
<td align="left">4. Find the p-value, </td>
</tr>
<tr class="odd">
<td align="left">5. Make a decision, and</td>
</tr>
<tr class="even">
<td align="left">6. Write a conclusion specific to the problem, including scope of
inference discussion.  Report and discuss an estimate of the differences.</td>
</tr>
</tbody>
</table>
</div>
<div id="section2-6" class="section level2">
<h2><span class="header-section-number">0.5</span> Connecting randomization (nonparametric) and parametric tests</h2>
<p>In developing statistical inference techniques, we need to define the test
statistic, <span class="math inline">\(T\)</span>, that measures the quantity of interest. To compare the means of
two groups, a statistic is needed
that measures their differences. In general, for comparing two groups, the
choices are simple – a difference in the means often works well and is a
natural choice. There are other options such as tracking the ratio of means or
possibly the difference in medians. Instead of just using the difference in the
means, we also could “standardize” the difference in the means by dividing by
an appropriate quantity that reflects the variation in the difference in the
means. All of these are valid and can sometimes provide similar results - it
ends up that there are many possibilities for testing using the randomization
(nonparametric)

techniques introduced previously. Parametric

statistical
methods focus on means because the statistical theory surrounding means is
quite a bit easier (not easy, just easier) than other options but there are
just a couple of test statistics that you can use and end up with named
distributions to use for generating inferences. Randomization techniques allow
inference for other quantities but our focus here will be on using
randomization for inferences on means to see the similarities with the more
traditional parametric procedures.</p>
<p>In two-sample mean situations, instead of working just with the
difference in the means, we often calculate a test statistic that is called the
<strong><em>equal variance two-independent samples t-statistic</em></strong>. The test statistic is</p>
<p><span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},\]</span></p>
<p>where <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> are the sample variances for the two groups, <span class="math inline">\(n_1\)</span> and
<span class="math inline">\(n_2\)</span> are the sample sizes for the two groups, and the
<strong><em>pooled sample standard deviation</em></strong>,</p>
<p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}.\]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic keeps the important comparison between the means in the
numerator that we used before and standardizes (re-scales) that difference so
that <span class="math inline">\(t\)</span> will follow a <span class="math inline">\(t\)</span>-distribution

(a parametric

“named” distribution) if
certain assumptions are met. But first we should see if standardizing the
difference in the means had an impact on our permutation test

results. Instead
of using the <code>diffmean</code> function, we will use the <code>t.test</code> function (see
its full use below) and have it calculate the formula for <span class="math inline">\(t\)</span> for us. The R
code “<code>$statistic</code>” is basically a way of extracting just the number we want
to use for <span class="math inline">\(T\)</span> from a larger set of output the <code>t.test</code> function wants to
provide you. We will see below that <code>t.test</code> switches the order of the
difference (now it is <em>Average</em> - <em>Unattractive</em>) – always carefully check for
the direction of the difference in the results. Since we are doing a two-sided
test, the code resembles the permutation test code in Section <a href="acknowledgments.html#section2-4">0.3</a>
with the new <span class="math inline">\(t\)</span>-statistic replacing the difference in the sample means that we
used before.</p>
<p>The permutation distribution

in Figure <a href="acknowledgments.html#fig:Figure2-12">0.22</a> looks
similar to the previous results with slightly different x-axis scaling. The
observed <span class="math inline">\(t\)</span>-statistic was <span class="math inline">\(-2.17\)</span> and the proportion of permuted results that
were as or more extreme than the observed result
was 0.031. This difference is due to a different set of random permutations
being selected. If you run permutation code, you will often get slightly
different results each time you run it. If you are uncomfortable with the
variation in the results, you can run more than <span class="math inline">\(B=\)</span> 1,000 permutations (say
10,000) and the variability in the resulting p-values will be reduced further.
Usually this uncertainty will not cause any substantive problems – but do not
be surprised if your results vary from a colleagues if you are both analyzing
the same data set or if you re-run your permutation code.</p>

<div class="figure"><span id="fig:Figure2-12"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-12-1.png" alt="Permutation distribution of the \(t\)-statistic." width="960" />
<p class="caption">
Figure 0.22: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">t.test</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
Tobs</code></pre>
<pre><code>##        t 
## -2.17023</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">t.test</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
}</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.031</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre>
<p>The parametric version

of these results is based on using what is called
the <strong><em>two-independent sample t-test</em></strong>. There are actually two versions of this
test, one that assumes that variances are equal in the groups and one that
does not. There is a rule of thumb that if the <strong>ratio of the larger standard
deviation over the smaller standard deviation is less than 2, the equal variance
procedure is OK</strong>. It ends up that this assumption
is less important if the sample sizes in the groups are approximately equal
and more important if the groups contain different numbers of observations. In
comparing the two potential test statistics, the procedure that assumes equal
variances has a complicated denominator (see the formula above for <span class="math inline">\(t\)</span>
involving <span class="math inline">\(s_p\)</span>) but a simple formula for <strong><em>degrees of freedom</em></strong> (<strong><em>df</em></strong>)


for the <span class="math inline">\(t\)</span>-distribution (<span class="math inline">\(df=n_1+n_2-2\)</span>) that approximates the
distribution of the test statistic, <span class="math inline">\(t\)</span>, under the null hypothesis. The
procedure that assumes unequal variances has a simpler test statistic and a
very complicated degrees of freedom formula. The equal variance procedure is
most similar to the ANOVA methods we will consider in Chapters
<a href="#chapter3"><strong>??</strong></a> and <a href="chapter4.html#chapter4">1</a> so that
will be our focus for the two group problem. Fortunately, both of these methods
are readily available in the <code>t.test</code> function in R if needed.</p>
<p>(ref:fig2-13) Plots of <span class="math inline">\(t\)</span>-distributions with 2, 10, and 20 degrees of freedom
and a normal distribution (dashed line). Note how the <span class="math inline">\(t\)</span>-distributions get
closer to the normal distribution as the degrees of freedom increase and at 20
degrees of freedom, the <span class="math inline">\(t\)</span>-distribution <em>almost</em> matches a standard normal
curve.</p>
<div class="figure"><span id="fig:Figure2-13"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-13-1.png" alt="(ref:fig2-13)" width="672" />
<p class="caption">
Figure 0.23: (ref:fig2-13)
</p>
</div>
<p>If the assumptions for the equal variance <span class="math inline">\(t\)</span>-test are met and the null
hypothesis is true, then the sampling distribution of the test statistic should
follow a <span class="math inline">\(t\)</span>-distribution

with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom. The <strong><em>t-distribution</em></strong> is a bell-shaped curve that is more spread out for smaller
values of degrees of freedom as shown in Figure <a href="acknowledgments.html#fig:Figure2-13">0.23</a>. The
<span class="math inline">\(t\)</span>-distribution looks more and more like a <strong><em>standard normal distribution</em></strong>

(<span class="math inline">\(N(0,1)\)</span>) as the degrees of freedom increase.</p>
<p>To get the p-value for the parametric <span class="math inline">\(t\)</span>-test,

we need to calculate the
test statistic and <span class="math inline">\(df\)</span>, then look up the areas in the tails of the
<span class="math inline">\(t\)</span>-distribution

relative to the observed <span class="math inline">\(t\)</span>-statistic. We’ll learn how to use
R to do this below, but for now we will allow the <code>t.test</code> function to take
care of this. The <code>t.test</code> function uses our formula notation (<code>Years~Attr</code>)
and then <code>data=...</code> as we saw before for making plots. To get the
equal-variance test result, the <code>var.equal=T</code> option needs to be turned on.
Then <code>t.test</code> provides us with lots of useful output. The three results we’ve
been discussing are available in the output below – the test statistic value
(-2.17), <span class="math inline">\(df=73\)</span>, and the p-value, from the <span class="math inline">\(t\)</span>-distribution with 73 degrees of freedom, of 0.033.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(Years <span class="op">~</span><span class="st"> </span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811</code></pre>
<p>So the parametric <span class="math inline">\(t\)</span>-test gives a p-value of 0.033 from a test statistic of
-2.1702. The negative sign on the test statistic occurred because the function
took <em>Average</em> - <em>Unattractive</em> which is the opposite direction as <code>diffmean</code>.
The p-value is very similar to the two permutation results found before. The
reason for this similarity is that the permutation distribution looks an awful
like a <span class="math inline">\(t\)</span>-distribution with 73 degrees
of freedom. Figure <a href="acknowledgments.html#fig:Figure2-14">0.24</a> shows how similar the two distributions
happened to be here.</p>
<p>(ref:fig2-14) Plot of permutation and <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=73\)</span>. Note the
close match in the two distributions, especially in the tails of the
distributions where we are obtaining the p-values.</p>
<div class="figure"><span id="fig:Figure2-14"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-14-1.png" alt="(ref:fig2-14)" width="576" />
<p class="caption">
Figure 0.24: (ref:fig2-14)
</p>
</div>
<p>In your previous statistics course, you might have used an applet or
a table to find p-values such as what was provided in the previous R output.
When not directly provided in the output of a function, R can be used to look up
p-values<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> from
named distributions such as the <span class="math inline">\(t\)</span>-distribution. In this case, the distribution
of the test statistic under the null hypothesis is a <span class="math inline">\(t(73)\)</span> or a <span class="math inline">\(t\)</span> with 73
degrees of freedom. The <code>pt</code> function is used to get p-values from the

<span class="math inline">\(t\)</span>-distribution in the same manner that <code>pdata</code> could help us to find p-values
from the permutation distribution.

We need to provide the <code>df=...</code> and specify
the tail of the distribution of interest using the <code>lower.tail</code> option along
with the cutoff of interest. If we want the area to the left of -2.17:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.1702</span>, <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>T)</code></pre>
<pre><code>## [1] 0.01662286</code></pre>
<p>And we can double it to get the p-value that <code>t.test</code> provided earlier, because
the <span class="math inline">\(t\)</span>-distribution is symmetric:
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.1702</span>, <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>T)</code></pre>
<pre><code>## [1] 0.03324571</code></pre>
<p>More generally, we could always make the test statistic positive using the
absolute value (<code>abs</code>), find the area to the right of it (<code>lower.tail=F</code>), and then double that for a
two-sided test p-value:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="kw">abs</span>(<span class="op">-</span><span class="fl">2.1702</span>), <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>F)</code></pre>
<pre><code>## [1] 0.03324571</code></pre>
<p>Permutation distributions  do not need to match the named
parametric distribution

to work correctly, although this happened in the previous example.
The parametric approach, the <span class="math inline">\(t\)</span>-test, requires certain conditions to be met
for the sampling distribution of the
statistic to follow the named distribution and provide accurate p-values. The
conditions for the equal variance t-test are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>:

Each observation obtained is unrelated to all other
observations. To assess this, consider whether anything in the data collection
might lead to clustered or related observations that are un-related to the
differences in the groups. For example, was the same person measured more than
once<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a>?</p></li>
<li><p><strong>Equal variances</strong> in the groups (because we used a procedure that assumes
equal variances! – there is another procedure that allows you to relax this
assumption if needed…). To assess this, compare the standard deviations and
variability in the beanplots  and see if they look noticeably different. Be
particularly critical of this assessment if the sample sizes differ greatly
between groups.</p></li>
<li><p><strong>Normal distributions</strong> of the observations in each group. We’ll learn more
diagnostics later, but the boxplots and beanplots are a good place to start to
help you look for skews or outliers, which were both present here. If you find
skew and/or outliers, that would suggest a problem with the assumption of
normality as normal distributions
  
are symmetric and extreme observations occur
very rarely.</p></li>
</ol>
<p>For the permutation test,  we relax the third condition and replace it with:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em>Similar distributions for the groups:</em></strong> The permutation approach
allows valid inferences as long as the two groups have similar shapes and only
possibly differ in their centers. In other words, the distributions need not
look normal for the procedure to work well, but they do need to look similar.
</li>
</ol>
<p>In the prisoner “juror” study, we can assume that the independent
observation condition is met because there is no information suggesting that
the same subjects were measured more than once or that some other type of
grouping in the responses was present (like the subjects were divided in
groups and placed in rooms to discuss their responses prior to submitting
them). The equal variance condition might be violated. The variances need
not be equal as the procedure
can still provide reasonable results with some violation of this assumption.
The standard deviations are 2.8 vs 4.4, so this difference is not “large”
according to the rule of thumb noted above. It is, however, close to being
considered problematic. It would be difficult to reasonably assume that the
normality condition is met here (Figure <a href="acknowledgments.html#fig:Figure2-6">0.15</a>) with clear right
skews in both groups and potential outliers which causes concerns for (3) for
the parametric procedure. The shapes look similar for the two groups so there
is less reason to be concerned with using the permutation approach based on
its version of condition (3) above.</p>
<p>The permutation approach is resistant

to impacts of violations of the
normality assumption. It is not resistant to impacts of violations of any of
the other assumptions.

In fact, it can be quite sensitive to unequal variances
as it will detect differences in the variances of the groups instead of
differences in the means. Its scope of inference is the same as the parametric
approach.  It also can provide similarly inaccurate conclusions in the presence
of non-independent observations as for the parametric approach. In this
example, we discover that parametric and permutation approaches provide very
similar inferences.</p>
</div>
<div id="section2-7" class="section level2">
<h2><span class="header-section-number">0.6</span> Second example of permutation tests</h2>
<p>In every chapter, the first example, used to motivate and explain
the methods, is followed with a “worked” example where we focus just on the
results. In a
previous semester, some of the STAT 217 students (<strong><em>n</em></strong>=79) provided
information on their <em>Sex</em><a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a>, <em>Age</em>, and current cumulative <em>GPA</em>. We might be
interested in whether Males and Females had different average GPAs. First,
we can take a look at the difference in the responses by groups based on the
output and as displayed in Figure <a href="acknowledgments.html#fig:Figure2-15">0.25</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r">s217 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/s217.csv&quot;</span>)
<span class="kw">require</span>(mosaic)
<span class="kw">require</span>(beanplot)</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre>
<pre><code>##        F        M 
## 3.338378 3.088571</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>

<div class="figure"><span id="fig:Figure2-15"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-15-1.png" alt="Side-by-side boxplot and beanplot of GPAs of STAT 217 students by sex." width="480" />
<p class="caption">
Figure 0.25: Side-by-side boxplot and beanplot of GPAs of STAT 217 students by sex.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)
<span class="kw">beanplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;lightblue&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre>
<p>In these data, the distributions of the GPAs look to be left skewed but maybe not
as dramatically as the responses were right-skewed in the MockJury example. The
Female GPAs look to be slightly higher than for Males (0.25 GPA difference in the
means) but is that a “real” difference? We need our inference tools to more fully
assess these differences.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre>
<pre><code>##   diffmean 
## -0.2498069</code></pre>
<p>First, we can try the parametric approach:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>So the test statistic was observed to be <span class="math inline">\(t=2.69\)</span> and it hopefully
follows a <span class="math inline">\(t(77)\)</span> distribution under the null hypothesis. This provides a
p-value of 0.008713 that we can trust if all the conditions to use this
procedure are met.
Compare these results to the permutation approach, which relaxes that normality
assumption, with the results that follow. In the permutation test, <span class="math inline">\(T=2.692\)</span> and
the p-value is 0.005 which is a little smaller than the result provided
by the parametric approach. The agreement of the two approaches, again, provides
some re-assurance about the use of either approach. </p>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">t.test</span>(GPA<span class="op">~</span><span class="kw">shuffle</span>(Sex), <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
}
<span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<p>(ref:fig2-16) Histogram and density curve of permutation distribution of test
statistic for STAT 217 GPAs.</p>
<div class="figure"><span id="fig:Figure2-16"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-16-1.png" alt="(ref:fig2-16)" width="960" />
<p class="caption">
Figure 0.26: (ref:fig2-16)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre>
<p>Here is a full write-up of the results using all 6+ hypothesis testing

steps,
using the permutation  results for the grade data:</p>
<ol start="0" style="list-style-type: decimal">
<li><p><em>Isolate the claim to be proved and method to use (define a test statistic T)</em>.
We want to test for a difference in the means between males and females and
will use the equal variance two-sample t-test statistic to compare them, and
we think that the 5% significance level seems reasonable in this situation.</p></li>
<li><p>Write the null and alternative hypotheses</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_\text{male} = \mu_\text{female}\)</span></p>
<ul>
<li>where <span class="math inline">\(\mu_\text{male}\)</span> is the true mean GPA for males and
<span class="math inline">\(\mu_\text{female}\)</span> is true mean GPA for females.</li>
</ul></li>
<li><p><span class="math inline">\(H_A: \mu_\text{male} \ne \mu_\text{female}\)</span></p></li>
</ul></li>
<li><p>Check conditions for the procedure being used</p>
<ul>
<li><p><strong>Independent observations condition</strong>: It appears that this assumption
is met because there is no reason to assume any clustering or grouping of
responses that might create dependence in the observations. The only
possible consideration is that the observations were taken from different
sections and there could be some differences between the sections. However,
for overall GPA there is not too much likelihood that the overall GPAs
would vary greatly so this not likely to be a big issue. The only way this
could create a violation here is if certain sections tended to attract
students with different GPA levels (such as the 9 am section had the
best/worst GPA students…).</p></li>
<li><p><strong>Equal variance condition</strong>: There is a small difference in the range
of the observations in the two groups but the standard deviations are very
similar so there is effectively no evidence that this condition is violated.</p></li>
<li><p><strong>Similar distribution condition</strong>: Based on the side-by-side boxplots and
beanplots, it appears that both groups have slightly left-skewed
distributions, which could be problematic for the parametric approach, but
there is essentially no evidence that the permutation approach condition is violated
since the distributions look to have fairly similar shapes.</p></li>
</ul></li>
<li><p>Find the value of the appropriate test statistic</p>
<ul>
<li><span class="math inline">\(T=2.69\)</span> from the previous R output.</li>
</ul></li>
<li><p>Find the p-value</p>
<ul>
<li><p>p-value=0.005 from the permutation distribution results.</p></li>
<li><p>This means that there is about a 0.5% chance we would observe
a difference in mean GPA (female-male or male-female) of 0.25 points or more
if there in fact is no difference in true mean GPA between females and males
in STAT 217 in a particular semester.</p></li>
</ul></li>
<li><p>Decision</p>
<ul>
<li>Since the p-value is “small”, we find strong evidence against the null
hypothesis and can reject the null hypothesis.</li>
</ul></li>
<li><p>Conclusion and scope of inference, specific to the problem </p>
<ul>
<li><p>There is strong evidence against the null hypothesis of no difference
in the true mean GPA between males and females for the STAT 217 students
in this semester and so we conclude that there is evidence of a difference
in the mean GPAs between males and females in STAT 217 students.</p></li>
<li><p>Because this was not a randomized experiment, we can’t say that the
difference in sex causes the difference in mean GPA and because it was
not a random sample from a larger population, our inferences only pertain
the STAT 217 students that responded to the survey in that semester. </p></li>
<li><p>Females were estimated to have a higher mean GPA by 0.25 points. The next
section discusses confidence intervals that we could add to this result to
quantify the uncertainty in this estimate.</p></li>
</ul></li>
</ol>
</div>
<div id="section2-8" class="section level2">
<h2><span class="header-section-number">0.7</span> Confidence intervals and bootstrapping</h2>
<p>Randomly shuffling the treatments between the observations is like randomly
sampling the treatments without replacement. In other words, we randomly
sample one observation at a time from the treatments until we have <span class="math inline">\(n\)</span>
observations. This provides us with a technique
for testing hypotheses because it provides new splits of the observations
into groups that are as interesting as what we observed if the null hypothesis
is assumed true. In most situations, we also want to estimate parameters of
interest and provide <strong><em>confidence intervals</em></strong> for those parameters (an
interval where we are __% <strong><em>confident</em></strong> that the true parameter lies). As
before, there are two options we will consider – a parametric  and a
nonparametric approach. The nonparametric 
approach will be using what is
called <strong><em>bootstrapping</em></strong>

and draws its name from “pull yourself up by
your bootstraps” where you improve your situation based on your own efforts.
In statistics, we make our situation or inferences better by re-using the
observations we have by assuming that the sample represents the population.
Since each observation represents other similar observations in the
population that we didn’t get to measure, if we <strong><em>sample with replacement</em></strong>
to generate a new data set of size <em>n</em> from our data set (also of size <em>n</em>)
it mimics the process of taking repeated random samples  of size <span class="math inline">\(n\)</span> from our
population of interest. This process also
ends up giving us useful sampling distributions

of statistics even when our
standard normality assumption is violated, similar to what we encountered
in the permutation tests. Bootstrapping is especially useful in situations
where we are interested in statistics other than the mean (say we want a
confidence interval for a median or a standard deviation) or when we consider
functions of more than one parameter and don’t want to derive the distribution
of the statistic (say the difference in two medians). In this text,
bootstrapping is used to provide more trustworthy inferences when some of our
assumptions (especially normality) might be violated for our parametric procedure.
</p>
<p>To perform bootstrapping, we will use the <code>resample</code> function from the
<code>mosaic</code> package. We can apply this function to a data set and get a new
version of the
data set by sampling new observations <em>with replacement</em> from the original one.
The new, bootstrapped version of the data set (called <code>MockJury_BTS</code> below)
contains a new variable called <code>orig.id</code> which is the number of the subject
from the original data set. By summarizing how often each of these id’s
occurred in a bootstrapped data set, we can see how the re-sampling works.
The <code>table</code> function will count up how many times each observation was used in
the bootstrap sample,

providing a row with the id followed by a row with the
count<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a>. In the first bootstrap
sample shown, the 1<sup>st</sup>, 4<sup>th</sup>, and 10<sup>th</sup> observations
were sampled one time each, the 5<sup>th</sup> observation was sampled three
times, and the 7<sup>th</sup>, 8<sup>th</sup>, 9<sup>th</sup>, and many others
were not sampled at all. Bootstrap sampling thus picks some observations
multiple times and to do that it has to ignore some<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a> observations.</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury_BTS &lt;-<span class="st"> </span><span class="kw">resample</span>(MockJury2)
<span class="kw">table</span>(<span class="kw">as.numeric</span>(MockJury_BTS<span class="op">$</span>orig.id))</code></pre>
<pre><code>## 
##  1  2  3  4  5  6 10 11 12 14 15 17 18 19 20 22 24 26 29 30 32 35 36 37 39 
##  1  2  2  1  3  2  1  2  1  1  3  1  2  1  2  1  2  1  2  2  2  2  1  2  2 
## 40 42 43 44 45 46 47 48 49 55 58 59 60 61 69 70 71 72 74 75 
##  2  1  1  4  2  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1</code></pre>
<p>Like in permutations, one randomization isn’t enough. A second bootstrap sample
is also provided to help you get a sense of what bootstrap data sets contain.
It did not select subject 7 but did select 6, 14, and 21 more than once.
You can see other variations in the resulting re-sampling of subjects with the
most sampled subjects 6 and 50 sampled four times. With <span class="math inline">\(n=75\)</span>, the the chance of
selecting any observation for any slot
in the new data set is <span class="math inline">\(1/75\)</span> and the expected or mean number of appearances we
expect to see for an observation is the number of random draws times the probably
of selection on each so <span class="math inline">\(75*1/75=1\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r">MockJury_BTS2 &lt;-<span class="st"> </span><span class="kw">resample</span>(MockJury2)
<span class="kw">table</span>(<span class="kw">as.numeric</span>(MockJury_BTS2<span class="op">$</span>orig.id))</code></pre>
<pre><code>## 
##  1  2  3  5  6  8 11 12 13 14 15 18 19 20 21 23 24 26 27 28 29 31 32 34 36 
##  1  1  1  1  4  1  1  1  1  3  1  1  1  1  3  2  2  1  1  1  2  1  2  1  2 
## 37 38 40 42 46 48 50 51 52 56 58 59 61 62 63 66 67 68 69 72 73 74 75 
##  1  2  1  1  1  2  4  1  1  1  3  2  1  1  1  1  1  1  2  3  1  4  2</code></pre>
<p>We can use the two results to get an idea of distribution of results in terms
of number of times observations might be re-sampled when sampling with
replacement and the variation in those results, as shown in
Figure <a href="acknowledgments.html#fig:Figure2-17">0.27</a>. We could also derive the expected counts for
each number of times of re-sampling when we start with all observations having
an equal chance and sampling with replacement but this isn’t important for
using bootstrapping methods.</p>
<p>(ref:fig2-17) Counts of number of times of observation (or not observed for
times re-sampled of 0) for two bootstrap samples.</p>
<div class="figure"><span id="fig:Figure2-17"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-17-1.png" alt="(ref:fig2-17)" width="480" />
<p class="caption">
Figure 0.27: (ref:fig2-17)
</p>
</div>
<p>The main point of this exploration was to see that each run of the
<code>resample</code> function provides a new version of the data set. Repeating this
<span class="math inline">\(B\)</span> times using
another <code>for</code> loop, we will track our quantity of interest, say <span class="math inline">\(T\)</span>, in all
these new “data sets” and call those results <span class="math inline">\(T^*\)</span>. The distribution of the
bootstrapped

<span class="math inline">\(T^*\)</span> statistics will tell us about the range of results to expect
for the statistic and the middle __% of the <span class="math inline">\(T^*\)</span>’s provides a
<strong><em>bootstrap confidence interval</em></strong><a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> for the true parameter – here the <em>difference in the two population means</em>.</p>
<p>To make this concrete, we can revisit our previous examples, starting
with the <code>MockJury2</code> data created before and our interest in comparing the
mean sentences for the <em>Average</em> and <em>Unattractive</em> picture groups. The
bootstrapping code is very similar to the permutation code except that we apply
the <code>resample</code> function to the entire data set as opposed to the <code>shuffle</code>
function that was applied only to the explanatory variable.</p>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2); Tobs</code></pre>
<pre><code>## diffmean 
## 1.837127</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span><span class="kw">resample</span>(MockJury2))
  }
<span class="kw">favstats</span>(Tstar)</code></pre>
<pre><code>##         min       Q1   median       Q3      max     mean        sd    n
##  -0.3627312 1.305773 1.833091 2.385281 4.988756 1.854428 0.8438987 1000
##  missing
##        0</code></pre>
<p>(ref:fig2-18) Histogram and density curve of bootstrap distributions of
difference in sample mean <code>Years</code> with vertical line for the observed
difference in the means of 1.84 years.</p>
<div class="figure"><span id="fig:Figure2-18"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-18-1.png" alt="(ref:fig2-18)" width="960" />
<p class="caption">
Figure 0.28: (ref:fig2-18)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p>In this situation, the observed difference in the mean sentences is 1.84 years
(<em>Unattractive</em> - <em>Average</em>), which is the vertical line in Figure
<a href="acknowledgments.html#fig:Figure2-18">0.28</a>.
The bootstrap distribution

shows the results for the difference in the sample
means when fake data sets are re-constructed by sampling from the original data set with
replacement. The bootstrap distribution is approximately centered at the observed
value (difference in the sample means) and is relatively symmetric.</p>
<p>The permutation distribution  in the same situation (Figure
<a href="acknowledgments.html#fig:Figure2-12">0.22</a>) had a similar shape but was centered at 0.
Permutations create sampling
distributions

based on assuming the null hypothesis is true, which is useful for
hypothesis testing. Bootstrapping creates distributions centered at the observed
result, which is the sampling distribution “under the alternative” or when no null
hypothesis is assumed; bootstrap distributions are useful for generating
confidence intervals for the true parameter values.</p>
<p>To create a 95% bootstrap confidence interval for the difference in
the true mean sentences (<span class="math inline">\(\mu_\text{Unattr}-\mu_\text{Avg}\)</span>), select the
middle 95% of results from
the bootstrap distribution. Specifically, find the 2.5<sup>th</sup>
percentile and the 97.5<sup>th</sup> percentile (values that put 2.5 and 97.5%
of the results to the left) in the bootstrap distribution, which leaves 95% in
the middle for the confidence interval. To find percentiles in a distribution
in R, functions are of the form <code>q[Name of distribution]</code>, with the function
<code>qt</code> extracting percentiles from a <span class="math inline">\(t\)</span>-distribution (examples below). From the
bootstrap results, use the <code>qdata</code> function on the <code>Tstar</code> results that
contain the bootstrap distribution of the statistic of interest.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.025</span>)</code></pre>
<pre><code>##         p  quantile 
## 0.0250000 0.2414232</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.975</span>)</code></pre>
<pre><code>##        p quantile 
## 0.975000 3.521528</code></pre>
<p>These results tell us that the 2.5<sup>th</sup> percentile of the bootstrap
distribution is at 0.24 years and the 97.5<sup>th</sup> percentile is at 3.52
years. We can combine these results to provide a 95% confidence for
<span class="math inline">\(\mu_\text{Unattr}-\mu_\text{Avg}\)</span> that is between 0.24 and 3.52 years. We can
interpret this
as with any confidence interval, that we are 95% confident that the difference
in the true mean suggested sentences (<em>Unattractive</em> minus <em>Average</em> groups) is
between 0.24 and 3.52 years. We can also obtain both percentiles in one line
of code using:</p>
<pre class="sourceCode r"><code class="sourceCode r">quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))
quantiles</code></pre>
<pre><code>##        quantile     p
## 2.5%  0.2414232 0.025
## 97.5% 3.5215278 0.975</code></pre>
<p>Figure <a href="acknowledgments.html#fig:Figure2-19">0.29</a> displays those same percentiles on the bootstrap
distribution residing in <code>Tstar</code>.</p>
<p>(ref:fig2-19) Histogram and density curve of bootstrap distribution with
95% bootstrap confidence intervals displayed (vertical lines).</p>
<div class="figure"><span id="fig:Figure2-19"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-19-1.png" alt="(ref:fig2-19)" width="960" />
<p class="caption">
Figure 0.29: (ref:fig2-19)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<p>Although confidence intervals can exist without referencing hypotheses,
we can
revisit our previous hypotheses and see what this confidence interval tells
us about the test of <span class="math inline">\(H_0: \mu_\text{Unattr} = \mu_\text{Avg}\)</span>. This null
hypothesis is equivalent to testing <span class="math inline">\(H_0: \mu_\text{Unattr} - \mu_\text{Avg} = 0\)</span>,
that the difference
in the true means is equal to 0 years. And the difference in the means was the
scale for our confidence interval, which did not contain 0 years. We will call
0 an interesting <strong><em>reference value</em></strong> for the confidence interval, because
here it is the value where the true means are equal to each other (have a
difference of 0 years). In general, if our confidence interval does not contain
0, then it is saying that 0 is not one of our likely values for the difference
in the true means. This implies that we should reject a claim that they are
equal. This provides the same inferences for the hypotheses that we considered
previously using both a parametric and permutation approach.</p>
<p>The general summary
is that we can use confidence intervals to test hypotheses by assessing whether
the reference value under the null hypothesis is in the confidence interval
(FTR <span class="math inline">\(H_0\)</span>) or outside the confidence interval (Reject <span class="math inline">\(H_0\)</span>). P-values

are more
informative about hypotheses (measure of evidence against the null hypothesis)
but confidence intervals are more informative
about the size of differences, so both offer useful information and, as shown
here, can provide consistent conclusions about hypotheses.</p>
<p>As in the previous situation, we also want to consider the parametric
approach
for comparison purposes and to have that method available, especially to help
us understand some methods where we will only consider parametric inferences
in later chapters. The parametric confidence interval is called the
<strong><em>equal variance, two-sample t confidence interval</em></strong> and additionally
assumes that the populations
being sampled from are normally distributed. It leads to using a <span class="math inline">\(t\)</span>-distribution

to form the interval. The output from the <code>t.test</code> function provides the
parametric 95% confidence interval calculated for you:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811</code></pre>
<p>The <code>t.test</code> function again switched the order of the groups and provides
slightly different end-points than our bootstrap confidence interval (both are
made at the 95% confidence level though), which was slightly narrower. Both
intervals have the same interpretation, only the methods for calculating the
intervals and the assumptions differ. Specifically, the bootstrap interval can
tolerate different distribution shapes other than normal and still provide
intervals that work well<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>. The other assumptions

are all the same as for the hypothesis
test, where we continue to assume that we have independent observations with
equal variances for the two groups.</p>
<p>The formula that <code>t.test</code> is using to calculate the parametric
<strong><em>equal variance two-sample t confidence interval</em></strong> is:</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]</span></p>
<p>In this situation, the <em>df</em> is again <span class="math inline">\(n_1+n_2-2\)</span> and
<span class="math inline">\(s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\)</span>. The <span class="math inline">\(t^*_{df}\)</span> is
a multiplier that comes from finding the percentile from the <span class="math inline">\(t\)</span>-distribution
that puts <span class="math inline">\(C\)</span>% in the middle of the distribution with <span class="math inline">\(C\)</span> being the confidence
level. It is important to note that this <span class="math inline">\(t^*\)</span> has nothing to do with the previous
test statistic <span class="math inline">\(t\)</span>. It is confusing and many of you will, at some point, happily
take the result from a test statistic calculation and use it for a multiplier
in a <span class="math inline">\(t\)</span>-based confidence interval. Figure <a href="acknowledgments.html#fig:Figure2-20">0.30</a> shows the
<span class="math inline">\(t\)</span>-distribution with 73 degrees of freedom and the cut-offs that put 95% of the
area in the middle.</p>
<p>(ref:fig2-20) Plot of <span class="math inline">\(t(73)\)</span> with cut-offs for putting 95% of distribution in
the middle.</p>
<div class="figure"><span id="fig:Figure2-20"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-20-1.png" alt="(ref:fig2-20)" width="480" />
<p class="caption">
Figure 0.30: (ref:fig2-20)
</p>
</div>
<p>For 95% confidence intervals, the multiplier is going to be close to 2 and
anything else is a sign of a mistake. We can use R to get the multipliers for
confidence intervals using the <code>qt</code> function in a similar fashion to how
<code>qdata</code> was used in the bootstrap results, except that this new value must be
used in the previous confidence interval formula. This function produces values
for requested percentiles, so if we want to put 95% in the middle, we place
2.5% in each tail of the distribution and need to request the 97.5<sup>th</sup>
percentile. Because the <span class="math inline">\(t\)</span>-distribution is always symmetric around 0, we merely
need to look up the value for the 97.5<sup>th</sup> percentile and know that the
multiplier for the 2.5<sup>th</sup> percentile is just <span class="math inline">\(-t^*\)</span>. The <span class="math inline">\(t^*\)</span>
multiplier to form the confidence interval is 1.993 for a 95% confidence interval
when the <span class="math inline">\(df=73\)</span> based on the results from <code>qt</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)</code></pre>
<pre><code>## [1] 1.992997</code></pre>
<p>Note that the 2.5<sup>th</sup> percentile is just the negative of this value due
to symmetry and the real source of the minus in the minus/plus in the formula
for the confidence interval.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df=</span><span class="dv">73</span>)</code></pre>
<pre><code>## [1] -1.992997</code></pre>
<p>We can also re-write the confidence interval formula into a slightly more
general form as</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\ \text{ OR }\ 
\bar{x}_1 - \bar{x}_2 \mp ME\]</span></p>
<p>where <span class="math inline">\(SE_{\bar{x}_1 - \bar{x}_2} = s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</span> and
<span class="math inline">\(ME = t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\)</span>. In some situations, researchers will
report the <strong><em>standard error</em></strong> (SE) or <strong><em>margin of error</em></strong> (ME) as a method
of quantifying the uncertainty in a statistic. The SE is an estimate of the
standard deviation of the statistic (here <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) and the ME
is an estimate of the precision of a statistic that can be used to directly
form a confidence interval. The ME depends on the choice of confidence level
although 95% is almost always selected.</p>
<p>To finish this example, R can be used to help you do calculations much
like a calculator except with much more power “under the hood”.  You have to
make sure you are careful with using <code>( )</code> to group items and remember that
the asterisk (*) is used for multiplication in R. We need the pertinent
information which is available from the <code>favstats</code> output repeated below to
calculate the confidence interval “by hand”<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> using R.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre>
<pre><code>##           Attr min Q1 median Q3 max     mean       sd  n missing
## 1      Average   1  2      3  5  12 3.973684 2.823519 38       0
## 2 Unattractive   1  2      5 10  15 5.810811 4.364235 37       0</code></pre>
<p>Start with typing the following command to calculate <span class="math inline">\(s_p\)</span> and store it in a
variable named <code>sp</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">38-1</span>)<span class="op">*</span>(<span class="fl">2.8235</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">37-1</span>)<span class="op">*</span>(<span class="fl">4.364</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">38</span><span class="op">+</span><span class="dv">37-2</span>))
sp</code></pre>
<pre><code>## [1] 3.665036</code></pre>
<p>Then calculate the confidence interval that <code>t.test</code> provided using:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">3.974-5.811</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">38</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">37</span>)</code></pre>
<pre><code>## [1] -3.5240302 -0.1499698</code></pre>
<p>The previous code uses <code>c(-1, 1)</code> times the margin of error to subtract and add
the ME to the difference in the sample means (<span class="math inline">\(3.974-5.811\)</span>), which generates the
lower and then upper bounds of the confidence interval. If desired, we can also
use just the last portion of the previous calculation to find the margin of error,
which is 1.69 here.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">38</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">37</span>)</code></pre>
<pre><code>## [1] 1.68703</code></pre>
</div>
<div id="section2-9" class="section level2">
<h2><span class="header-section-number">0.8</span> Bootstrap confidence intervals for difference in GPAs</h2>
<p>We can now apply the new confidence interval methods on the STAT 217 grade data.
This time we start with the parametric 95% confidence interval “by hand” in R
and then use <code>t.test</code> to verify our result. The <code>favstats</code> output provides
us with the required information to calculate the confidence interval:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>
<p>The <span class="math inline">\(df\)</span> are <span class="math inline">\(37+42-2 = 77\)</span>. Using the SDs from the two groups and their sample
sizes, we can calculate <span class="math inline">\(s_p\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">37-1</span>)<span class="op">*</span>(<span class="fl">0.4075</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">42-1</span>)<span class="op">*</span>(<span class="fl">0.41518</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">37</span><span class="op">+</span><span class="dv">42-2</span>))
sp</code></pre>
<pre><code>## [1] 0.4116072</code></pre>
<p>The margin of error is:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</code></pre>
<pre><code>## [1] 0.1847982</code></pre>
<p>All together, the 95% confidence interval is:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">3.338-3.0886</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</code></pre>
<pre><code>## [1] 0.0646018 0.4341982</code></pre>
<p>So we are 95% confident that the difference in the true mean GPAs between
females and males (females minus males) is between 0.065 and 0.434 GPA points.
We get a similar<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> result from the <code>t.test</code> output:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex,<span class="dt">data=</span>s217,<span class="dt">var.equal=</span>T)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>Note that we can easily switch to 90% or 99% confidence intervals by simply
changing the percentile in <code>qt</code> or changing the <code>conf.level</code> option in the
<code>t.test</code> function. In the following two lines of code, we added
octothorpes (#)<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> and then some text after function
calls to explain what is being calculated. In computer code, octothorpes
provide a way of adding comments that tell the software (here R) to ignore any
text after a “#” on a given line. In the color version of the text, comments are
also clearly distinguished.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.95</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co"># For 90% confidence and 77 df</span></code></pre>
<pre><code>## [1] 1.664885</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co">#For 99% confidence and 77 df</span></code></pre>
<pre><code>## [1] 2.641198</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T, <span class="dt">conf.level=</span><span class="fl">0.90</span>)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  0.09530553 0.40430837
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  0.004703598 0.494910301
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>As a review of some basic ideas with confidence intervals make sure
you can answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>What is the impact of increasing the confidence level in this situation?</p></li>
<li><p>What happens to the width of the confidence interval if the size of the
SE increases or decreases?</p></li>
<li><p>What about increasing the sample size – should that increase or decrease
the width of the interval?</p></li>
</ol>
<p>All the general results you learned before about impacts to widths of CIs hold
in this situation whether we are considering the parametric or bootstrap methods…</p>
<p>To finish this example, we will generate the comparable bootstrap 90%
confidence interval using the bootstrap distribution in
Figure <a href="acknowledgments.html#fig:Figure2-21">0.31</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">diffmean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217); Tobs</code></pre>
<pre><code>##   diffmean 
## -0.2498069</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b]&lt;-<span class="kw">diffmean</span>(GPA <span class="op">~</span><span class="st"> </span>Sex, <span class="dt">data=</span><span class="kw">resample</span>(s217))
  }
<span class="kw">qdata</span>(Tstar, <span class="fl">0.05</span>)</code></pre>
<pre><code>##          p   quantile 
##  0.0500000 -0.4032273</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.95</span>)</code></pre>
<pre><code>##           p    quantile 
##  0.95000000 -0.09521925</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))
quantiles</code></pre>
<pre><code>##        quantile    p
## 5%  -0.40322729 0.05
## 95% -0.09521925 0.95</code></pre>
<p>The output tells us that the 90% confidence interval is from -0.393 to -0.094 GPA
points. The bootstrap distribution with the observed difference in the sample
means and these cut-offs is displayed in Figure <a href="acknowledgments.html#fig:Figure2-21">0.31</a> using
this code:</p>
<p>(ref:fig2-21) Histogram and density curve of bootstrap distribution of
difference in sample mean GPAs (male minus female) with observed difference
(solid vertical line) and quantiles that delineate the 90% confidence intervals
(dashed vertical lines).</p>
<div class="figure"><span id="fig:Figure2-21"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-21-1.png" alt="(ref:fig2-21)" width="960" />
<p class="caption">
Figure 0.31: (ref:fig2-21)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(Tstar,<span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar),<span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre>
<p>In the previous output, the parametric 90% confidence interval is from
0.095 to 0.404, suggesting similar results again from the two approaches once
you account for the two different orders of differencing of the groups. Based on
the bootstrap CI, we can say that we are 90% confident that the difference in
the true mean GPAs for STAT 217 students is between -0.393 to -0.094 GPA points
(male minus females). Because sex cannot be assigned to the subjects, we cannot
infer that sex is causing this difference and because this was a voluntary
response sample of STAT 217 students in a given semester, we cannot infer that
a difference of this size would apply to all STAT 217 students or, certainly,
students this semester.</p>
<p>Throughout the text, pay attention to the distinctions between parameters
and statistics, focusing on the differences between estimates based on the sample
and inferences for the population of interest in the form of the parameters of
interest. Remember that statistics are summaries of the sample information and
parameters are characteristics of populations (which we rarely know). And that
our inferences are limited to the population that we randomly sampled from, if
we randomly sampled.</p>
</div>
<div id="section2-10" class="section level2">
<h2><span class="header-section-number">0.9</span> Chapter summary</h2>
<p>In this chapter, we reviewed basic statistical inference methods in the context
of a two-sample mean problem. You were introduced to using R to do permutation
testing and generate bootstrap confidence intervals as well as obtaining
parametric <span class="math inline">\(t\)</span>-test and confidence intervals in this same situation. You should
have learned how to use a <code>for</code> loop for doing the nonparametric inferences
and the <code>t.test</code> function for generating parametric inferences. In the two
examples considered, the parametric and nonparametric methods provided similar
results, suggesting that the assumptions were at least close to being met for
the parametric procedures. When parametric and nonparametric approaches
disagree, the nonparametric methods are likely to be more trustworthy since
they have less restrictive assumptions but can still have problems.
</p>
<p>When the noted conditions are not met in a hypothesis testing situation, the
Type I error  rates can be inflated, meaning that we reject the null hypothesis
more often than we have allowed to occur by chance. Specifically, we could have
a situation where our assumed 5% significance level test might actually reject
the null when it is true 20% of the time. If this is occurring, we call a
procedure <strong><em>liberal</em></strong> (it rejects too easily) and if the procedure is liberal,
how could we trust a small p-value to be a “real” result and not just an
artifact of violating the assumptions of the procedure? Likewise, for
confidence intervals we hope that our 95% confidence level procedure, when
repeated, will contain the true parameter 95% of the time. If our assumptions
are violated, we might actually have an 80% confidence level procedure and it
makes it hard to trust the reported results for our observed data set.
Statistical inference relies on a belief in the methods underlying our
inferences. If we don’t trust our assumptions, we shouldn’t trust the
conclusions to perform the way we want them to. As sample sizes increase
and/or violations of conditions lessen, then the procedures will perform better.
In Chapter <a href="#chapter3"><strong>??</strong></a>, some new tools for doing diagnostics are introduced
to help us assess how much those validity conditions are violated.</p>
</div>
<div id="section2-11" class="section level2">
<h2><span class="header-section-number">0.10</span> Summary of important R code</h2>
<p>The main components of R code used in this chapter follow with components to
modify in lighter and/or ALL CAPS text, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of working:</p>
<ul>
<li><p><strong>summary(<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of all variables in the data set.
</li>
</ul></li>
<li><p><strong>t.test(<font color='red'>Y</font> ~ <font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>, conf.level=0.95)</strong></p>
<ul>
<li>Provides two-sample t-test test statistic, df, p-value, and 95%
confidence interval. </li>
</ul></li>
<li><p><strong>2<code>*</code>pt(abs(<font color='red'>Tobs</font>), df=<font color='red'>DF</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the two-sided test p-value for an observed 2-sample t-test
statistic of <code>Tobs</code>. </li>
</ul></li>
<li><p><strong>hist(<font color='red'>DATASETNAME$Y</font>)</strong></p>
<ul>
<li>Makes a histogram of a variable named <code>Y</code> from the data set of
interest.</li>
</ul></li>
<li><p><strong>boxplot(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Makes a boxplot of a variable named Y for groups in X from the data set.</li>
</ul></li>
<li><p><strong>beanplot(<font color='red'>Y</font>~<font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>, log="“, col=”lightblue“, method=”jitter")</strong></p>
<ul>
<li><p>Requires the <code>beanplot</code> package is loaded.</p></li>
<li><p>Makes a beanplot of a variable named Y for groups in X from the data set. </p></li>
</ul></li>
<li><p><strong>mean(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>); sd(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>This usage of <code>mean</code> and <code>sd</code> requires the <code>mosaic</code> package.</p></li>
<li><p>Provides the mean and sd of responses of Y for each group described in X.</p></li>
</ul></li>
<li><p><strong>favstats(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of Y by groups described in X.</li>
</ul></li>
<li><p><strong>Tobs <code>&lt;-</code> t.test(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
Tstar[b] <code>&lt;-</code> t.test(<font color='red'>Y</font>~shuffle(<font color='red'>X</font>), data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 permuted versions of the test
statistic using the <code>shuffle</code> function and keep track of the results in
<code>Tstar</code></li>
</ul></li>
<li><p><strong>pdata(Tstar, abs(<font color='red'>Tobs</font>), lower.tail=F)[[1]]</strong></p>
<ul>
<li>Finds the proportion of the permuted test statistics in Tstar that are
less than -|Tobs| or greater than |Tobs|, useful for finding the two-sided
test p-value. </li>
</ul></li>
</ul>

<ul>
<li><p><strong>Tobs <code>&lt;-</code> diffmean(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
Tstar[b] <code>&lt;-</code> diffmean(<font color='red'>Y</font>~<font color='red'>X</font>, data=resample(<font color='red'>DATASETNAME</font>))<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 bootstrapped versions of the
data set using the <code>resample</code> function and keep track of the results of
the statistic in <code>Tstar</code>.</li>
</ul></li>
<li><p><strong>qdata(Tstar, c(0.025, 0.975))</strong></p>
<ul>
<li>Provides the values that delineate the middle 95% of the results in the
bootstrap distribution (<code>Tstar</code>). </li>
</ul></li>
</ul>
</div>
<div id="section2-12" class="section level2">
<h2><span class="header-section-number">0.11</span> Practice problems</h2>
<p>Load the <code>HELPrct</code> data set from the <code>mosaicData</code> package <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaicData" role="doc-biblioref">2018</a>)</span>
(you need to
install the <code>mosaicData</code> package once to be able to load it). The HELP study
was a clinical trial for adult inpatients recruited from a
detoxification unit. Patients with no primary care physician were randomly
assigned to receive a multidisciplinary assessment and a brief motivational
intervention or usual care and various outcomes were observed. Two of the
variables in the data set are <code>sex</code>, a factor with levels <em>male</em> and <em>female</em>
and <code>daysanysub</code> which is the time (in days) to first use of any substance
post-detox. We are interested in the difference in mean number of days to first
use of any substance post-detox between males and females. There are some
missing responses and the following code will produce <code>favstats</code> with the
missing values and then provide a data set that by
applying the <code>na.omit</code> function removes any observations with missing
values.
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaicData)
<span class="kw">data</span>(HELPrct)
HELPrct2 &lt;-<span class="st"> </span>HELPrct[, <span class="kw">c</span>(<span class="st">&quot;daysanysub&quot;</span>, <span class="st">&quot;sex&quot;</span>)] <span class="co">#Just focus on two variables</span>
HELPrct3 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(HELPrct2) <span class="co">#Removes subjects with missing values</span>
<span class="kw">favstats</span>(daysanysub<span class="op">~</span>sex, <span class="dt">data=</span>HELPrct2)
<span class="kw">favstats</span>(daysanysub<span class="op">~</span>sex, <span class="dt">data=</span>HELPrct3)</code></pre>
<p>2.1. Based on the results provided, how many observations were missing for males
and females? Missing values here likely mean that the subjects didn’t use any
substances post-detox in the time of the study but might have at a later
date – the study just didn’t run long enough. This is called <strong><em>censoring</em></strong>.
What is the problem with the numerical summaries here if the missing responses
were all something larger than the largest observation?</p>
<p>2.2. Make a beanplot and a boxplot of <code>daysanysub</code> ~ <code>sex</code> using the
<code>HELPrct3</code> data set created above. Compare the distributions, recommending
parametric or nonparametric inferences.</p>
<p>2.3. Generate the permutation results and write out the 6+ steps of the
hypothesis test, making sure to note the numerical value of observed test
statistic you are using and include a discussion of the scope of inference.</p>
<p>2.4. Interpret the p-value for these results.</p>
<p>2.5. Generate the parametric <code>t.test</code> results, reporting the test-statistic,
its distribution under the null hypothesis, and compare the p-value to those
observed using the permutation approach.</p>
<p>2.6. Make and interpret a 95% bootstrap confidence interval for the difference
in the means.</p>

<p>#One-Way ANOVA {#chapter3}</p>
</div>
<div id="section3-1" class="section level2">
<h2><span class="header-section-number">0.12</span> Situation</h2>
<p>In Chapter <a href="#chapter2"><strong>??</strong></a>, tools for comparing the means of two groups
were considered. More generally, these methods are used for a quantitative
response and a categorical explanatory variable (group) which had two and only
two levels. The full prisoner rating data set actually contained three groups
(Figure <a href="acknowledgments.html#fig:Figure3-1">0.32</a>) with <em>Beautiful</em>, <em>Average</em>, and <em>Unattractive</em>
rated pictures randomly assigned to the subjects for sentence ratings. In a
situation with more than two groups, we have two choices. First, we could rely
on our two group comparisons, performing tests for every possible pair
(<em>Beautiful</em> vs <em>Average</em>, <em>Beautiful</em> vs <em>Unattractive</em>, and <em>Average</em> vs
<em>Unattractive</em>). We spent Chapter <a href="#chapter2"><strong>??</strong></a> doing inferences for differences
between <em>Average</em> and <em>Unattractive</em>. The other two comparisons would lead us to
initially end up with three p-values and no direct answer about our initial
question of interest – is there some overall difference in the average sentences
provided across the groups? In this chapter, we will learn a new method, called
<strong><em>Analysis of Variance</em></strong>, <strong><em>ANOVA</em></strong>, or sometimes <strong><em>AOV</em></strong> that directly assesses whether there is
evidence of some overall difference in the means among the groups. This version
of an ANOVA is called a <strong><em>One-Way ANOVA</em></strong> since there is just
one<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> grouping variable. After we perform our One-Way ANOVA test for
overall evidence of a difference, we will revisit the comparisons similar to those
considered in Chapter <a href="#chapter2"><strong>??</strong></a> to get more details on specific differences
among <em>all</em> the pairs of groups – what we call <strong><em>pair-wise comparisons</em></strong>.
An issue is created when you perform many tests simultaneously and we will augment
our previous methods with an adjusted method for pairwise comparisons to make our
results valid called <strong><em>Tukey’s Honest Significant Difference</em></strong>.</p>
<p>To make this more concrete, we return to the original MockJury data, making
side-by-side boxplots and beanplots (Figure <a href="acknowledgments.html#fig:Figure3-1">0.32</a>) as well as
summarizing the suggested sentence lengths by the three groups using <code>favstats</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(heplots)
<span class="kw">require</span>(mosaic)
<span class="kw">data</span>(MockJury)
<span class="kw">favstats</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>##           Attr min Q1 median   Q3 max     mean       sd  n missing
## 1    Beautiful   1  2      3  6.5  15 4.333333 3.405362 39       0
## 2      Average   1  2      3  5.0  12 3.973684 2.823519 38       0
## 3 Unattractive   1  2      5 10.0  15 5.810811 4.364235 37       0</code></pre>
<p>(ref:fig3-1) Boxplot and beanplot of the sentences (years) for the three
treatment groups.</p>
<div class="figure"><span id="fig:Figure3-1"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-1-1.png" alt="(ref:fig3-1)" width="960" />
<p class="caption">
Figure 0.32: (ref:fig3-1)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(Years<span class="op">~</span>Attr,<span class="dt">data=</span>MockJury)
<span class="kw">require</span>(beanplot)
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr,<span class="dt">data=</span>MockJury,<span class="dt">log=</span><span class="st">&quot;&quot;</span>,<span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>,<span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre>
<p>There are slight differences in the sample sizes in the three groups with <span class="math inline">\(37\)</span>
<em>Unattractive</em>, <span class="math inline">\(38\)</span> <em>Average</em> and <span class="math inline">\(39\)</span> <em>Beautiful</em> group responses, providing a
data set has a total sample size of <span class="math inline">\(N=114\)</span>. The <em>Beautiful</em> and <em>Average</em> groups
do not appear to be very different with means of 4.33 and 3.97 years. In
Chapter <a href="#chapter2"><strong>??</strong></a>, we found moderate evidence regarding the difference in
<em>Average</em> and <em>Unattractive</em>. It is less clear whether we might find evidence of a
difference between <em>Beautiful</em> and <em>Unattractive</em> groups since we are comparing
means of 5.81 and 4.33 years. All the distributions appear to be right skewed
with relatively similar shapes. The variability in <em>Average</em> and <em>Unattractive</em>
groups seems like it could be slightly different leading to an overall concern of
whether the variability is the same in all the groups.</p>
</div>
<div id="section3-2" class="section level2">
<h2><span class="header-section-number">0.13</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</h2>
<p>We introduced the statistical model <span class="math inline">\(y_{ij} = \mu_j+\varepsilon_{ij}\)</span> in
Chapter <a href="#chapter2"><strong>??</strong></a> for the situation with <span class="math inline">\(j = 1 \text{ or } 2\)</span> to denote
a situation where there were two groups and, for the model that is consistent
with the alternative hypothesis, the means differed. Now we have three groups
and the previous model can be extended to this new situation by allowing <span class="math inline">\(j\)</span>
to be 1, 2, or 3. Now that we have more than two groups, we need to admit that
what we were doing in Chapter <a href="#chapter2"><strong>??</strong></a> was actually fitting what is called
a <strong><em>linear model</em></strong>.

The linear model assumes that the responses follow a normal
distribution with the linear model defining the mean, all observations have the
same variance, and the parameters for the mean in the model enter linearly. This
last condition is hard to explain at this level of material – it is sufficient
to know that there are models where the parameters enter the model nonlinearly and
that they are beyond the scope of this function and this material. By employing a general modeling methodology, we will be able to use the same general modeling framework for the methods
in Chapters <a href="#chapter3"><strong>??</strong></a>, <a href="chapter4.html#chapter4">1</a>, <a href="chapter6.html#chapter6">3</a>,
<a href="chapter7.html#chapter7">4</a>, and <a href="chapter8.html#chapter8">5</a>.</p>
<p>As in Chapter <a href="#chapter2"><strong>??</strong></a>, we have a null hypothesis that defines a
situation (and model) where all the groups have the same mean. Specifically,
the <strong><em>null hypothesis</em></strong> in the general situation with <span class="math inline">\(J\)</span> groups
(<span class="math inline">\(J\ge 2\)</span>) is to have all the <span class="math inline">\(\underline{\text{true}}\)</span> group means equal,</p>
<p><span class="math display">\[H_0:\mu_1 = \ldots = \mu_J.\]</span></p>
<p>This defines a model where all the groups have the same mean so it can be
defined in terms of a single mean, <span class="math inline">\(\mu\)</span>, for the <span class="math inline">\(i^{th}\)</span> observation from
the <span class="math inline">\(j^{th}\)</span> group as <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span>. This is not the model
that most researchers want to be the final description of their study as it
implies no difference in the groups. There is more caution required to specify
the alternative hypothesis with more than two groups. The
<strong><em>alternative hypothesis</em></strong> needs to be the logical negation of this null
hypothesis of all groups having equal means; to make the null hypothesis
false, we only need one group to differ but more than one group could differ
from the others. Essentially, there are many ways to “violate” the null
hypothesis so we choose some delicate wording for the alternative hypothesis
when there are more than 2 groups. Specifically, we state the alternative as</p>
<p><span class="math display">\[H_A: \text{ Not all } \mu_j \text{ are equal}\]</span></p>
<p>or, in words, <strong>at least one of the true means differs among the J groups</strong>.
You will be attracted to trying to say that all means are different in the
alternative but we do not put this strict a requirement in place to reject the
null hypothesis. The alternative model

allows all the true group means to
differ but does require that they differ with</p>
<p><span class="math display">\[y_{ij} = {\color{red}{\mu_j}}+\varepsilon_{ij}.\]</span></p>
<p>This linear model

states that the response for the <span class="math inline">\(i^{th}\)</span> observation in
the <span class="math inline">\(j^{th}\)</span> group, <span class="math inline">\(\mathbf{y_{ij}}\)</span>, is modeled with a group <span class="math inline">\(j\)</span>
(<span class="math inline">\(j=1, \ldots, J\)</span>) population mean, <span class="math inline">\(\mu_j\)</span>, and a random error for each subject
in each group <span class="math inline">\(\varepsilon_{ij}\)</span>, that we assume follows a normal distribution and
that all the random errors have the same variance, <span class="math inline">\(\sigma^2\)</span>. We can write the assumption about the random errors, often called the <strong><em>normality assumption</em></strong>,
as <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. There is a second way to write out this
model that allows extension to more complex models discussed below, so we
need a name for this version of the model. The model written in terms of the
<span class="math inline">\({\color{red}{\mu_j}}\text{&#39;s}\)</span> is called the
<b><font color='red'>cell means model</font></b> and is the
easier version of this model to understand.
</p>
<p>One of the reasons we learned about beanplots is that
it helps us visually consider all the aspects of this model.

In the right panel of Figure <a href="acknowledgments.html#fig:Figure3-1">0.32</a>,
we can see the wider, bold horizontal lines that provide the estimated group means.
The bigger the differences in the sample means, the more likely we are to find
evidence against the null hypothesis. You can also see the null model on the plot
that assumes all the groups have the same mean as displayed in the
dashed horizontal line
at 4.7 years (the R code below shows the overall mean of <em>Years</em> is 4.7). While
the hypotheses focus on the means, the model also contains assumptions about the
distribution of the responses – specifically that the distributions are normal
and that all the groups have the same variability.

As discussed previously, it
appears that the distributions are right skewed and the variability might not be
the same for all the groups. The boxplot provides the information about the skew
and variability but since it doesn’t display the means it is not directly related
to the linear model and hypotheses we are considering.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(MockJury<span class="op">$</span>Years)</code></pre>
<pre><code>## [1] 4.692982</code></pre>
<p>There is a second way to write out the One-Way ANOVA model

that provides a framework
for extensions to more complex models described in Chapter <a href="chapter4.html#chapter4">1</a> and
beyond. The other <strong><em>parameterization</em></strong> (way of writing out or defining) of the
model is called the <b><font color='purple'>reference-coded model</font></b> since it
writes out the model in terms of a


<strong><em>baseline group</em></strong> and deviations from that baseline or reference level. The
reference-coded model for the <span class="math inline">\(i^{th}\)</span> subject in the <span class="math inline">\(j^{th}\)</span> group is
<span class="math inline">\(y_{ij} ={\color{purple}{\boldsymbol{\alpha + \tau_j}}}+\varepsilon_{ij}\)</span> where
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> (alpha) is the true mean for the
baseline group (first alphabetically) and the <span class="math inline">\(\color{purple}{\boldsymbol{\tau_j}}\)</span>
(tau <span class="math inline">\(j\)</span>) are the deviations from the baseline group for group <span class="math inline">\(j\)</span>. The deviation
for the baseline group, <span class="math inline">\(\color{purple}{\boldsymbol{\tau_1}}\)</span>, is always set to 0
so there are really just deviations for groups 2 through <span class="math inline">\(J\)</span>. The equivalence
between the two models can be seen by considering the mean for the first, second,
and <span class="math inline">\(J^{th}\)</span> groups in both models:</p>
<p><span class="math display">\[\begin{array}{lccc}
&amp; \textbf{Cell means:} &amp;&amp; \textbf{Reference-coded:}\\
\textbf{Group } 1: &amp; \color{red}{\mu_1} &amp;&amp; \color{purple}{\boldsymbol{\alpha}} \\
\textbf{Group } 2: &amp; \color{red}{\mu_2} &amp;&amp; \color{purple}{\boldsymbol{\alpha + \tau_2}} \\
\ldots &amp; \ldots &amp;&amp; \ldots \\
\textbf{Group } J: &amp; \color{red}{\mu_J} &amp;&amp; \color{purple}{\boldsymbol{\alpha +\tau_J}}
\end{array}\]</span></p>
<p>The hypotheses for the reference-coded model are similar to those in the
cell-means coding except that they are defined in terms of the deviations,
<span class="math inline">\({\color{purple}{\boldsymbol{\tau_j}}}\)</span>. The null hypothesis is that there is
no deviation from the baseline for any group – that all the <span class="math inline">\({\color{purple}{\boldsymbol{\tau_j\text{&#39;s}}}}=0\)</span>,</p>
<p><span class="math display">\[\boldsymbol{H_0: \tau_2=\ldots=\tau_J=0}.\]</span></p>
<p>The alternative hypothesis is that at least one of the deviations is not 0,</p>
<p><span class="math display">\[\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal } \bf{0}.\]</span></p>
<p>In this chapter, you are welcome to use either version (unless we instruct you
otherwise) but we have to use the reference-coding in subsequent chapters. The
next task is to learn how to use R’s linear model, <code>lm</code>, function to get
estimates of the parameters in each model, but first a quick review of these
new ideas:</p>
<p><b><font color='red'>Cell Means Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: {\color{red}{\mu_1= \ldots = \mu_J}}\)</span>        
    <span class="math inline">\(H_A: {\color{red}{\text{ Not all } \mu_j \text{ equal}}}\)</span></p></li>
<li><p>Null hypothesis in words: No difference in the true means between the groups.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span> </p></li>
<li><p>Alternative hypothesis in words: At least one of the true means differs between
the groups.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} = \color{red}{\mu_j}+\varepsilon_{ij}.\)</span>
</p></li>
</ul>
<p><b><font color='purple'>Reference-coded Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: \color{purple}{\boldsymbol{\tau_2 = \ldots = \tau_J = 0}}\)</span>
        
<span class="math inline">\(H_A: \color{purple}{\text{ Not all } \tau_j \text{ equal 0}}\)</span></p></li>
<li><p>Null hypothesis in words: No deviation of the true mean for any groups from the
baseline group.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} =\boldsymbol{\alpha} +\varepsilon_{ij}\)</span>
</p></li>
<li><p>Alternative hypothesis in words: At least one of the true deviations is
different from 0 or that at least one group has a different true mean than the
baseline group.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} =\color{purple}{\boldsymbol{\alpha + \tau_j}}+\varepsilon_{ij}\)</span> </p></li>
</ul>
<p>In order to estimate the models discussed above, the <code>lm</code> function is used<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a>. The <code>lm</code> function continues to
use the same format as previous functions, <code>lm(Y~X, data=datasetname)</code>.
It ends up that this code will give you the reference-coded version of the
model by default (The developers of R thought it was that important!).


We want to start with the
cell-means version of the model,

so we have to override the standard technique
and add a “<code>-1</code>” to the formula interface to tell R that we want to the
cell-means coding. Generally, this looks like <code>lm(Y~X-1, data=datasetname).</code>

Once we fit a model in R, the <code>summary</code> function run on the model provides a
useful “summary” of the model coefficients and a suite of other potentially
interesting information. For the moment, we will focus on the estimated model
coefficients, so only those lines are output. When fitting this version
of the One-Way ANOVA model,
you will find a row of output for each group relating the <span class="math inline">\(\mu_j\text{&#39;s}\)</span>.
The output contains columns for an estimate (<code>Estimate</code>), standard error
(<code>Std. Error</code>), <span class="math inline">\(t\)</span>-value (<code>t value</code>), and p-value (<code>Pr(&gt;|t|)</code>). We’ll
learn to use all the output in the following material, but for now just focus
on the estimates of the parameters that the function provides in the first
column (“Estimate”) of the coefficient table.</p>
<pre class="sourceCode r"><code class="sourceCode r">lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years<span class="op">~</span>Attr<span class="dv">-1</span>, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm1)<span class="op">$</span>coefficients</code></pre>
<pre><code>##                  Estimate Std. Error  t value     Pr(&gt;|t|)
## AttrBeautiful    4.333333  0.5729959 7.562590 1.225982e-11
## AttrAverage      3.973684  0.5804864 6.845439 4.412410e-10
## AttrUnattractive 5.810811  0.5882785 9.877653 6.857681e-17</code></pre>
<p>In general, we denote estimated parameters with a hat over the parameter of
interest to show that it is an estimate. For the true mean of group <span class="math inline">\(j\)</span>,
<span class="math inline">\(\mu_j\)</span>, we estimate it with <span class="math inline">\(\hat{\mu}_j\)</span>, which is just the sample mean for group
<span class="math inline">\(j\)</span>, <span class="math inline">\(\bar{x}_j\)</span>. The model suggests an estimate for each observation that we denote
as <span class="math inline">\(\hat{y}_{ij}\)</span> that we will also call a <strong><em>fitted value</em></strong> based on the model
being considered. The
same estimate is used for all observations in the each group. R tries to help you to
sort out which row of output corresponds to which group by appending the group name
with the variable name. Here, the variable name was <code>Attr</code> and the first group
alphabetically was <em>Beautiful</em>, so R provides a row labeled <code>AttrBeautiful</code>
with an estimate of 4.3333. The sample means from the three groups can be seen to
directly match for that group and the other two.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##     4.333333     3.973684     5.810811</code></pre>
<p>The reference-coded version of the same model is more complicated but ends up

giving the same results once we understand what it is doing. It uses a different
parameterization to accomplish this, so has different model output. Here is the model
summary:</p>
<pre class="sourceCode r"><code class="sourceCode r">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm2)<span class="op">$</span>coefficients</code></pre>
<pre><code>##                    Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)       4.3333333  0.5729959  7.5625901 1.225982e-11
## AttrAverage      -0.3596491  0.8156524 -0.4409343 6.601182e-01
## AttrUnattractive  1.4774775  0.8212161  1.7991335 7.471470e-02</code></pre>
<p>The estimated model coefficients are <span class="math inline">\(\hat{\alpha} = 4.333\)</span> years,
<span class="math inline">\(\hat{\tau}_2 =-0.3596\)</span> years, <span class="math inline">\(\hat{\tau}_3=1.4775\)</span> years where R selected group 1
for <em>Beautiful</em>, 2 for <em>Average</em>, and 3 for <em>Unattractive</em>. The way you can figure
out the baseline group (group 1 is <em>Beautiful</em> here) is to see which category label
is <em>not present</em> in the output. <strong>The baseline level is typically the first group
label alphabetically</strong>, but you should always check this. Based on these definitions,
there are interpretations available for each coefficient. For <span class="math inline">\(\hat{\alpha} = 4.333\)</span> years, this is an estimate of the mean sentencing time for the <em>Beautiful</em> group.
<span class="math inline">\(\hat{\tau}_2 =-0.3596\)</span> years is the deviation of the <em>Average</em> group’s mean from
the <em>Beautiful</em> group’s mean (specifically, it is <span class="math inline">\(0.36\)</span> years lower). Finally,
<span class="math inline">\(\hat{\tau}_3=1.4775\)</span> years tells us that the <em>Unattractive</em> group mean sentencing
time is 1.48 years higher than the <em>Beautiful</em> group mean sentencing time. These
interpretations lead directly to
reconstructing the estimated means for each group by combining the baseline and
pertinent deviations as shown in Table <a href="acknowledgments.html#tab:Table3-1">0.4</a>.</p>
<p>(ref:tab3-1) Constructing group mean estimates from the reference-coded linear
model estimates.</p>
<table>
<caption><span id="tab:Table3-1">Table 0.4: </span> (ref:tab3-1)</caption>
<colgroup>
<col width="20%" />
<col width="40%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="left">Formula</th>
<th align="left">Estimates</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Beautiful</td>
<td align="left"><span class="math inline">\(\hat{\alpha}\)</span></td>
<td align="left"><strong>4.3333</strong> years</td>
</tr>
<tr class="even">
<td align="left">Average</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_2\)</span></td>
<td align="left">4.3333 - 0.3596 = <strong>3.974</strong>
years</td>
</tr>
<tr class="odd">
<td align="left">Unattractive</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_3\)</span></td>
<td align="left">4.3333 + 1.4775 = <strong>5.811</strong>
years</td>
</tr>
</tbody>
</table>
<p>We can also visualize the results of our linear models using what are called
<strong><em>term-plots</em></strong> or <strong><em>effect-plots</em></strong>
 
(from the <code>effects</code> package; <span class="citation">(Fox et al. <a href="#ref-R-effects" role="doc-biblioref">2018</a>)</span>)

as displayed in Figure <a href="acknowledgments.html#fig:Figure3-2">0.33</a>. We don’t want to use the word
“effect” for these model components unless we have random assignment in the study

design so we generically call these <strong><em>term-plots</em></strong> as they display terms or
components from the model in hopefully useful ways to aid in model interpretation
even in the presence of complicated model parameterizations. Specifically, these
plots take an estimated model and show you its estimates along with 95% confidence
intervals generated by the linear model. To make this plot, you need to install and
load the <code>effects</code> package and then use <code>plot(allEffects(...))</code> functions
together on the <code>lm</code> object called <code>lm2</code> that was estimated above. You can
find the correspondence between the displayed means and the estimates that were
constructed in Table <a href="acknowledgments.html#tab:Table3-1">0.4</a>.
</p>
<p>(ref:fig3-2) Plot of the estimated group mean sentences from the reference-coded
model for the MockJury data from the <code>effects</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(lm2))</code></pre>
<div class="figure"><span id="fig:Figure3-2"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-2-1.png" alt="(ref:fig3-2)" width="384" />
<p class="caption">
Figure 0.33: (ref:fig3-2)
</p>
</div>
<p>In order to assess evidence for having different means for the groups, we will
compare either of the previous models (cell-means or reference-coded) to a null
model based on the null hypothesis (<span class="math inline">\(H_0: \mu_1 = \ldots = \mu_J\)</span>) which implies a
model of <span class="math inline">\(\color{red}{y_{ij} = \mu+\varepsilon_{ij}}\)</span> in the cell-means version
where <span class="math inline">\({\color{red}{\mu}}\)</span> is a common mean for all the observations. We will call
this the <b><font color='red'>mean-only</font></b> model since it only has a single mean
in it. In the reference-coded version of the model, we have a null hypothesis that
<span class="math inline">\(H_0: \tau_2 = \ldots = \tau_J = 0\)</span>, so the “mean-only” model is

<span class="math inline">\(\color{purple}{y_{ij} =\boldsymbol{\alpha}+\varepsilon_{ij}}\)</span> with
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> having the same definition as
<span class="math inline">\(\color{red}{\mu}\)</span> for the cell means model – it forces a common value for the
mean for all the groups. Moving from the <em>reference-coded</em> model to the <em>mean-only</em>
model is also an example of a situation where we move from a “full” model


to a
“reduced” model by setting some coefficients in the “full” model to 0 and, by doing
this, get a simpler or “reduced” model.

Simple models can be good as they are easier
to interpret, but having a model for <span class="math inline">\(J\)</span> groups that suggests no difference in the
groups is not a very exciting result
in most, but not all, situations<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a>. In order for R to provide
results for the mean-only model, we remove the grouping variable, <code>Attr</code>, from
the model formula and just include a “1”. The <code>(Intercept)</code> row of the output
provides the estimate for the mean-only model as a reduced model from either the
cell-means or reference-coded models when we assume that the mean is the same
for all groups:</p>
<pre class="sourceCode r"><code class="sourceCode r">lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>MockJury)
<span class="kw">summary</span>(lm3)<span class="op">$</span>coefficients</code></pre>
<pre><code>##             Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 4.692982  0.3403532 13.78857 5.765681e-26</code></pre>
<p>This model provides an estimate of the common mean for all observations of
<span class="math inline">\(4.693 = \hat{\mu} = \hat{\alpha}\)</span> years. This value also is the dashed, horizontal
line in the beanplot in Figure <a href="acknowledgments.html#fig:Figure3-1">0.32</a>. Some people
call this mean-only model estimate the grand or overall mean.</p>
</div>
<div id="section3-3" class="section level2">
<h2><span class="header-section-number">0.14</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</h2>
<p>The previous discussion showed two ways of parameterizing models for the
One-Way ANOVA model and getting estimates from output but still hasn’t
addressed how to assess evidence related to whether the observed differences
in the means among the groups is “real”. In this section, we develop what is
called the <strong><em>ANOVA F-test</em></strong>

that provides a method of aggregating the
differences among the means of 2 or more groups and testing our null hypothesis
of no difference in the means vs the alternative. In order to develop the test,
some additional notation is needed. The sample size in each group is denoted
<span class="math inline">\(n_j\)</span> and the total sample size is
<span class="math inline">\(\boldsymbol{N=\Sigma n_j = n_1 + n_2 + \ldots + n_J}\)</span> where <span class="math inline">\(\Sigma\)</span>
(capital sigma) means “add up over whatever follows”. An estimated
<strong><em>residual</em></strong> (<span class="math inline">\(e_{ij}\)</span>) is the difference between an observation, <span class="math inline">\(y_{ij}\)</span>,
and the model estimate, <span class="math inline">\(\hat{y}_{ij} = \hat{\mu}_j\)</span>, for that observation,
<span class="math inline">\(y_{ij}-\hat{y}_{ij} = e_{ij}\)</span>. It is basically what is left over that the mean
part of the model (<span class="math inline">\(\hat{\mu}_{j}\)</span>) does not explain. It is also a window
into how “good” the model might be because it reflects what the model was unable to explain.</p>
<p>Consider the four different fake results for a situation with four groups (<span class="math inline">\(J=4\)</span>)
displayed in Figure <a href="acknowledgments.html#fig:Figure3-3">0.34</a>. Which of the different results shows
the most and least evidence of differences in the means? In trying to answer
this, think about both how different the means are (obviously important) and
how variable the results are around the mean. These situations were created to
have the same means in Scenarios 1 and 2 as well as matching means in Scenarios
3 and 4. The variability around the means matches by shading (lighter or
darker). In Scenarios 1 and 2, the differences in the means is smaller than in
the other two results. But Scenario 2 should provide more evidence of what
little difference in present than Scenario 1 because it has less variability
around the means. The best situation for finding group differences here is
Scenario 4 since it has the largest difference in the means and the least
variability around those means. Our test statistic somehow needs to allow a
comparison of the variability in the means to the overall variability to help
us get results that reflect that Scenario 4 has the strongest evidence of a
difference and Scenario 1 would have the least.</p>
<p>(ref:fig3-3) Demonstration of different amounts of difference in means relative
to variability. Scenarios have same means in rows and same variance around means
in columns of plot.</p>
<div class="figure"><span id="fig:Figure3-3"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-3-1.png" alt="(ref:fig3-3)" width="576" />
<p class="caption">
Figure 0.34: (ref:fig3-3)
</p>
</div>
<p>The statistic that allows the comparison of relative amounts of variation is called
the <strong><em>ANOVA F-statistic</em></strong>. It is developed using <strong><em>sums of squares</em></strong>

which are measures of total variation like those that are used in the numerator of the
standard deviation (<span class="math inline">\(\Sigma_1^N(y_i-\bar{y})^2\)</span>) that took all the observations,
subtracted the mean, squared the differences, and then added up the results
over all the observations to generate a measure of total variability. With
multiple groups, we will focus on decomposing that total variability
(<strong><em>Total Sums of Squares</em></strong>) into variability among the means (we’ll call this
<strong><em>Explanatory Variable</em></strong> <span class="math inline">\(\mathbf{A}\textbf{&#39;s}\)</span> <strong><em>Sums of Squares</em></strong>) and
variability in the residuals 
or errors (<strong><em>Error Sums of Squares</em></strong>). We define each of these quantities in
the One-Way ANOVA situation as follows:</p>
<ul>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{Total}} =\)</span> Total Sums of Squares
<span class="math inline">\(= \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the total variation in the responses around the overall or
<strong><em>grand mean</em></strong> (<span class="math inline">\(\bar{\bar{y}}\)</span>, the estimated mean for all the
observations and available from the mean-only model).</p></li>
<li><p>By summing over all <span class="math inline">\(n_j\)</span> observations in each group, <span class="math inline">\(\Sigma^{n_j}_{i=1}(\ )\)</span>,
and then adding those results up across the groups, <span class="math inline">\(\Sigma^J_{j=1}(\ )\)</span>,
we accumulate the variation across all <span class="math inline">\(N\)</span> observations.</p></li>
<li><p>Note: this is the residual variation if the null model is used, so there
is no further decomposition possible for that model.</p></li>
<li><p>This is also equivalent to the numerator of the sample variance,
<span class="math inline">\(\Sigma^{N}_{1}(y_{i}-\bar{y})^2\)</span> which is what you get when you ignore
the information on the potential differences in the groups.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{A}} =\)</span> Explanatory Variable <em>A</em>’s Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(\bar{y}_{j}-\bar{\bar{y}})^2 =\Sigma^J_{j=1}n_j(\bar{y}_{j}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the variation in the group means around the grand mean based on
the explanatory variable <span class="math inline">\(A\)</span>.</p></li>
<li><p>Also called sums of squares for the treatment, regression, or model.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_\textbf{E} =\)</span> Error (Residual) Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2 =\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(e_{ij})^2\)</span></p>
<ul>
<li><p>This is the variation in the responses around the group means.</p></li>
<li><p>Also called the sums of squares for the residuals, with the second
version of the formula showing that it is just the squared residuals added
up across all the observations.</p></li>
</ul></li>
</ul>
<p>The possibly surprising result given the mass of notation just presented is that
the total sums of squares is <strong>ALWAYS</strong> equal to the sum of explanatory variable
<span class="math inline">\(A\text{&#39;s}\)</span> sum of squares and the error sums of squares,</p>
<p><span class="math display">\[\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E}.\]</span></p>
<p>This result is called the <strong><em>sums of squares decomposition formula</em></strong>.

The equality means that if the <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> goes up, then the
<span class="math inline">\(\textbf{SS}_\textbf{E}\)</span> must go down if <span class="math inline">\(\textbf{SS}_{\textbf{Total}}\)</span> remains
the same. We use these results to build our test statistic and organize this information in
what is called an <strong><em>ANOVA table</em></strong>.

The ANOVA table is generated using the
<code>anova</code> function applied to the reference-coded model, <code>lm2</code>:

</p>
<pre class="sourceCode r"><code class="sourceCode r">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)
<span class="kw">anova</span>(lm2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Years
##            Df  Sum Sq Mean Sq F value Pr(&gt;F)
## Attr        2   70.94  35.469    2.77  0.067
## Residuals 111 1421.32  12.805</code></pre>
<p>Note that the ANOVA table has a row labelled <code>Attr</code>, which contains information
for the grouping variable (we’ll generally refer to this as explanatory variable
<span class="math inline">\(A\)</span> but here it is the picture group that was randomly assigned), and a row
labeled <code>Residuals</code>, which is synonymous with “Error”. The Sums of Squares
(SS) are available in the <code>Sum Sq</code> column. It doesn’t show a row for “Total” but
the <span class="math inline">\(\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E} = 1492.26\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">70.94</span> <span class="op">+</span><span class="st"> </span><span class="fl">1421.32</span></code></pre>
<pre><code>## [1] 1492.26</code></pre>
<div class="figure"><span id="fig:Figure3-4"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-4-1.png" alt="(ref:fig3-4)" width="576" />
<p class="caption">
Figure 0.35: (ref:fig3-4)
</p>
</div>
<p>It may be easiest to understand the <em>sums of squares decomposition</em> by connecting
it to our permutation ideas.


In a permutation situation, the total variation
(<span class="math inline">\(SS_\text{Total}\)</span>) cannot change – it is the same responses varying
around the grand mean. However, the amount of variation attributed to variation
among the means and in the residuals can change if we change which observations go
with which group. In Figure <a href="acknowledgments.html#fig:Figure3-4">0.35</a> (panel a), the means, sums of
squares, and 95% confidence intervals for each mean are displayed for the three
treatment levels from the original prisoner rating data. Three permuted versions
of the data set are summarized in panels (b), (c), and (d). The <span class="math inline">\(\text{SS}_A\)</span> is 70.9
in the real data set and between 3.7 and 27.5 in the permuted data sets.
If you had
to pick among the plots for the one with the most evidence of a difference in the
means, you hopefully would pick panel (a). This visual “unusualness” suggests
that this observed result is unusual relative to the possibilities under
permutations, which are, again, the possibilities tied to having the null
hypothesis being true. But note that the differences here are not that great
between these three permuted data sets and the real one. It is likely that at
least some might have selected panel (d) as also looking like it shows
some evidence of differences (maybe not the most?) as it looks like it
shows some evidence differences.</p>
<p>One way to think about <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> is that it is a function that
converts the variation in the group means into a single value. This makes it a
reasonable test statistic in a permutation testing context.

By comparing the
observed <span class="math inline">\(\text{SS}_A =\)</span> 70.9 to the permutation results of
20.3, 27.5, and 3.7 we see
that the observed result is much more extreme than the three alternate versions.
In contrast to our previous test statistics where positive and negative
differences were possible, <span class="math inline">\(\text{SS}_A\)</span> is always positive with a value of 0
corresponding to no variation in the means. The larger the <span class="math inline">\(\text{SS}_A\)</span>, the more
variation there is in the means. The permutation p-value for the alternative
hypothesis of <strong>some</strong> (not of greater or less than!) difference in the true
means of the groups will involve counting the number of permuted <span class="math inline">\(SS_A^*\)</span> results
that are as large or larger than what we observed.
</p>
<p>(ref:fig3-4) Plot of means and 95% confidence intervals for the three groups
for the real Mock Jury data (a) and three different permutations of the treatment labels
to the same responses in (b), (c), and (d). Note that <code>SSTotal</code> is always the same
but the different amounts of variation associated with the means (<code>SSA</code>) or the
errors (<code>SSE</code>) changes in permutation.</p>
<p>To do a permutation test,

we need to be able to calculate and extract the
<span class="math inline">\(\text{SS}_A\)</span> value. In the ANOVA table, it is the second number in the first row;
we can use the bracket, <code>[,]</code>, referencing to extract that
number from the ANOVA table that <code>anova</code> produces with
<code>anova(lm(Years~Attr, data=MockJury))[1, 2]</code>. We’ll store the observed value
of <span class="math inline">\(\text{SS}_A\)</span> in <code>Tobs</code>, reusing some ideas from Chapter <a href="#chapter2"><strong>??</strong></a>.
</p>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury))[<span class="dv">1</span>,<span class="dv">2</span>]; Tobs</code></pre>
<pre><code>## [1] 70.93836</code></pre>
<p>The following code performs the permutations <code>B=1,000</code> times using the
<code>shuffle</code> function, builds up a vector of results in <code>Tobs</code>, and then makes
a plot of the resulting permutation distribution:</p>
<p>(ref:fig3-5) Histogram and density curve of permutation distribution of
<span class="math inline">\(\text{SS}_A\)</span> with the observed value of <span class="math inline">\(\text{SS}_A\)</span> displayed as a bold,
vertical line. The proportion of results that are as large or larger than the observed
value of <span class="math inline">\(\text{SS}_A\)</span> provides an estimate of the p-value.</p>
<div class="figure"><span id="fig:Figure3-5"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-5-1.png" alt="(ref:fig3-5)" width="960" />
<p class="caption">
Figure 0.36: (ref:fig3-5)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury))[<span class="dv">1</span>,<span class="dv">2</span>]
  }
<span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">550</span>))
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<p>The right-skewed distribution (Figure <a href="acknowledgments.html#fig:Figure3-5">0.36</a>) contains the
distribution of <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span> under permutations (where
all the groups are assumed to be equivalent under the null hypothesis). While
the observed result is larger than many of the <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span>, there are
also many permuted results that are much larger than observed. The proportion
of permuted results that exceed the observed value is found using <code>pdata</code>
as before, except only for the area to the right of the observed result. We know
that <code>Tobs</code> will always be positive so no absolute values are required here.
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.067</code></pre>
<p>This provides a permutation-based p-value of 0.067 and suggests marginal evidence
against the null hypothesis of no difference in the true means. We would interpret
this p-value as saying that there is a 6.7% chance of getting a <span class="math inline">\(\text{SS}_A\)</span>
as large or larger than we observed, given that the null hypothesis is true.
</p>
<p>It ends up that some nice parametric statistical results
are available (if our assumptions are met) for the ratio of estimated variances,
the estimated variances are called <strong><em>Mean Squares</em></strong>.

To turn sums of squares into mean square (variance) estimates,
we divide the sums of squares by the amount of free information available. For
example, remember the typical variance estimator introductory statistics,
<span class="math inline">\(\Sigma^N_1(y_i-\bar{y})^2/(N-1)\)</span>? Your instructor spent some time trying various
approaches to explaining why we have a denominator of <span class="math inline">\(N-1\)</span>. The most useful for our
purposes moving forward is that we “lose” one piece of information to estimate
the mean and there are <span class="math inline">\(N\)</span> deviations around the single mean so we divide by
<span class="math inline">\(N-1\)</span>. The main point is that the sums of squares were divided by something and
we got an estimator for the variance, here of the observations overall.</p>
<p>Now consider <span class="math inline">\(\text{SS}_E = \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2\)</span>
which still has <span class="math inline">\(N\)</span> deviations but it varies around the <span class="math inline">\(J\)</span> means, so the</p>
<p><span class="math display">\[\textbf{Mean Square Error} = \text{MS}_E = \text{SS}_E/(N-J).\]</span></p>
<p>Basically, we lose <span class="math inline">\(J\)</span> pieces of information in this calculation because we have
to estimate <span class="math inline">\(J\)</span> means. The similar calculation of the <strong><em>Mean Square for variable</em></strong> <span class="math inline">\(\mathbf{A}\)</span>
(<span class="math inline">\(\text{MS}_A\)</span>) is harder to see in the formula
(<span class="math inline">\(\text{SS}_A = \Sigma^J_{j=1}n_j(\bar{y}_i-\bar{\bar{y}})^2\)</span>), but the same
reasoning can be used to understand the denominator for forming <span class="math inline">\(\text{MS}_A\)</span>:
there are <span class="math inline">\(J\)</span> means that vary around the grand mean so</p>
<p><span class="math display">\[\text{MS}_A = \text{SS}_A/(J-1).\]</span></p>
<p>In summary, the two mean squares are simply:</p>
<ul>
<li><p><span class="math inline">\(\text{MS}_A = \text{SS}_A/(J-1)\)</span>, which estimates the variance of the group
means around the grand mean.</p></li>
<li><p><span class="math inline">\(\text{MS}_{\text{Error}} = \text{SS}_{\text{Error}}/(N-J)\)</span>, which estimates
the variation of the errors around the group means.</p></li>
</ul>

<p>These results are put together using a ratio to define the <strong><em>ANOVA F-statistic</em></strong>
(also called the <strong><em>F-ratio</em></strong>) as:</p>
<p><span class="math display">\[F=\text{MS}_A/\text{MS}_{\text{Error}}.\]</span></p>
<p>If the variability in the means is “similar” to the variability in the residuals,
the statistic would have a value around 1. If that variability is similar then
there would be no evidence of a difference in the means. If the <span class="math inline">\(\text{MS}_A\)</span> is much
larger than the <span class="math inline">\(\text{MS}_E\)</span>, the <span class="math inline">\(F\)</span>-statistic will provide evidence against
the null hypothesis. The “size” of the <span class="math inline">\(F\)</span>-statistic is formalized by finding the
p-value. The <span class="math inline">\(F\)</span>-statistic, if assumptions discussed below are met and we assume
the null hypothesis is true, follows what is called an <span class="math inline">\(F\)</span>-distribution.

The
<strong><em>F-distribution</em></strong> is a right-skewed distribution whose shape is defined by what
are called the <strong><em>numerator degrees of freedom</em></strong> (<span class="math inline">\(J-1\)</span>) and the
<strong><em>denominator degrees of freedom</em></strong> (<span class="math inline">\(N-J\)</span>). These names correspond to the values
that we used to calculate the mean squares and where in the <span class="math inline">\(F\)</span>-ratio each mean
square was used; <span class="math inline">\(F\)</span>-distributions are denoted by their degrees of freedom using
the convention of <span class="math inline">\(F\)</span> (<em>numerator df</em>, <em>denominator df</em>). Some examples of
different <span class="math inline">\(F\)</span>-distributions are displayed for you in Figure <a href="acknowledgments.html#fig:Figure3-6">0.37</a>. </p>
<p>The characteristics of the F-distribution can be summarized as:</p>
<ul>
<li><p>Right skewed,</p></li>
<li><p>Nonzero probabilities for values greater than 0,</p></li>
<li><p>Its shape changes depending on the <strong>numerator</strong> and <strong>denominator DF</strong>, and</p></li>
<li><p><strong>Always use the right-tailed area for p-values.</strong></p></li>
</ul>
<p>(ref:fig3-6) Density curves of four different <span class="math inline">\(F\)</span>-distributions. Upper left is an
<span class="math inline">\(F(2, 111)\)</span>, upper right is <span class="math inline">\(F(2, 10)\)</span>, lower left is <span class="math inline">\(F(6, 10)\)</span>, and lower right
is <span class="math inline">\(F(6, 111)\)</span>. P-values are found using the areas to the right of the observed
<span class="math inline">\(F\)</span>-statistic value in all F-distributions. </p>
<div class="figure"><span id="fig:Figure3-6"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-6-1.png" alt="(ref:fig3-6)" width="480" />
<p class="caption">
Figure 0.37: (ref:fig3-6)
</p>
</div>
<p>Now we are ready to discuss an ANOVA table since we know about each of its
components. Note the general format of the ANOVA table is in Table <a href="acknowledgments.html#tab:Table3-2">0.5</a><a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>:
</p>

<table>
<caption><span id="tab:Table3-2">Table 0.5: </span> General One-Way ANOVA table.</caption>
<colgroup>
<col width="13%" />
<col width="7%" />
<col width="18%" />
<col width="21%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source  </th>
<th align="left">DF </th>
<th align="left">Sums of<br />
Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F-ratio</th>
<th align="left">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Variable A</td>
<td align="left"><span class="math inline">\(J-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A=\text{SS}_A/(J-1)\)</span></td>
<td align="left"><span class="math inline">\(F=\text{MS}_A/\text{MS}_E\)</span></td>
<td align="left">Right tail of <span class="math inline">\(F(J-1,N-J)\)</span></td>
</tr>
<tr class="even">
<td align="left">Residuals</td>
<td align="left"><span class="math inline">\(N-J\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_E\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_E = \text{SS}_E/(N-J)\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(N-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_{\text{Total}}\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The table is oriented to help you reconstruct the <span class="math inline">\(F\)</span>-ratio from each of its
components. The output from R is similar although it does not provide the last row
and sometimes switches the order of columns in different functions we will use. The R version of the table for the type
of picture effect (<code>Attr</code>) with <span class="math inline">\(J=3\)</span> levels and <span class="math inline">\(N=114\)</span> observations, repeated
from above, is:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(lm2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Years
##            Df  Sum Sq Mean Sq F value Pr(&gt;F)
## Attr        2   70.94  35.469    2.77  0.067
## Residuals 111 1421.32  12.805</code></pre>
<p>The p-value from the <span class="math inline">\(F\)</span>-distribution is 0.067.  We can
verify this result using the observed <span class="math inline">\(F\)</span>-statistic of 2.77
(which came from taking the ratio of the two mean squares,
F=35.47/12.8)
which follows an <span class="math inline">\(F(2, 111)\)</span> distribution if the null hypothesis is true and some
other assumptions are met.</p>
<p>Using the <code>pf</code> function provides us with areas in the
specified <span class="math inline">\(F\)</span>-distribution with the <code>df1</code> provided to the function as the
numerator <em>df</em> and <code>df2</code> as the denominator <em>df</em> and <code>lower.tail=F</code> reflecting
our desire for a right tailed area. 
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pf</span>(<span class="fl">2.77</span>, <span class="dt">df1=</span><span class="dv">2</span>, <span class="dt">df2=</span><span class="dv">111</span>, <span class="dt">lower.tail=</span>F)</code></pre>
<pre><code>## [1] 0.06699803</code></pre>
<p>The result from the <span class="math inline">\(F\)</span>-distribution using this parametric procedure is similar to
the p-value obtained using permutations with the test statistic of the
<span class="math inline">\(\text{SS}_A\)</span>, which was 0.067. The <span class="math inline">\(F\)</span>-statistic obviously is another
potential test statistic to use as a test statistic in a permutation approach,
now that we know about it. We should check that we get similar results from it
with permutations as we did from using <span class="math inline">\(\text{SS}_A\)</span> as a permutation-test test
statistic. The following code generates the permutation distribution

for the
<span class="math inline">\(F\)</span>-statistic (Figure <a href="acknowledgments.html#fig:Figure3-7">0.38</a>) and assesses how unusual the observed
<span class="math inline">\(F\)</span>-statistic of 2.77 was in this permutation distribution.
The only change in the code involves moving from extracting <span class="math inline">\(\text{SS}_A\)</span> to
extracting the <span class="math inline">\(F\)</span>-ratio which is in the 4<sup>th</sup> column of the <code>anova</code>
output:</p>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury))[<span class="dv">1</span>,<span class="dv">4</span>]; Tobs</code></pre>
<pre><code>## [1] 2.770024</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury))[<span class="dv">1</span>,<span class="dv">4</span>]
}

<span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0.062</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<p>(ref:fig3-7) Histogram and density curve of the permutation distribution of
the F-statistic with bold, vertical line for the observed value of the test
statistic of 2.77.</p>
<div class="figure"><span id="fig:Figure3-7"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-7-1.png" alt="(ref:fig3-7)" width="960" />
<p class="caption">
Figure 0.38: (ref:fig3-7)
</p>
</div>
<p>The permutation-based p-value is 0.062 which, again, matches the other
results closely. The first conclusion is that using a test statistic of either
the <span class="math inline">\(F\)</span>-statistic or the <span class="math inline">\(\text{SS}_A\)</span> provide similar permutation results.
However, we tend to favor using the <span class="math inline">\(F\)</span>-statistic because it is more commonly used
in reporting ANOVA results, not because it is any better in a permutation context.</p>
<p>It is also interesting to compare the permutation distribution for the
<span class="math inline">\(F\)</span>-statistic and the parametric <span class="math inline">\(F(2, 111)\)</span> distribution
(Figure <a href="acknowledgments.html#fig:Figure3-8">0.39</a>). They do not match perfectly but are quite similar.
Some the differences around 0 are due to the behavior of the method used to create
the density curve and are not really a problem for the methods. The similarity in
the two curves explains why both methods give similar results. In some
situations, the correspondence will not be quite so close.</p>
<p>(ref:fig3-8) Comparison of <span class="math inline">\(F(2, 111)\)</span> (dashed line) and permutation distribution
(solid line).</p>
<div class="figure"><span id="fig:Figure3-8"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-8-1.png" alt="(ref:fig3-8)" width="480" />
<p class="caption">
Figure 0.39: (ref:fig3-8)
</p>
</div>
<p>So how can we rectify this result (<span class="math inline">\(\text{p-value}\approx 0.06\)</span>) and the
Chapter <a href="#chapter2"><strong>??</strong></a> result that detected a difference between <em>Average</em>
and <em>Unattractive</em> with a <span class="math inline">\(\text{p-value}\approx 0.03\)</span>? I selected the two groups
to compare in Chapter <a href="#chapter2"><strong>??</strong></a> because they were furthest apart.
“Cherry-picking” the comparison that is likely to be most different creates a
false sense of the real situation and inflates the Type I error rate because of
the selection<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>.


If the entire suite of pairwise comparisons are considered, this
result may lose some of its luster. In other words, if we consider the suite of
three pair-wise differences (and the tests) implicit in comparing all of them,
we may need stronger evidence in the most different pair than a
p-value of 0.033 to suggest overall differences. In this situation,
the <em>Beautiful</em> and <em>Average</em>
groups are not that different from each other so their difference does not
contribute much to the overall <span class="math inline">\(F\)</span>-test. In Section <a href="acknowledgments.html#section3-6">0.17</a>, we will
revisit this topic and consider a method that is
statistically valid for performing all possible pair-wise comparisons that is also
consistent with our overall test results.</p>
</div>
<div id="section3-4" class="section level2">
<h2><span class="header-section-number">0.15</span> ANOVA model diagnostics including QQ-plots</h2>
<p>The requirements for a One-Way ANOVA <span class="math inline">\(F\)</span>-test are similar to those discussed in
Chapter <a href="#chapter2"><strong>??</strong></a>, except that there are now <span class="math inline">\(J\)</span> groups instead of only 2.
Specifically, the linear model assumes:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>,</p></li>
<li><p><strong>Equal variances</strong>, and</p></li>
<li><p><strong>Normal distributions</strong>.</p></li>
</ol>
<p>For assessing equal variances across the groups, it is best to use plots to assess this. We can use boxplots and beanplots to compare the spreads of the
groups, which were provided in Figure <a href="acknowledgments.html#fig:Figure3-1">0.32</a>. The range and IQRs
should be relatively similar across the groups if you do not find evidence of a
problem with this assumption. You should start with noting how clear or big the
violation of the assumption might be but remember that there will always be some
differences in the variation among groups even if the true variability is exactly
equal in the populations. In addition to our direct plotting, there are some
diagnostic plots available from the <code>lm</code> function that can help us more
clearly assess potential violations of the previous assumptions.
</p>
<p>We can obtain a suite of four diagnostic plots by using the <code>plot</code> function on
any linear model object that we have fit. To get all the plots together in four
panels we need to add the <code>par(mfrow=c(2,2))</code> command to tell R to make a graph
with 4 panels<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lm2, <span class="dt">pch=</span><span class="dv">16</span>)</code></pre>
<p>There are two plots in Figure <a href="acknowledgments.html#fig:Figure3-9">0.40</a> with useful information for assessing the
equal variance assumption. The “Residuals vs Fitted” panel in the top left panel displays the residuals <span class="math inline">\((e_{ij} = y_{ij}-\hat{y}_{ij})\)</span> on the y-axis and the fitted values
<span class="math inline">\((\hat{y}_{ij})\)</span> on the x-axis.

This allows you to see if the variability of the
observations differs across the groups as a function of the mean of the groups,
because all the observations in the same group get the same fitted value – the
mean of the group. In this plot, the points seem to have fairly similar spreads
at the fitted values for the three groups with fitted values of 4, 4.3, and 6.
The “Scale-Location” plot in the lower left panel has the same x-axis but the
y-axis contains the square-root of the absolute value of the standardized
residuals.

The absolute value transforms all the residuals into a magnitude
scale (removing direction) and the square-root helps you see differences in
variability more accurately. The standardization scales the residuals to have a variance
of 1 so help you in other displays to get a sense of how many standard deviations
you are away from the mean in the residual distribution. The visual assessment is
similar in the two plots – you want to consider whether it appears that the
groups have somewhat similar or noticeably different amounts of variability. If
you see a clear funnel shape in the Residuals vs Fitted

or an increase or decrease
in the upper edge of points in the Scale-Location plot that may indicate a
violation of the constant variance assumption.

Remember that some variation
across the groups is expected and is OK, but large differences in spreads are problematic for all the procedures that involve linear models. When discussing
these results, you want to discuss how clearly the differences in variation are
and whether that <em>shows a clear violation of the assumption</em> of equal variance
for all observations. Like in hypothesis testing, you can never prove that you’ve
met assumptions based on a plot “looking OK”, but you can say that there is no
clear evidence that the assumption is violated!
</p>

<div class="figure"><span id="fig:Figure3-9"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-9-1.png" alt="Default diagnostic plots for the Mock Jury full linear model." width="960" />
<p class="caption">
Figure 0.40: Default diagnostic plots for the Mock Jury full linear model.
</p>
</div>
<p>The linear model also assumes that all the random errors (<span class="math inline">\(\varepsilon_{ij}\)</span>) follow a
normal distribution. To gain insight into the validity of this assumption, we
can explore the original observations as displayed in the beanplots, mentally
subtracting off the differences in the means and focusing on the shapes of the
distributions of observations in each group. These plots are especially good for
assessing whether there is a skew or outliers present in each group.   If so,
by definition, the normality assumption is violated. But our assumption is
about the distribution of all the errors after removing the differences
in the means and so we want an overall assessment technique to understand how
reasonable our assumption is overall for our model. The residuals from the entire

model provide us with estimates of the random errors and if the normality
assumption is met, then the residuals all-together should approximately follow a
normal distribution. The <strong><em>Normal Q-Q Plot</em></strong> in the upper right panel of
Figure <a href="acknowledgments.html#fig:Figure3-9">0.40</a> is a direct visual assessment of how well our
residuals match what we would expect from a normal distribution. Outliers, skew,
heavy and light-tailed aspects of distributions (all violations of normality)
show up in this plot once you learn to read it – which is our next task. To
make it easier to read QQ-plots, it is nice to start with just considering
histograms and/or density plots of the residuals and to see how that maps into
this new display. We can obtain the residuals from the linear model using the
<code>residuals</code> function on any linear model object.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
eij &lt;-<span class="st"> </span><span class="kw">residuals</span>(lm2)
<span class="kw">hist</span>(eij, <span class="dt">main=</span><span class="st">&quot;Histogram of residuals&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(eij), <span class="dt">main=</span><span class="st">&quot;Density plot of residuals&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Density&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;Residuals&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure3-10"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-10-1.png" alt="Histogram and density curve of the linear model raw residuals from the Mock Jury linear model." width="960" />
<p class="caption">
Figure 0.41: Histogram and density curve of the linear model raw residuals from the Mock Jury linear model.
</p>
</div>
<p>Figure <a href="acknowledgments.html#fig:Figure3-10">0.41</a> shows that there is a right skew present in the
residuals for the model for the prisoner ratings that accounted for different
means in the three picture groups, which is consistent with the initial assessment of
some right skew in the plots of observations in each group.</p>
<p>A Quantile-Quantile plot (<strong><em>QQ-plot</em></strong>)

shows the “match” of an observed
distribution with a theoretical distribution, almost always the normal
distribution. They are also known as Quantile Comparison, Normal Probability,
or Normal Q-Q plots, with the last two names being specific to comparing
results to a normal distribution. In this version<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a>, the QQ-plots display the value of
observed percentiles in the residual distribution on the y-axis versus the
percentiles of a theoretical normal distribution on the x-axis. If the
observed <strong>distribution of the residuals matches the shape of the normal
distribution, then the plotted points should follow a 1-1 relationship.</strong>
If the points follow the displayed straight line then that suggests that the
residuals have a similar shape to a normal distribution. Some variation is
expected around the line and some patterns of deviation are worse
than others for our models, so you need to go beyond saying “it does not match
a normal distribution”. It is best to be specific about the type of deviation
you are detecting. And to do that, we need to practice interpreting some
QQ-plots.</p>
<p>The QQ-plot of the linear model residuals from Figure <a href="acknowledgments.html#fig:Figure3-9">0.40</a> is extracted and enhanced it a little to make Figure <a href="acknowledgments.html#fig:Figure3-11">0.42</a> so we
can just focus on it.

We know from looking at the histogram that this is a
slightly right skewed distribution. The QQ-plot places the observed <strong><em>standardized</em></strong><a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a> <strong><em>residuals</em></strong> on the y-axis
and the theoretical normal values on the x-axis. The most noticeable deviation
from the 1-1 line is in the lower left corner of the plot. These are for the
negative residuals (left tail) and there are many residuals at around the same
value that are a little smaller than -1. If the distribution had followed the
normal distribution here, the points would be on the 1-1 line and there would
be some standardized residuals much smaller than -1.5. So we are not getting as
much spread in the smaller residuals as we would expect in a normal distribution.
If you go back to the histogram you can see that the smallest residuals are all
stacked up and do not spread out like the left tail of a normal distribution
should. In the right tail (positive) residuals, there is also a systematic
lifting from the 1-1 line to larger values in the residuals than the normal
would generate. For example, the point labeled as “82” in Figure <a href="acknowledgments.html#fig:Figure3-9">0.40</a> (the 82<sup>nd</sup>
observation in the data set) has a value of 3 in residuals but should actually
be smaller (maybe 2.5) if the distribution was normal. Put together, this pattern
in the QQ-plot suggests that the left tail is too compacted (too short) and
the right tail is too spread out – this is the right skew we identified from the
histogram and density curve!
</p>

<div class="figure"><span id="fig:Figure3-11"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-11-1.png" alt="QQ-plot of residuals from Mock Jury linear model." width="576" />
<p class="caption">
Figure 0.42: QQ-plot of residuals from Mock Jury linear model.
</p>
</div>
<p>Generally, when both tails deviate on the same side of the line (forming a
sort of quadratic curve, especially in more extreme cases), that is evidence
of a skew. To see some different potential shapes in QQ-plots, six different
data sets are displayed in Figures <a href="acknowledgments.html#fig:Figure3-12">0.43</a> and <a href="acknowledgments.html#fig:Figure3-13">0.44</a>.
In each row, a QQ-plot and associated density curve are displayed. If the points
are both above the 1-1 line in the lower and upper tails as in
Figure <a href="acknowledgments.html#fig:Figure3-12">0.43</a>(a), then the pattern is a right skew, here even more
extreme than in the previous real data set. If the points are below the 1-1 line in both
tails as in Figure <a href="acknowledgments.html#fig:Figure3-12">0.43</a>(c), then the pattern is identified as a
left skew. Skewed residual distributions (either direction) are problematic for
models that assume normally distributed responses but not necessarily for our
permutation approaches if all the groups have similar skewed shapes. The other
problematic pattern is to have more spread than a normal curve as in
Figure <a href="acknowledgments.html#fig:Figure3-12">0.43</a>(e) and (f). This shows up with the points being
below the line in the left tail (more extreme negative than expected by the normal)
and the points being above the line for the right tail (more extreme positive
than the normal predicts). We call these distributions <strong><em>heavy-tailed</em></strong>
which can manifest as distributions with outliers in both tails or just a bit
more spread out than a normal distribution. Heavy-tailed residual distributions
can be problematic for our models as the variation is greater than what the normal
distribution can account for and our methods might under-estimate the
variability in the results. The opposite pattern with the left tail above the
line and the right tail below the line suggests less spread (<strong><em>lighter-tailed</em></strong>)
than a normal as in Figure <a href="acknowledgments.html#fig:Figure3-12">0.43</a>(g) and (h). This pattern is
relatively harmless and you can proceed with methods that assume normality safely
as they will just be a little conservative. For any of the patterns, you would
note a potential violation of the normality assumption and then proceed to
describe the type of violation and how clear or extreme it seems to be.</p>
<p>(ref:fig3-12) QQ-plots and density curves of four simulated distributions with
different shapes.</p>
<div class="figure"><span id="fig:Figure3-12"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-12-1.png" alt="(ref:fig3-12)" width="576" />
<p class="caption">
Figure 0.43: (ref:fig3-12)
</p>
</div>
<p>Finally, to help you calibrate expectations for data that are actually normally
distributed, two data sets simulated from normal distributions are displayed in
Figure <a href="acknowledgments.html#fig:Figure3-13">0.44</a>. Note how neither follows the line exactly but
that the overall pattern matches fairly well. <strong>You have to allow for some
variation from the line in real data sets</strong> and focus on when there are really
noticeable issues in the distribution of the residuals such as those
displayed above. Again, you will never be able to prove that you have normally
distributed residuals even if the residuals are all exactly on the line, but if
you see QQ-plots as in Figure <a href="acknowledgments.html#fig:Figure3-12">0.43</a> you can determine that there is clear evidence of violations of the normality assumption.
  </p>

<div class="figure"><span id="fig:Figure3-13"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-13-1.png" alt="Two more simulated data sets, both generated from normal distributions." width="576" />
<p class="caption">
Figure 0.44: Two more simulated data sets, both generated from normal distributions.
</p>
</div>
<p>The last issues with assessing the assumptions in an ANOVA relates to
situations where the methods are more or less <strong><em>resistant</em></strong><a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a> to violations of assumptions.

In simulation studies of the performance of the <span class="math inline">\(F\)</span>-test, researchers
have found that the
parametric ANOVA <span class="math inline">\(F\)</span>-test is more resistant to violations of the assumptions of
the normality and equal variance assumptions if the design is balanced.

A <strong><em>balanced design</em></strong> occurs when each group is measured the same number of
times. The resistance decreases as the data set becomes less balanced, as the
sample sizes in the groups are more different, so having close to balance is
preferred to a more imbalanced situation if there is a choice available. There
is some intuition available here – it makes some sense that you would have better
results in comparing groups if the information available is similar in all the
groups and none are relatively under-represented. We can check the number of
observations in each group to see if they are equal or similar using the
<code>tally</code> function from the <code>mosaic</code> package. This function is useful for
being able to get counts of observations, especially for cross-classifying
observations on two variables that is used in Chapter <a href="chapter5.html#chapter5">2</a>. For just
a single variable, we use <code>tally(~x, data=...)</code>:
</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">tally</span>(<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre>
<pre><code>## Attr
##    Beautiful      Average Unattractive 
##           39           38           37</code></pre>
<p>So the sample sizes do vary among the groups and the design is technically not
balanced, but it is also very close to being balanced with only two more
observations in the largest group compared to the smallest group size. This
tells us that the <span class="math inline">\(F\)</span>-test should have some resistance to violations of
assumptions. This nearly balanced design, and the moderate sample size
(over 37 per group is considered a good but not large sample), make the
parametric and nonparametric approaches provide similar results in this data
set even in the presence of the skewed residual error distribution that presents a violation to the assumptions of the parametric procedure.</p>
</div>
<div id="section3-5" class="section level2">
<h2><span class="header-section-number">0.16</span> Guinea pig tooth growth One-Way ANOVA example</h2>
<p>A second example of the One-way ANOVA methods involves a study of length of
odontoblasts (cells that are responsible for tooth growth) in 60 Guinea Pigs
(measured in microns) from <span class="citation">Crampton (<a href="#ref-Crampton1947" role="doc-biblioref">1947</a>)</span>. <span class="math inline">\(N=60\)</span> Guinea Pigs were obtained
from a local breeder and each received one of three dosages (0.5, 1, or
2 mg/day) of Vitamin C via one of two delivery methods, Orange Juice (<em>OJ</em>) or
ascorbic acid (the stuff in vitamin C capsules, called <span class="math inline">\(\text{VC}\)</span> below) as the source
of Vitamin C in their diets. Each guinea pig was randomly assigned to receive
one of the six different treatment combinations possible
(OJ at 0.5 mg, OJ at 1 mg, OJ at 2 mg, VC at 0.5 mg, VC at 1 mg, and VC at 2 mg).
The animals were treated similarly otherwise and we can assume lived in
separate cages and only one observation was taken for each guinea pig, so we
can assume the observations are independent<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a>. We need to create a variable that
combines the levels of delivery type (OJ, VC) and the dosages (0.5, 1, and 2)
to use our One-Way ANOVA on the six levels. The <code>interaction</code> function can be
used create a new variable that is based on combinations of the levels of other
variables. Here a new variable is created in the <code>ToothGrowth</code> tibble that
we called <code>Treat</code> using the <code>interaction</code> function that provides a six-level grouping variable for our One-Way
ANOVA to compare the combinations of treatments. To get a sense of the pattern of
observations in the data set, the counts in <code>supp</code> (supplement type) and
<code>dose</code> are provided and then the counts in the new categorical explanatory variable, <code>Treat</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(ToothGrowth) <span class="co">#Available in Base R</span>
<span class="kw">require</span>(tibble)
ToothGrowth &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(ToothGrowth) <span class="co">#Convert data.frame to tibble</span>
<span class="kw">require</span>(mosaic)</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tally</span>(<span class="op">~</span>supp, <span class="dt">data=</span>ToothGrowth) <span class="co">#Supplement Type (VC or OJ)</span></code></pre>
<pre><code>## supp
## OJ VC 
## 30 30</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tally</span>(<span class="op">~</span>dose, <span class="dt">data=</span>ToothGrowth) <span class="co">#Dosage level</span></code></pre>
<pre><code>## dose
## 0.5   1   2 
##  20  20  20</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Creates a new variable Treat with 6 levels</span>
ToothGrowth<span class="op">$</span>Treat &lt;-<span class="st"> </span><span class="kw">with</span>(ToothGrowth, <span class="kw">interaction</span>(supp, dose)) 

<span class="co">#New variable that combines supplement type and dosage</span>
<span class="kw">tally</span>(<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth) </code></pre>
<pre><code>## Treat
## OJ.0.5 VC.0.5   OJ.1   VC.1   OJ.2   VC.2 
##     10     10     10     10     10     10</code></pre>
<p>The <code>tally</code> function helps us to check for balance;

this is a balanced design
because the same number of guinea pigs (<span class="math inline">\(n_j=10 \text{ for } j=1, 2,\ldots, 6\)</span>)
were measured in each treatment combination.</p>
<p>With the variable <code>Treat</code> prepared, the first task is to visualize the results
using boxplots and beanplots<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a> (Figure <a href="acknowledgments.html#fig:Figure3-14">0.45</a>) and generate some summary statistics for
each group using <code>favstats</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth)</code></pre>
<pre><code>##    Treat  min     Q1 median     Q3  max  mean       sd  n missing
## 1 OJ.0.5  8.2  9.700  12.25 16.175 21.5 13.23 4.459709 10       0
## 2 VC.0.5  4.2  5.950   7.15 10.900 11.5  7.98 2.746634 10       0
## 3   OJ.1 14.5 20.300  23.45 25.650 27.3 22.70 3.910953 10       0
## 4   VC.1 13.6 15.275  16.50 17.300 22.5 16.77 2.515309 10       0
## 5   OJ.2 22.4 24.575  25.95 27.075 30.9 26.06 2.655058 10       0
## 6   VC.2 18.5 23.375  25.95 28.800 33.9 26.14 4.797731 10       0</code></pre>
<p>(ref:fig3-14) Boxplot and beanplot of odontoblast growth responses for the six
treatment level combinations.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth, <span class="dt">ylab=</span><span class="st">&quot;Odontoblast Growth in microns&quot;</span>)
<span class="kw">beanplot</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;yellow&quot;</span>,
         <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Odontoblast Growth in microns&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure3-14"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-14-1.png" alt="(ref:fig3-14)" width="1152" />
<p class="caption">
Figure 0.45: (ref:fig3-14)
</p>
</div>
<p>Figure <a href="acknowledgments.html#fig:Figure3-14">0.45</a> suggests that the mean tooth growth increases with
the dosage level and that OJ might lead to higher growth rates than VC except
at a dosage of 2 mg/day. The variability around the means looks to be small
relative to the differences among the means, so we should expect a small
p-value from our <span class="math inline">\(F\)</span>-test. The design is balanced as noted above (<span class="math inline">\(n_j=10\)</span>
for all six groups) so the methods are some what resistant to impacts from
non-normality and non-constant variance but we should still
assess the patterns in the plots.

There is some suggestion of non-constant variance in the plots but this will be explored
further below when we can remove the difference in the means and combine all
the residuals together.

There might be some skew in the responses in some of
the groups but there are only 10 observations per group so
visual evidence of skew in the boxplots and beanplots
could be generated by impacts of very few of the observations.</p>
<p>Now we can apply our 6+ steps for performing a hypothesis test

with these observations. The initial step is deciding on the claim to be assessed and the
test statistic to use. This is a six group situation with a quantitative
response, identifying it as a One-Way ANOVA where we want to test a null
hypothesis that all the groups have the same population mean, at least to
start. We will use a 5% significance level.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Hypotheses</strong>:</p>
<p><span class="math inline">\(\boldsymbol{H_0: \mu_{\text{OJ}0.5} = \mu_{\text{VC}0.5} = \mu_{\text{OJ}1} = \mu_{\text{VC}1} = \mu_{\text{OJ}2} = \mu_{\text{VC}2}}\)</span></p>
<p><strong>vs</strong></p>
<p><span class="math inline">\(\boldsymbol{H_A:}\textbf{ Not all } \boldsymbol{\mu_j} \textbf{ equal}\)</span></p>
<ul>
<li><p>The null hypothesis could also be written in reference-coding as below
since OJ.0.5 is chosen as the baseline group (discussed below).</p>
<ul>
<li><span class="math inline">\(\boldsymbol{H_0:\tau_{\text{VC}0.5}=\tau_{\text{OJ}1}=\tau_{\text{VC}1}=\tau_{\text{OJ}2}=\tau_{\text{VC}2}=0}\)</span></li>
</ul></li>
<li><p>The alternative hypothesis can be left a bit less specific:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal 0}\)</span></li>
</ul></li>
</ul></li>
<li><p><strong>Validity conditions</strong>:
</p>
<ul>
<li><p>Independence:</p>
<ul>
<li><p>This is where the separate cages note above is important. Suppose that
there were cages that contained multiple animals and they competed for
food or could share illness or levels of activity. The animals in one
cage might be systematically different from the others and this
“clustering” of observations would present a potential violation of the
independence assumption.</p>
<p>If the experiment had the animals in separate
cages, there is no clear dependency in the design of the study and we can
assume<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a> that there is no problem with
this assumption.</p></li>
</ul></li>
<li><p>Constant variance:</p>
<ul>
<li>As noted above, there is some indication of a difference in the
variability among the groups in the boxplots and beanplots but the sample
size was small in each group. We need to fit the linear model to get
the other diagnostic plots to make an overall assessment.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(m2,<span class="dt">pch=</span><span class="dv">16</span>)</code></pre>
<div class="figure"><span id="fig:Figure3-15"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-15-1.png" alt="Diagnostic plots for the odontoblast growth model." width="960" />
<p class="caption">
Figure 0.46: Diagnostic plots for the odontoblast growth model.
</p>
</div>
<ul>
<li><p>The Residuals vs Fitted panel in Figure <a href="acknowledgments.html#fig:Figure3-15">0.46</a> shows some
difference in the spreads but the spread is not that different between the groups.</p></li>
<li><p>The Scale-Location plot also shows just a little less variability in the group
with the smallest fitted value but the spread of the groups looks fairly similar in
this alternative scaling.</p></li>
<li><p>Put together, the evidence for non-constant variance is not that strong
and we can proceed comfortably that there is at least not a major problem
with this assumption.</p></li>
</ul></li>
<li><p>Normality of residuals: </p>
<ul>
<li>The Normal Q-Q plot shows a small deviation in the lower tail but nothing that
we wouldn’t expect from a normal distribution. So there is no evidence of a problem
with the normality assumption in the upper right panel of Figure <a href="acknowledgments.html#fig:Figure3-15">0.46</a>.</li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic</strong>:</p>
<ul>
<li>The ANOVA table for our model follows, providing an <span class="math inline">\(F\)</span>-statistic of 41.557:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: len
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)
## Treat      5 2740.10  548.02  41.557 &lt; 2.2e-16
## Residuals 54  712.11   13.19</code></pre></li>
<li><p><strong>Find the p-value</strong>:</p>
<ul>
<li><p>There are two options here, especially since it seems that our assumptions about
variance and normality are not violated (note that we do not say “met” – we just have
no clear evidence against them). The parametric and nonparametric approaches should
provide similar results here.</p></li>
<li><p>The parametric approach is easiest – the p-value comes from the previous
ANOVA table as <code>&lt; 2e-16</code>. First, note that this is in scientific notation
that is a compact way of saying that the p-value here is <span class="math inline">\(2.2*10^{-16}\)</span> or
0.00000000000000022.
When you see <code>2.2e-16</code> in R output, it also means
that the calculation is at the numerical precision limits of the computer.
What R is really trying to report is that this is a very small number.
<strong>When you encounter p-values that are smaller than 0.0001, you should just
report that the p-value &lt; 0.0001.</strong> Do not report that it is 0 as this gives
the false impression that there is no chance of the result occurring when
it is just a really small probability. This p-value came from an <span class="math inline">\(F(5,54)\)</span>
distribution (this is the distribution of the test statistic if the null hypothesis
is true).</p></li>
<li><p>The nonparametric approach is not too hard so we can compare the two approaches here as well:</p></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(len<span class="op">~</span>Treat, <span class="dt">data=</span>ToothGrowth))[<span class="dv">1</span>,<span class="dv">4</span>]; Tobs</code></pre>
<pre><code>## [1] 41.55718</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(len<span class="op">~</span><span class="kw">shuffle</span>(Treat), <span class="dt">data=</span>ToothGrowth))[<span class="dv">1</span>,<span class="dv">4</span>]
}
<span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">3</span>))
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">3</span>), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<div class="figure"><span id="fig:Figure3-16"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-16-1.png" alt="Histogram and density curve of permutation distribution for $F$-statistic for odontoblast growth data. Observed test statistic in bold, vertical line at 41.56." width="480" />
<p class="caption">
Figure 0.47: Histogram and density curve of permutation distribution for <span class="math inline">\(F\)</span>-statistic for odontoblast growth data. Observed test statistic in bold, vertical line at 41.56.
</p>
</div>
<ul>
<li><strong>The permutation p-value was reported as 0.
This should be reported as
p-value &lt; 0.001</strong> since we did 1,000 permutations and found that none of the
permuted <span class="math inline">\(F\)</span>-statistics, <span class="math inline">\(F^*\)</span>, were larger than the observed <span class="math inline">\(F\)</span>-statistic
of 41.56. The permuted results do not exceed 6 as seen in Figure
<a href="acknowledgments.html#fig:Figure3-16">0.47</a>, so the observed result is <em>really unusual</em> relative
to the null hypothesis. As suggested previously, the parametric and
nonparametric approaches should be similar here and they were.</li>
</ul></li>
<li><p><strong>Make a decision</strong>:</p>
<ul>
<li>Reject <span class="math inline">\(H_0\)</span> since the p-value is very small.</li>
</ul>
<p></p></li>
<li><p><strong>Write a conclusion and do scope of inference</strong>:</p>
<ul>
<li><p>There is strong evidence that the different treatments
(combinations of OJ/VC and dosage levels) <strong>cause some</strong> difference in the <strong>true</strong>
mean odontoblast growth for <strong>these</strong> guinea pigs.</p>
<ul>
<li><p>We can make the causal statement of the treatment causing differences
because the treatments were randomly assigned but these inferences only
apply to these guinea pigs since they were not randomly selected from a
larger population.</p></li>
<li><p>Remember that we are making inferences to the population or true
means and not the sample means and want to make that clear in any
conclusion. When there is not a random sample from a population it is
more natural to discuss the true means since we can’t extend to the
population values.</p></li>
<li><p>The alternative is that there is some difference in the true means
– be sure to make the wording clear that you aren’t saying that all
the means differ. In fact, if you look back at Figure
<a href="acknowledgments.html#fig:Figure3-14">0.45</a>, the means for the 2 mg dosages look almost the
same so we will have a tough time arguing that all groups differ. The
<span class="math inline">\(F\)</span>-test is about finding evidence of some difference <em>somewhere</em> among the true means. The next section will
provide some additional tools to get more specific about the source of
those detected differences and allow us to get at estimates of the differences we observed to complete our interpretation.</p></li>
</ul></li>
</ul></li>
</ol>
<p>Before we leave this example, we should revisit our model estimates and
interpretations. The default model parameterization uses reference-coding.
Running the model <code>summary</code> function on <code>m2</code> provides the estimated
coefficients:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)<span class="op">$</span>coefficients</code></pre>

<pre><code>##             Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)    13.23   1.148353 11.520847 3.602548e-16
## TreatVC.0.5    -5.25   1.624017 -3.232726 2.092470e-03
## TreatOJ.1       9.47   1.624017  5.831222 3.175641e-07
## TreatVC.1       3.54   1.624017  2.179781 3.365317e-02
## TreatOJ.2      12.83   1.624017  7.900166 1.429712e-10
## TreatVC.2      12.91   1.624017  7.949427 1.190410e-10</code></pre>
<p>For some practice with the reference coding used in these models, let’s find
the estimates for observations for a couple of the groups. To work with the
parameters, you need to start with determining the baseline category that was
used by considering which level is not displayed in the output. The
<code>levels</code> function can list the groups in a categorical variable and their
coding in the data set. The first level is usually the baseline category but
you should check this in the model summary as well.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">levels</span>(ToothGrowth<span class="op">$</span>Treat)</code></pre>
<pre><code>## [1] &quot;OJ.0.5&quot; &quot;VC.0.5&quot; &quot;OJ.1&quot;   &quot;VC.1&quot;   &quot;OJ.2&quot;   &quot;VC.2&quot;</code></pre>
<p>There is a <code>VC.0.5</code> in the second row of the model summary, but there is no row for
<code>0J.0.5</code> and so this must be the baseline category. That means that the fitted value
or model estimate for the OJ at 0.5 mg/day group is the same as the <code>(Intercept)</code> row
or <span class="math inline">\(\hat{\alpha}\)</span>, estimating a mean tooth growth of 13.23 microns when the pigs get OJ
at a 0.5 mg/day dosage level. You should always start with working on the baseline level
in a reference-coded model. To get estimates for any other group, then you can use the
<code>(Intercept)</code> estimate and add the deviation for the group of interest. For
<code>VC.0.5</code>, the estimated mean tooth growth is
<span class="math inline">\(\hat{\alpha} + \hat{\tau}_2 = \hat{\alpha} + \hat{\tau}_{\text{VC}0.5}=13.23 + (-5.25)=7.98\)</span>
microns. It is also potentially interesting to directly interpret the estimated difference
(or deviation) between <code>OJ.0.5</code> (the baseline) and <code>VC.0.5</code> (group 2) that
is <span class="math inline">\(\hat{\tau}_{\text{VC}0.5}= -5.25\)</span>: we estimate that the mean tooth growth in
<code>VC.0.5</code> is 5.25 microns shorter than it is in <code>OJ.0.5</code>. This and many other
direct comparisons of groups are likely of interest to researchers involved in
studying the impacts of these supplements on tooth growth and the next section
will show us how to do that (correctly!).</p>
<p>The reference-coding is still going to feel a little uncomfortable so the comparison
to the cell-means model and exploring the effect plot can help to reinforce
that both models patch together the same estimated means for each group. For
example, we can find our estimate of 7.98 microns for the VC0.5 group in the
output and Figure <a href="acknowledgments.html#fig:Figure3-17">0.48</a>. Also note that Figure
<a href="acknowledgments.html#fig:Figure3-17">0.48</a> is the same whether you plot the results from
<code>m2</code> or <code>m3</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(len<span class="op">~</span>Treat<span class="dv">-1</span>, <span class="dt">data=</span>ToothGrowth)
<span class="kw">summary</span>(m3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = len ~ Treat - 1, data = ToothGrowth)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -8.20  -2.72  -0.27   2.65   8.27 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## TreatOJ.0.5   13.230      1.148  11.521 3.60e-16
## TreatVC.0.5    7.980      1.148   6.949 4.98e-09
## TreatOJ.1     22.700      1.148  19.767  &lt; 2e-16
## TreatVC.1     16.770      1.148  14.604  &lt; 2e-16
## TreatOJ.2     26.060      1.148  22.693  &lt; 2e-16
## TreatVC.2     26.140      1.148  22.763  &lt; 2e-16
## 
## Residual standard error: 3.631 on 54 degrees of freedom
## Multiple R-squared:  0.9712, Adjusted R-squared:  0.968 
## F-statistic:   303 on 6 and 54 DF,  p-value: &lt; 2.2e-16</code></pre>

<div class="figure"><span id="fig:Figure3-17"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-17-1.png" alt="Effect plot of the One-Way ANOVA model for the odontoblast growth data." width="384" />
<p class="caption">
Figure 0.48: Effect plot of the One-Way ANOVA model for the odontoblast growth data.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(m2))</code></pre>

</div>
<div id="section3-6" class="section level2">
<h2><span class="header-section-number">0.17</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</h2>

<p>With evidence that the true means are likely not all equal, many researchers
want to know which groups show evidence of differing from one another. This
provides information on the source of the overall difference that was
detected and detailed information on which groups differed from one another.
Because this is a shot-gun/unfocused sort of approach, some people think it
is an over-used procedure. Others feel that
it is an important method of addressing detailed questions about group
comparisons in a valid and safe way. For example, we might want to know if OJ is
different from VC <em>at the 0.5 mg/day</em> dosage level and these methods will allow
us to get an answer to this sort of question.
It also will test for differences between the OJ.0.5 and VC.2 groups
and every other pair of levels that you can construct. This method actually
takes us back to the methods in Chapter <a href="#chapter2"><strong>??</strong></a> where we compared the means of two
groups except that we need to deal with potentially many pair-wise comparisons,
making an adjustment to account for that inflation in Type I errors

that occurs due to many tests being performed at the same time.
There are many different statistical
methods to make all the pair-wise comparisons, but we will employ the most
commonly used one, called <strong><em>Tukey’s Honest Significant Difference</em></strong> (Tukey’s HSD) 
method<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a>. The name suggests that not using it could lead to a
dishonest answer and that it will give you an honest result. It is more that if you don’t
do some sort of correction for all the tests you are performing, you might find some <strong><em>spurious</em></strong><a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a> results. There are other methods that could be used
to do a similar correction and also provide “honest” inferences; we are just going to learn
one of them.</p>
<p>Generally, the challenge in this situation is that if you perform many tests at the same time (instead of just one test), you inflate the
Type I error rate.

We can define the <strong><em>family-wise error rate</em></strong> 
as the probability that at least one error is made on a set of tests or, more
compactly, Pr(At least 1 error is made) where Pr() is the probability of an
event occurring. The family-wise error is meant to capture the overall
situation in terms of measuring the likelihood of making a mistake if we
consider many tests, each with some chance of making their own mistake, and
focus on how often we make at least one error when we do many tests. A quick
probability calculation shows the magnitude of the problem. If we start with a
5% significance level test, then Pr(Type I error on one test) =0.05 and the Pr(no
errors made on one test) =0.95, by definition. This is our standard hypothesis
testing situation. Now, suppose we have <span class="math inline">\(m\)</span> independent tests, then</p>
<p><span class="math display">\[\begin{array}{ll}
&amp; \text{Pr(make at least 1 Type I error given all null hypotheses are true)} \\
&amp; = 1 - \text{Pr(no errors made)} \\
&amp; = 1 - 0.95^m.
\end{array}\]</span></p>
<p>Figure <a href="acknowledgments.html#fig:Figure3-18">0.49</a> shows how the probability of having at least one
false detection grows rapidly with the number of tests, <span class="math inline">\(m\)</span>. The plot stops at 100
tests since it is effectively a 100% chance of at least one false detection.
It might seem like doing 100 tests is a lot, but in Genetics research it is
possible to consider situations where millions of tests are
considered so these are real issues to be concerned about in many situations.
Researchers want to make sure that when they report a “significant” result that
it is really likely to be a real result and will show up as a difference in the
next data set they collect<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a>.</p>
<p>(ref:fig3-18) Plot of family-wise error rate as the number of tests performed
increases. Dashed line indicates 0.05.</p>
<div class="figure"><span id="fig:Figure3-18"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-18-1.png" alt="(ref:fig3-18)" width="480" />
<p class="caption">
Figure 0.49: (ref:fig3-18)
</p>
</div>
<p>In pair-wise comparisons between all the pairs of means in a One-Way ANOVA, the number of
tests is based on the number of pairs. We can calculate the number of tests using
<span class="math inline">\(J\)</span> choose 2, <span class="math inline">\(\begin{pmatrix}J\\2\end{pmatrix}\)</span>, to get the number of unique pairs of
size 2 that we can make out of <span class="math inline">\(J\)</span> individual treatment levels. We don’t need to
explore the combinatorics formula for this, as the <code>choose</code> function in R can give us the
answers:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">choose</span>(<span class="dv">3</span>,<span class="dv">2</span>)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">choose</span>(<span class="dv">4</span>,<span class="dv">2</span>)</code></pre>
<pre><code>## [1] 6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">choose</span>(<span class="dv">5</span>,<span class="dv">2</span>)</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">choose</span>(<span class="dv">6</span>,<span class="dv">2</span>)</code></pre>
<pre><code>## [1] 15</code></pre>
<p>So if you have three groups (prisoner rating study), there are 3 unique pairs to
compare. For six groups, like in the guinea pig study, we have to consider 15 tests to compare all the unique pairs of groups. 15 tests seems like enough that we
should be worried about inflated family-wise error rates. Fortunately, the
Tukey’s HSD method controls the family-wise error rate at your specified level
(say 0.05) across any number of pair-wise comparisons. This means that the
overall rate of at least one Type I error is controlled at the specified
significance level, often 5%. To do this, each test must use a slightly more
conservative cut-off than if just one test is performed and the procedure helps
us figure out how much more conservative we need to be.</p>
<p>Tukey’s HSD starts  with focusing on the difference between the groups with the
largest and smallest means (<span class="math inline">\(\bar{y}_{max}-\bar{y}_{min}\)</span>). If
<span class="math inline">\((\bar{y}_{max}-\bar{y}_{min}) \le \text{Margin of Error}\)</span>
for the difference in the means, then all other pairwise differences, say
<span class="math inline">\(\vert \bar{y}_j - \bar{y}_{j&#39;}\vert\)</span>, for two groups <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span>, will be less
than or equal to that margin of error. This also means that any confidence
intervals for any difference in the means will contain 0. Tukey’s HSD selects a
critical value so that (<span class="math inline">\(\bar{y}_{max}-\bar{y}_{min}\)</span>) will be less than the margin of
error in 95% of data sets drawn from populations with a common mean. This implies
that in 95% of data sets in which all the population means are the same, all
confidence intervals for differences in pairs of means will contain 0. Tukey’s
HSD provides confidence intervals for the difference in true means between
groups <span class="math inline">\(j\)</span> and <span class="math inline">\(j&#39;\)</span>, <span class="math inline">\(\mu_j-\mu_{j&#39;}\)</span>, for all pairs where <span class="math inline">\(j \ne j&#39;\)</span>, using</p>
<p><span class="math display">\[(\bar{y}_j - \bar{y}_{j&#39;}) \mp \frac{q^*}{\sqrt{2}}\sqrt{\text{MS}_E\left(\frac{1}{n_j}+
\frac{1}{n_{j&#39;}}\right)}\]</span></p>
<p>where
<span class="math inline">\(\frac{q^*}{\sqrt{2}}\sqrt{\text{MS}_E\left(\frac{1}{n_j}+\frac{1}{n_{j&#39;}}\right)}\)</span>
is the margin of error for the intervals. The distribution used
to find the multiplier, <span class="math inline">\(q^*\)</span>, for the confidence intervals is available in the
<code>qtukey</code> function and generally provides a slightly larger multiplier than the
regular <span class="math inline">\(t^*\)</span> from our two-sample <span class="math inline">\(t\)</span>-based confidence interval discussed in
Chapter <a href="#chapter2"><strong>??</strong></a>. The formula otherwise is very similar to the one used in Chapter <a href="#chapter2"><strong>??</strong></a> with the SE for the difference in the means based on a measure of residual variance (here <span class="math inline">\(MS_E\)</span>) times <span class="math inline">\(\left(\frac{1}{n_j}+\frac{1}{n_{j&#39;}}\right)\)</span> which weights the results based on the relative sample sizes in the groups.</p>
<p>We will use the <code>confint</code>, <code>cld</code>, and <code>plot</code> functions
applied to output from the <code>glht</code> function (all from the <code>multcomp</code> package;
<span class="citation">Hothorn, Bretz, and Westfall (<a href="#ref-Hothorn2008" role="doc-biblioref">2008</a>)</span>, <span class="citation">(Hothorn, Bretz, and Westfall <a href="#ref-R-multcomp" role="doc-biblioref">2019</a>)</span>) to easily get the required comparisons from our
ANOVA model.

Unfortunately, its code format is a little complicated – but there are
just two places to modify the code, by including the model name and after <code>mcp</code>
(stands for <em>multiple comparisons</em>) in the <code>linfct</code> option, you need to include the
explanatory variable name as <code>VARIABLENAME="Tukey"</code>. The last part is to get the
Tukey HSD multiple comparisons run on our explanatory variable. Once we obtain the
intervals, we can use them to test <span class="math inline">\(H_0: \mu_j = \mu_{j&#39;} \text{ vs } H_A: \mu_j \ne \mu_{j&#39;}\)</span>
by assessing whether 0 is in the confidence interval for each pair. If 0 is in the
interval, then there is no evidence of a difference for that pair. If 0 is not in the
interval, then we reject <span class="math inline">\(H_0\)</span> and have evidence <em>at the specified family-wise significance
level</em> of a difference for that pair. You will see a switch to using the
word “detection” to describe rejected null hypotheses of no difference as it
can help to compactly write up these results. The following code provides the numerical
and graphical<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a> results of applying Tukey’s HSD
to the linear model for the Guinea Pig data:</p>
<p>(ref:fig3-19) Graphical display of pair-wise comparisons from Tukey’s HSD for the
Guinea Pig data. Any confidence intervals that do not contain 0 provide evidence
of a difference in the pair of groups.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(multcomp)
Tm2 &lt;-<span class="st"> </span><span class="kw">glht</span>(m2, <span class="dt">linfct =</span> <span class="kw">mcp</span>(<span class="dt">Treat =</span> <span class="st">&quot;Tukey&quot;</span>))
<span class="kw">confint</span>(Tm2)</code></pre>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = len ~ Treat, data = ToothGrowth)
## 
## Quantile = 2.9534
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                      Estimate lwr      upr     
## VC.0.5 - OJ.0.5 == 0  -5.2500 -10.0464  -0.4536
## OJ.1 - OJ.0.5 == 0     9.4700   4.6736  14.2664
## VC.1 - OJ.0.5 == 0     3.5400  -1.2564   8.3364
## OJ.2 - OJ.0.5 == 0    12.8300   8.0336  17.6264
## VC.2 - OJ.0.5 == 0    12.9100   8.1136  17.7064
## OJ.1 - VC.0.5 == 0    14.7200   9.9236  19.5164
## VC.1 - VC.0.5 == 0     8.7900   3.9936  13.5864
## OJ.2 - VC.0.5 == 0    18.0800  13.2836  22.8764
## VC.2 - VC.0.5 == 0    18.1600  13.3636  22.9564
## VC.1 - OJ.1 == 0      -5.9300 -10.7264  -1.1336
## OJ.2 - OJ.1 == 0       3.3600  -1.4364   8.1564
## VC.2 - OJ.1 == 0       3.4400  -1.3564   8.2364
## OJ.2 - VC.1 == 0       9.2900   4.4936  14.0864
## VC.2 - VC.1 == 0       9.3700   4.5736  14.1664
## VC.2 - OJ.2 == 0       0.0800  -4.7164   4.8764</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span>
<span class="kw">plot</span>(Tm2)</code></pre>
<div class="figure"><span id="fig:Figure3-19"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-19-1.png" alt="(ref:fig3-19)" width="960" />
<p class="caption">
Figure 0.50: (ref:fig3-19)
</p>
</div>
<p>Figure <a href="acknowledgments.html#fig:Figure3-19">0.50</a> contains confidence intervals for the difference in
the means for all 15 pairs of groups. For example, the first row in the plot contains
the confidence interval for comparing VC.0.5 and OJ.0.5 (VC.0.5 <strong>minus</strong> OJ.0.5). In the numerical output, you can find that this 95%
family-wise confidence interval goes from -10.05 to -0.45 microns (<code>lwr</code> and
<code>upr</code> in the numerical output provide the CI endpoints). This interval does not
contain 0 since its upper end point is -0.45 microns and so we can now say that
there is evidence that OJ and VC have different true mean growth rates at the 0.5 mg
dosage level. We can go further and say that we are 95% confident that the difference
in the true mean tooth growth between VC.0.5 and OJ.0.5 (VC.0.5-OJ.0.5) is between
-10.05 and -0.45 microns, after adjusting for comparing all the pairs of groups.
But there are fourteen more similar intervals…</p>
<p>If you put all these pair-wise tests together, you can generate an overall
interpretation of Tukey’s HSD results that discusses sets of groups that
are not detectably different from one another and those groups that were
distinguished from other sets of groups. To do this, start with listing
out the groups that are not detectably different (CIs contain 0), which,
here, only occurs for four of the pairs. The CIs that contain 0 are for the
pairs VC.1 and OJ.0.5, OJ.2 and OJ.1, VC.2 and OJ.1, and, finally, VC.2 and
OJ.2. So VC.2, OJ.1, and OJ.2 are all not detectably different from each
other and VC.1 and OJ.0.5 are also not detectably different. If you look carefully, VC.0.5 is detected as different from every other group. So there are basically
three sets of groups that can be grouped together as “similar”: VC.2, OJ.1,
and OJ.2; VC.1 and OJ.0.5; and VC.0.5. Sometimes groups overlap with some
levels not being detectably different from other levels that belong to
different groups and the story is not as clear as it is in this case. An
example of this sort of overlap is seen in the next section.</p>
<p>There is a method that many researchers use to more efficiently generate and
report these sorts of results that is called a <strong><em>compact letter display</em></strong> 
(CLD, <span class="citation">Piepho (<a href="#ref-Piepho2004" role="doc-biblioref">2004</a>)</span>)<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a>. The <code>cld</code> function can be applied to the results from
<code>glht</code> to generate the CLD that we can use to provide a “simple” summary of
the sets of groups. In this discussion, we define a <strong>set as a union of different
groups that can contain one or more members</strong> and the member of these groups are
the different treatment levels.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld</span>(Tm2)</code></pre>
<pre><code>## OJ.0.5 VC.0.5   OJ.1   VC.1   OJ.2   VC.2 
##    &quot;b&quot;    &quot;a&quot;    &quot;c&quot;    &quot;b&quot;    &quot;c&quot;    &quot;c&quot;</code></pre>

<p>Groups with the same letter are not detectably different (are in the same set)
and groups that are detectably different get different letters (are in
different sets). Groups can have more than one letter to reflect “overlap”
between the sets of groups and sometimes a set of groups contains only a
single treatment level (VC.0.5 is a set of size 1). Note
that if the groups have the same letter, this does not mean they are the same,
just that there is <strong>insufficient evidence to declare a difference for that pair</strong>. If we consider
the previous output for the CLD, the “a” set contains VC.0.5, the “b” set contains
OJ.0.5 and VC.1, and the “c” set contains OJ.1, OJ.2, and VC.2. These are exactly
the groups of treatment levels that we obtained by going through all fifteen
pairwise results.</p>
<p>One benefit of this work is that the CLD letters can be added to a plot (such as the beanplot) to
help fully report the results and understand the sorts of differences Tukey’s
HSD detected. The lines with <code>text</code> in them are involved in placing text on the figure but are
something you could do in image editing software just as easily.
Figure <a href="acknowledgments.html#fig:Figure3-20">0.51</a> enhances the discussion by showing that the
“<b><font color='blue'>a</font></b>” group with VC.0.5 had the lowest average tooth
growth, the “<b><font color='red'>b</font></b>” group had intermediate tooth growth
for treatments OJ.0.5 and VC.1, and the highest growth rates came from
OJ.1, OJ.2, and VC.2. Even though VC.2 had the highest average growth rate,
we are not able to prove that its true mean is any higher
than the other groups labeled with “<b><font color='green'>c</font></b>”. Hopefully the
ease of getting to the story of the Tukey’s HSD results from a plot like this
explains why it is common to report results using these methods instead of
reporting 15 confidence intervals for all the pair-wise differences.</p>
<p>(ref:fig3-20) Beanplot of odontoblast growth by group with Tukey’s HSD compact
letter display.</p>
<div class="figure"><span id="fig:Figure3-20"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-20-1.png" alt="(ref:fig3-20)" width="960" />
<p class="caption">
Figure 0.51: (ref:fig3-20)
</p>
</div>
<p>There are just a couple of other details to mention on this set of methods. First,
note that we interpret the set of confidence intervals simultaneously: We are
95% confident that <strong>ALL</strong> the intervals contain the respective differences in
the true means (this is a <strong><em>family-wise interpretation</em></strong>). These intervals are
adjusted from our regular 2 sample <span class="math inline">\(t\)</span> intervals from Chapter <a href="#chapter2"><strong>??</strong></a>
to allow this stronger interpretation. Specifically,
they are wider. Second, if sample sizes are unequal in the groups, Tukey’s HSD
is conservative and provides a family-wise error rate that is lower than the
<em>nominal</em> (or specified) level. In other words, it fails less often than expected
and the intervals provided are a little wider than needed, containing all the
pairwise differences at higher than the nominal confidence level of (typically)
95%. Third, this is a parametric approach and violations of normality and
constant variance will push the method in the other direction, potentially
making the technique dangerously liberal. Nonparametric approaches to this
problem are also possible, but will not be considered here.</p>
</div>
<div id="section3-7" class="section level2">
<h2><span class="header-section-number">0.18</span> Pair-wise comparisons for Prisoner Rating data</h2>
<p>In our previous work with the prisoner rating data, the overall ANOVA test
provided only marginal evidence of some difference in the true means across the
three groups with a p-value=0.067. Tukey’s HSD does not require you
to find a small p-value from your overall <span class="math inline">\(F\)</span>-test to employ the methods
but if you apply it to situations with p-values larger than your
<em>a priori</em> significance level,
you are unlikely to find any pairs that are detected as being different. Some
statisticians suggest that you shouldn’t employ follow-up tests such as Tukey’s
HSD when there is not sufficient evidence to reject the overall null hypothesis
and would be able to reasonably criticize the following results. But for the
sake of completeness, we can find the pair-wise comparison results at our
typical 95% family-wise confidence level in this situation, with the three
confidence intervals displayed in Figure <a href="acknowledgments.html#fig:Figure3-21">0.52</a>.</p>

<div class="figure"><span id="fig:Figure3-21"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-21-1.png" alt="Tukey’s HSD confidence interval results at the 95% family-wise confidence level for the Mock Jury linear model." width="480" />
<p class="caption">
Figure 0.52: Tukey’s HSD confidence interval results at the 95% family-wise confidence level for the Mock Jury linear model.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)
<span class="kw">require</span>(multcomp)
Tm2 &lt;-<span class="st"> </span><span class="kw">glht</span>(lm2, <span class="dt">linfct =</span> <span class="kw">mcp</span>(<span class="dt">Attr =</span> <span class="st">&quot;Tukey&quot;</span>))</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(Tm2)</code></pre>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Years ~ Attr, data = MockJury)
## 
## Quantile = 2.3756
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                               Estimate lwr     upr    
## Average - Beautiful == 0      -0.3596  -2.2973  1.5780
## Unattractive - Beautiful == 0  1.4775  -0.4734  3.4284
## Unattractive - Average == 0    1.8371  -0.1262  3.8005</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld</span>(Tm2)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##          &quot;a&quot;          &quot;a&quot;          &quot;a&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">2.5</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span>
<span class="kw">plot</span>(Tm2)</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(Tm2, <span class="dt">level=</span><span class="fl">0.9</span>)</code></pre>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Years ~ Attr, data = MockJury)
## 
## Quantile = 2.0737
## 90% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                               Estimate lwr     upr    
## Average - Beautiful == 0      -0.3596  -2.0511  1.3318
## Unattractive - Beautiful == 0  1.4775  -0.2255  3.1805
## Unattractive - Average == 0    1.8371   0.1233  3.5510</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld</span>(Tm2, <span class="dt">level=</span><span class="fl">0.1</span>)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##         &quot;ab&quot;          &quot;a&quot;          &quot;b&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">2.5</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span>
<span class="kw">plot</span>(<span class="kw">confint</span>(Tm2, <span class="dt">level=</span>.<span class="dv">9</span>))</code></pre>
<div class="figure"><span id="fig:Figure3-22"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-22-1.png" alt="Tukey’s HSD 90% family-wise confidence intervals for the Mock Jury linear model." width="480" />
<p class="caption">
Figure 0.53: Tukey’s HSD 90% family-wise confidence intervals for the Mock Jury linear model.
</p>
</div>
<p>At the family-wise 5% significance level, there are no pairs that are detectably different
– they all get the same letter of “a”.   Now we will produce results for the reader that
thought a 10% significance was suitable for this application before seeing any
of the results. We just need to change the confidence level or significance
level that the CIs or tests are produced with inside the functions. For the <code>confint</code>
function, the <code>level</code> option is the confidence level and for the <code>cld</code>, it is the
family-wise significance level. Note that 90% confidence corresponds to a 10%
significance level.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(Tm2, <span class="dt">level=</span><span class="fl">0.9</span>)</code></pre>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: lm(formula = Years ~ Attr, data = MockJury)
## 
## Quantile = 2.0737
## 90% family-wise confidence level
##  
## 
## Linear Hypotheses:
##                               Estimate lwr     upr    
## Average - Beautiful == 0      -0.3596  -2.0511  1.3318
## Unattractive - Beautiful == 0  1.4775  -0.2255  3.1804
## Unattractive - Average == 0    1.8371   0.1233  3.5509</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld</span>(Tm2, <span class="dt">level=</span><span class="fl">0.1</span>)</code></pre>
<pre><code>##    Beautiful      Average Unattractive 
##         &quot;ab&quot;          &quot;a&quot;          &quot;b&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">2.5</span>,<span class="dv">1</span>,<span class="dv">1</span>)) <span class="co">#Makes room on the plot for the group names</span>
<span class="kw">plot</span>(<span class="kw">confint</span>(Tm2, <span class="dt">level=</span>.<span class="dv">9</span>))</code></pre>
<p>(ref:fig3-23) Beanplot of sentences with compact letter display results from 10%
family-wise significance level Tukey’s HSD. <em>Average</em> and <em>Unattractive</em> picture
groups are detected as being different and are displayed as belonging to
different groups. <em>Beautiful</em> picture responses are not detected as different
from the other two groups.</p>
<div class="figure"><span id="fig:Figure3-23"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-23-1.png" alt="(ref:fig3-23)" width="480" />
<p class="caption">
Figure 0.54: (ref:fig3-23)
</p>
</div>
<p>With family-wise 10% significance and 90% confidence levels, the
<em>Unattractive</em> and <em>Average</em> picture groups are detected as being different but
the <em>Average</em> group is not detected as different from <em>Beautiful</em> and
<em>Beautiful</em> is not detected to be different from <em>Unattractive</em>. This leaves the
“overlap” of groups across the sets of groups that was noted earlier. The
<em>Beautiful</em> level is not detected as being dissimilar from levels in two
different sets and so gets two different letters.</p>
<p>The beanplot (Figure <a href="acknowledgments.html#fig:Figure3-23">0.54</a>) helps to clarify some of the reasons
for this set of results. The detection of a difference between <em>Average</em> and
<em>Unattractive</em> just barely occurs and the mean for <em>Beautiful</em> is between the
other two so it ends up not being detectably different from either one. This
sort of overlap is actually a fairly common occurrence in these sorts of
situations so be prepared a mixed set of letters for some levels.</p>
<pre class="sourceCode r"><code class="sourceCode r">old.par &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mai=</span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;white&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)
<span class="kw">text</span>(<span class="kw">c</span>(<span class="dv">1</span>), <span class="kw">c</span>(<span class="fl">5.3</span>),<span class="st">&quot;ab&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>)
<span class="kw">text</span>(<span class="kw">c</span>(<span class="dv">2</span>), <span class="kw">c</span>(<span class="fl">5.1</span>),<span class="st">&quot;a&quot;</span>, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>)
<span class="kw">text</span>(<span class="kw">c</span>(<span class="dv">3</span>), <span class="kw">c</span>(<span class="fl">6.8</span>),<span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>)</code></pre>
</div>
<div id="section3-8" class="section level2">
<h2><span class="header-section-number">0.19</span> Chapter summary</h2>
<p>In this chapter, we explored methods for comparing a quantitative response across
<span class="math inline">\(J\)</span> groups (<span class="math inline">\(J \ge 2\)</span>), with what is called the One-Way ANOVA procedure. The initial
test is based on assessing evidence against a null hypothesis of no difference in the true means for the <span class="math inline">\(J\)</span> groups. There are two different methods for estimating these One-Way ANOVA models:
the cell-means model and the reference-coded versions of the model.
There are times when either model will be preferred, but for the rest of the text,
the reference coding is used (sorry!). The ANOVA <span class="math inline">\(F\)</span>-statistic,
often presented with
underlying information in the ANOVA table,

provides a method of assessing evidence
against the null hypothesis either using permutations or via the <span class="math inline">\(F\)</span>-distribution.
Pair-wise comparisons using Tukey’s HSD provide a method for comparing all the groups
and are a nice complement to the overall ANOVA results. A compact letter display was
shown that enhanced the interpretation of Tukey’s HSD result.</p>
<p>In the guinea pig example, we are left with some lingering questions based on these
results. It appears that the effect of <em>dosage</em> changes as a function of the
<em>delivery method</em> (OJ, VC) because the size of the differences between OJ and VC
change for different dosages. These methods can’t directly assess the question
of whether the effect of delivery method is the same or not across the
different dosages. In Chapter <a href="chapter4.html#chapter4">1</a>, the two variables, <em>Dosage</em> and
<em>Delivery method</em> are modeled as two separate variables so we can consider their
effects both separately and together. This allows more refined hypotheses, such as
<em>Is the effect of delivery method the same for all dosages?</em>, to be tested. This
will introduce new models and methods for analyzing data where there are two
factors as explanatory variables in a model for a quantitative response variable
in what is called the Two-Way ANOVA.</p>

</div>
<div id="section3-9" class="section level2">
<h2><span class="header-section-number">0.20</span> Summary of important R code</h2>
<p>The main components of R code used in this chapter follow with components to
modify in lighter and/or ALL CAPS text, remembering that any R packages mentioned
need to be installed and loaded for this code to have a chance of working:</p>
<ul>
<li><p><strong><font color='red'>MODELNAME</font> &lt;- lm(<font color='red'>Y</font>~<font color='red'>X</font>,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>Probably the most frequently used command in R.</p></li>
<li><p>Here it is used to fit the reference-coded One-Way ANOVA model with Y as the
response variable and X as the grouping variable, storing the estimated model
object in MODELNAME. </p></li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> &lt;- lm(<font color='red'>Y</font>~<font color='red'>X</font>-1,
data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Fits the cell means version of the One-Way ANOVA model.</li>
</ul></li>
<li><p><strong>summary(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Generates model summary information including the estimated model coefficients,
SEs, t-tests, and p-values.</li>
</ul></li>
<li><p><strong>anova(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li><p>Generates the ANOVA table but <strong>must only be run on the
reference-coded version of the model</strong>. </p></li>
<li><p>Results are incorrect if run on the cell-means model since the reduced model
under the null is that the mean of all the observations is 0!</p></li>
</ul></li>
<li><p><strong>pf(<font color='red'>FSTATISTIC</font>, df1=<font color='red'>NUMDF</font>,
df2=<font color='red'>DENOMDF</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the p-value for an observed <span class="math inline">\(F\)</span>-statistic with NUMDF and DENOMDF degrees
of freedom. </li>
</ul></li>
<li><p><strong>par(mfrow=c(2,2)); plot(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Generates four diagnostic plots including the Residuals vs Fitted and
Normal Q-Q plot.</li>
</ul></li>
<li><p><strong>plot(allEffects(<font color='red'>MODELNAME</font>))</strong></p>
<ul>
<li><p>Requires the <code>effects</code> package be loaded.</p></li>
<li><p>Plots the estimated model component. </p></li>
</ul></li>
<li><p><strong>Tm2 &lt;- glht(<font color='red'>MODELNAME</font>, linfct=mcp(<font color='red'>X</font>=“Tukey”));
confint(Tm2); plot(Tm2); cld(Tm2)</strong></p>
<ul>
<li><p>Requires the <code>multcomp</code> package to be installed and loaded.</p></li>
<li><p>Can only be run on the reference-coded version of the model.</p></li>
<li><p>Generates the text output and plot for Tukey’s HSD as well as the compact
letter display.</p></li>
</ul></li>
</ul>

</div>
<div id="section3-10" class="section level2">
<h2><span class="header-section-number">0.21</span> Practice problems</h2>
<p>For these practice problems, you will work with the cholesterol data set from
the <code>multcomp</code> package that was used to generate the Tukey’s HSD results. To
load the data set and learn more about the study, use the following code:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(multcomp)
<span class="kw">data</span>(cholesterol)
<span class="kw">require</span>(tibble)
cholesterol &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(cholesterol)
<span class="kw">help</span>(cholesterol)</code></pre>
<p>3.1. Graphically explore the differences in the changes in Cholesterol levels for
the five levels using boxplots and beanplots.</p>
<p>3.2. Is the design balanced? </p>
<p>3.3. Complete all 6+ steps of the hypothesis test using the parametric <span class="math inline">\(F\)</span>-test,
reporting the ANOVA table and the distribution of the test statistic under the null.</p>
<p>3.4. Discuss the scope of inference using the information that the treatment levels
were randomly assigned to volunteers in the study.</p>
<p>3.5. Generate the permutation distribution and find the p-value. Compare the parametric
p-value to the permutation test results.</p>
<p>3.6. Perform Tukey’s HSD on the data set. Discuss the results – which pairs were
detected as different and which were not? Bigger reductions in cholesterol are
good, so are there any levels you would recommend or that might provide similar
reductions?</p>
<p>3.7. Find and interpret the CLD and compare that to your interpretation of results
from 3.6.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Crampton1947">
<p>Crampton, E. 1947. “The Growth of the Odontoblast of the Incisor Teeth as a Criterion of Vitamin c Intake of the Guinea Pig.” <em>The Journal of Nutrition</em> 33 (5): 491–504. <a href="http://jn.nutrition.org/content/33/5/491.full.pdf">http://jn.nutrition.org/content/33/5/491.full.pdf</a>.</p>
</div>
<div id="ref-R-heplots">
<p>Fox, John, and Michael Friendly. 2018. <em>Heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</em>. <a href="https://CRAN.R-project.org/package=heplots">https://CRAN.R-project.org/package=heplots</a>.</p>
</div>
<div id="ref-R-effects">
<p>Fox, John, Sanford Weisberg, Michael Friendly, and Jangman Hong. 2018. <em>Effects: Effect Displays for Linear, Generalized Linear, and Other Models</em>. <a href="https://CRAN.R-project.org/package=effects">https://CRAN.R-project.org/package=effects</a>.</p>
</div>
<div id="ref-Gandrud2015">
<p>Gandrud, Christopher. 2015. <em>Reproducible Research with R and R Studio, Second Edition</em>. Chapman Hall, CRC.</p>
</div>
<div id="ref-Hothorn2008">
<p>Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2008. “Simultaneous Inference in General Parametric Models.” <em>Biometrical Journal</em> 50 (3): 346–63.</p>
</div>
<div id="ref-R-multcomp">
<p>Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2019. <em>Multcomp: Simultaneous Inference in General Parametric Models</em>. <a href="https://CRAN.R-project.org/package=multcomp">https://CRAN.R-project.org/package=multcomp</a>.</p>
</div>
<div id="ref-Kampstra2008">
<p>Kampstra, Peter. 2008. “Beanplot: A Boxplot Alternative for Visual Comparison of Distributions.” <em>Journal of Statistical Software, Code Snippets</em> 28 (1): 1–9. <a href="http://www.jstatsoft.org/v28/c01/">http://www.jstatsoft.org/v28/c01/</a>.</p>
</div>
<div id="ref-R-beanplot">
<p>Kampstra, Peter. 2014. <em>Beanplot: Visualization via Beanplots (Like Boxplot/Stripchart/Violin Plot)</em>. <a href="https://CRAN.R-project.org/package=beanplot">https://CRAN.R-project.org/package=beanplot</a>.</p>
</div>
<div id="ref-Piepho2004">
<p>Piepho, Hans-Peter. 2004. “An Algorithm for a Letter-Based Representation of All-Pairwise Comparisons.” <em>Journal of Computational and Graphical Statistics</em> 13 (2): 456–66.</p>
</div>
<div id="ref-Plaster1989">
<p>Plaster, M. E. 1989. “Inmates as Mock Jurors: The Effects of Physical Attractiveness Upon Juridic Decisions.” Master’s thesis, Greenville, NC: East Carolina University.</p>
</div>
<div id="ref-R-mosaicData">
<p>Pruim, Randall, Daniel Kaplan, and Nicholas Horton. 2018. <em>MosaicData: Project Mosaic Data Sets</em>. <a href="https://CRAN.R-project.org/package=mosaicData">https://CRAN.R-project.org/package=mosaicData</a>.</p>
</div>
<div id="ref-R-mosaic">
<p>Pruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2019. <em>Mosaic: Project Mosaic Statistics and Mathematics Teaching Utilities</em>. <a href="https://CRAN.R-project.org/package=mosaic">https://CRAN.R-project.org/package=mosaic</a>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2019. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-RStudio">
<p>RStudio Team. 2018. <em>RStudio: Integrated Development Environment for R</em>. Boston, MA: RStudio, Inc. <a href="http://www.rstudio.com/">http://www.rstudio.com/</a>.</p>
</div>
<div id="ref-Westfall1993">
<p>Westfall, Peter H., and S. Stanley Young. 1993. <em>Resampling-Based Multiple Testing: Examples and Methods for P-Value Adjustment</em>. New York: Wiley.</p>
</div>
<div id="ref-R-readr">
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2018. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>A <strong><em>placebo</em></strong> is a treatment level designed to
mimic the potentially efficacious level(s) but that can have no actual effect. The
<strong><em>placebo effect</em></strong> is the effect that thinking that an effective treatment was
received has on subjects. There are other related issues in performing experiments
like the <strong><em>Hawthorne</em></strong> or <strong><em>observer effect</em></strong> where subjects modify behavior
because they are being observed.<a href="acknowledgments.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>We will reserve the term “effect” for situations where we could
potentially infer causal impacts on the response of the explanatory variable which
occurs in situations where the levels of the explanatory variable are randomly
assigned to the subjects.<a href="acknowledgments.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>I recorded a video that walks through getting R and RStudio installed on a PC available in the digital version <a href="https://montana.techsmithrelay.com/J1Ww"><strong>here</strong></a>. If you want to see them installed on a mac, you can try <a href="https://www.youtube.com/watch?v=GFImMj1lMRI"><strong>this version</strong></a>. Or for either version, try searching YouTube for “How to install R and RStudio”.<a href="acknowledgments.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>The need to keep the code up-to-date as R continues to evolve is one reason that this book is locally published and that this is the 5<sup>th</sup> version in
five years…<a href="acknowledgments.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>There are ways to read “.xls” and “.xlsx” files
directly into R that we will explore later.<a href="acknowledgments.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>If
you are having trouble getting the file converted and read into R, copy and
run the following code:
<code>treadmill &lt;- read_csv("http://www.math.montana.edu/courses/s217/documents/treadmill.csv")</code>.<a href="acknowledgments.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Most computer lab computers at
Montana State University have RStudio installed and so provide another venue
to work.<a href="acknowledgments.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>You can also use Ctrl+Enter if you like hot keys.<a href="acknowledgments.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Tibbles are R are objects that can contain both
categorical and quantitative variables on your <span class="math inline">\(n\)</span> subjects with a name for each
variable that is also the name of each column in a matrix.  Each subject is a
row of the data set. The name (supposedly) is due to the way <em>table</em> sounds in the accent of a particularly influential developer at RStudio who is from New Zealand.<a href="acknowledgments.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>The median, quartiles and whiskers sometimes occur at the same
values when there are many tied observations. If you can’t see all the
components of the boxplot, produce the numerical summary to help you understand
what happened.<a href="acknowledgments.html#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>You will more typically hear “data is” but that more often refers to
information, sometimes even statistical summaries of data sets, than to
observations collected as part of a study, suggesting the confusion of this
term in the general public. We will explore a data set in Chapter <a href="chapter5.html#chapter5">2</a>
related to perceptions of this issue collected by researchers at
<a href="http://fivethirtyeight.com/" class="uri">http://fivethirtyeight.com/</a>.<a href="acknowledgments.html#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>Either try to remember “data is a plural word” or replace “data” with “things” in your sentence and consider whether it sounds right.<a href="acknowledgments.html#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>As noted previously, we reserve the term “effect” for situations where random assignment  allows us to consider causality as the reason for the differences in the response variable among levels of the explanatory variable, but this is only the case if we find evidence against the null hypothesis of no difference
in the groups.<a href="acknowledgments.html#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>If you’ve taken calculus, you will
know that the
curve is being constructed so that the integral from <span class="math inline">\(-\infty\)</span> to
<span class="math inline">\(\infty\)</span> is 1. If you don’t know calculus, think of a rectangle with area
of 1 based on its height and width. These cover the same area but the top of the
region wiggles.<a href="acknowledgments.html#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>Jittering typically
involves adding random variability to each observation that
is uniformly distributed in a range determined based on the spacing of the
observation. This means that if you re-run the <code>jitter</code> function, the results will change.
For more details, type <code>help(jitter)</code> in the console in RStudio.<a href="acknowledgments.html#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>Remember the
bell-shaped curve you encountered in introductory statistics? If not, you can
see some at <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a><a href="acknowledgments.html#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>Well, you
can use other colors (try “lightblue” for example), but I think bisque
looks nice in these plots.<a href="acknowledgments.html#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>It is also possible to use the <code>subset</code> function to eliminate observations that do not have a particular trait and <code>subset(MockJury,subset=NotBeautiful)</code> would provide the same result as <code>MockJury[MockJury$NotBeautiful,]</code> did here.<a href="acknowledgments.html#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>The
hypothesis of no difference that is typically generated in the hopes of
being rejected in favor of the alternative hypothesis, which contains the sort
of difference that is of interest in the application.<a href="acknowledgments.html#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>The null model is the statistical model that
is implied by the chosen null hypothesis. Here, a null hypothesis of no
difference translates to having a model with the same mean for both groups.<a href="acknowledgments.html#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>We’ll see the <code>shuffle</code> function in a more common usage below;
while the code to generate <code>Perm1</code> is provided, it isn’t something to worry
about right now.<a href="acknowledgments.html#fnref21" class="footnote-back">↩</a></p></li>
<li id="fn22"><p>P-values  are the
probability of obtaining a result as extreme as or more extreme than we observed
given that the null hypothesis is true.<a href="acknowledgments.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>In statistics, vectors are one dimensional lists of numeric elements – basically a column from a matrix or our tibble.<a href="acknowledgments.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>We often say
“under” in statistics and we mean “given that the following is true”.<a href="acknowledgments.html#fnref24" class="footnote-back">↩</a></p></li>
<li id="fn25"><p>This is a fancy way of saying “in advance”,
here in advance of seeing the observations.<a href="acknowledgments.html#fnref25" class="footnote-back">↩</a></p></li>
<li id="fn26"><p>Statistically, a conservative method is
one that provides less chance of rejecting the null hypothesis in comparison to
some other method or less than some pre-defined standard.<a href="acknowledgments.html#fnref26" class="footnote-back">↩</a></p></li>
<li id="fn27"><p>We’ll leave the discussion of the CLT to your previous statistics coursework
or an internet search. For this material, just remember that it has something to do with distributions
looking more normal as the sample size increases.<a href="acknowledgments.html#fnref27" class="footnote-back">↩</a></p></li>
<li id="fn28"><p>On exams, you will be asked to describe the area of interest, sketch a
picture of the area of interest, and/or note the distribution you would use.<a href="acknowledgments.html#fnref28" class="footnote-back">↩</a></p></li>
<li id="fn29"><p>In some studies, the same subject might be measured in both conditions and
this violates the assumptions of this procedure.<a href="acknowledgments.html#fnref29" class="footnote-back">↩</a></p></li>
<li id="fn30"><p>Only male and female were provided as options on the survey.<a href="acknowledgments.html#fnref30" class="footnote-back">↩</a></p></li>
<li id="fn31"><p>The <code>as.numeric</code> function is also used here. It really isn’t important
but makes sure the output of <code>table</code> is sorted by observation number by first
converting the <em>orig.id</em> variable into a numeric vector.<a href="acknowledgments.html#fnref31" class="footnote-back">↩</a></p></li>
<li id="fn32"><p>In any bootstrap sample, about 1/3 of the observations are not used at all.<a href="acknowledgments.html#fnref32" class="footnote-back">↩</a></p></li>
<li id="fn33"><p>There are actually many ways to use this
information to make a confidence interval. We are using the simplest method
that is called the “percentile” method.<a href="acknowledgments.html#fnref33" class="footnote-back">↩</a></p></li>
<li id="fn34"><p>When hypothesis tests “work well” they have high
power  to detect differences while having Type I error rates  that are close
to what we choose <em>a priori</em>. When confidence intervals “work well”, they contain
the true parameter value in repeated random samples at around the selected
confidence level. <a href="acknowledgments.html#fnref34" class="footnote-back">↩</a></p></li>
<li id="fn35"><p>We will often use this term to
mean from the component summary information – not that you need to go back to
the data set and calculate the means and standard deviations.<a href="acknowledgments.html#fnref35" class="footnote-back">↩</a></p></li>
<li id="fn36"><p>We rounded the means a little and that caused the small
difference in results.<a href="acknowledgments.html#fnref36" class="footnote-back">↩</a></p></li>
<li id="fn37"><p>You can correctly call octothorpes <em>number</em> symbols or, in the
twitter verse, <em>hashtags</em>. For more on this symbol, see
“<a href="http://blog.dictionary.com/octothorpe/" class="uri">http://blog.dictionary.com/octothorpe/</a>”. Even after reading this, I call them
number symbols.<a href="acknowledgments.html#fnref37" class="footnote-back">↩</a></p></li>
<li id="fn38"><p>In Chapter <a href="chapter4.html#chapter4">1</a>, methods are discussed for when there are two
categorical explanatory variables that is called the Two-Way ANOVA and related
ANOVA tests are used in Chapter <a href="chapter8.html#chapter8">5</a> for working with extensions of
these models.<a href="acknowledgments.html#fnref38" class="footnote-back">↩</a></p></li>
<li id="fn39"><p>
If you look closely in the code for the rest of the book, any model for a
quantitative response will use this function, suggesting a common thread in
the most commonly used statistical models.<a href="acknowledgments.html#fnref39" class="footnote-back">↩</a></p></li>
<li id="fn40"><p>Suppose we were doing environmental monitoring
and were studying asbestos levels in soils. We might be hoping that the mean-only
model were reasonable to use if the groups being compared were in remediated areas
and in areas known to have never been contaminated.<a href="acknowledgments.html#fnref40" class="footnote-back">↩</a></p></li>
<li id="fn41"><p>Make sure you can work
from left to right and up and down to fill in the ANOVA table given just the
necessary information to determine the other components – there is always a
question like this on the exam…<a href="acknowledgments.html#fnref41" class="footnote-back">↩</a></p></li>
<li id="fn42"><p>This fits with a critique of p-value usage called
p-hacking or publication bias – where researchers search across many
results and only report their biggest differences. This biases the
results to detecting results more than they should be and then when
other researchers try to repeat the same studies, they fail to find
similar results.<a href="acknowledgments.html#fnref42" class="footnote-back">↩</a></p></li>
<li id="fn43"><p>We have been using this function quite a bit to make multi-panel
graphs but did not show you that line of code. But you need to use this command
for linear model diagnostics or you won’t get the plots we want from the model.
And you really just need <code>plot(lm2)</code> but the <code>pch=16</code> option makes it easier
to see some of the points in the plots.<a href="acknowledgments.html#fnref43" class="footnote-back">↩</a></p></li>
<li id="fn44"><p>Along with multiple names,
there is variation of what is plotted on the x and y axes and the scaling of
the values plotted, increasing the challenge of interpreting QQ-plots. We are
consistent about the x and y axis choices throughout this book but different functions that make
these plots in R do switch the axes.<a href="acknowledgments.html#fnref44" class="footnote-back">↩</a></p></li>
<li id="fn45"><p>Here this means re-scaled so that they should have similar
scaling to a standard normal with mean 0 and standard deviation 1. This does
not change the shape of the distribution but can make outlier identification
simpler – having a standardized residual more extreme than 5 or -5 would
suggest a deviation from normality since we rarely see values that many
standard deviations from the mean in a normal distribution. But mainly focus
on the shape of the pattern in the QQ-plot.<a href="acknowledgments.html#fnref45" class="footnote-back">↩</a></p></li>
<li id="fn46"><p>A resistant
procedure is one that is not severely impacted by a particular violation of an
assumption. For example, the median is resistant to the impact of
an outlier. But the mean is not a resistant measure as changing the value
of a single point changes the mean.<a href="acknowledgments.html#fnref46" class="footnote-back">↩</a></p></li>
<li id="fn47"><p>A violation of the independence
assumption could have easily been created if they measured cells in

two locations on each guinea pig or took measurements over time on each subject.<a href="acknowledgments.html#fnref47" class="footnote-back">↩</a></p></li>
<li id="fn48"><p>Note that to see all the group labels in the plot
when making the figure, you have to widen the plot window before copying the
figure out of R. You can resize the plot window using the small vertical and
horizontal “=” signs in the grey bars that separate the different panels in
RStudio.<a href="acknowledgments.html#fnref48" class="footnote-back">↩</a></p></li>
<li id="fn49"><p>In working with researchers on hundreds of projects, our
experience has been that we often require many conversations to discover
all the potential sources of issues in data sets, especially related to
assessing independence of the observations.<a href="acknowledgments.html#fnref49" class="footnote-back">↩</a></p></li>
<li id="fn50"><p>When this procedure is used with unequal group sizes it is also sometimes
called Tukey-Kramer’s method.<a href="acknowledgments.html#fnref50" class="footnote-back">↩</a></p></li>
<li id="fn51"><p>We often use “spurious” to describe falsely rejected null hypotheses, but
they are also called false detections.<a href="acknowledgments.html#fnref51" class="footnote-back">↩</a></p></li>
<li id="fn52"><p>Some researchers are now collecting multiple data
sets to use in a single study and using one data set to identify interesting
results and then using a validation or test data set that they withheld from
initial analysis to try to verify that the first results are also present in that
second data set. This also has problems but the only way to develop an understanding of a process is to look across a suite of studies and learn from that accumulation of evidence.<a href="acknowledgments.html#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>The plot of results usually contains all the labels of groups but if the labels are long or there many groups, sometimes the row labels are hard to see even with re-sizing the plot to make it taller in RStudio. The numerical output is
useful as a guide to help you read the plot.<a href="acknowledgments.html#fnref53" class="footnote-back">↩</a></p></li>
<li id="fn54"><p>Note that this method is implemented slightly differently than we explain here in some software packages so if you see this in a journal article, read the discussion carefully.<a href="acknowledgments.html#fnref54" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
